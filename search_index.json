{"LANDED-table":{"location":"internals.html#LANDED-table","title":"LANDED","text":"A worker in this state has successfully waited for all non-interruptible jobs on it after having concourse land-worker called. It will no longer be used to schedule any new containers or create volumes until it registers as RUNNING again.\n\n","depth":4,"section_tag":"worker-lifecycle"},"LANDING-table":{"location":"internals.html#LANDING-table","title":"LANDING","text":"The concourse land-worker command will put a worker in the LANDING state to safely drain its assignments for temporary downtime.\n\nThe ATC will wait for builds on the worker for jobs which aren't interruptible to finish, and transition the worker into LANDED state.\n\n","depth":4,"section_tag":"worker-lifecycle"},"RETIRING-table":{"location":"internals.html#RETIRING-table","title":"RETIRING","text":"The concourse retire-worker command will put a worker in the RETIRING state to remove it from the cluster permanently.\n\nThe ATC will wait for builds on the worker for jobs which aren't interruptible to finish, and remove the worker.\n\n","depth":4,"section_tag":"worker-lifecycle"},"RUNNING-table":{"location":"internals.html#RUNNING-table","title":"RUNNING","text":"A worker in this state is registered with the cluster and ready to start running containers and storing volumes.\n\n","depth":4,"section_tag":"worker-lifecycle"},"STALLED-table":{"location":"internals.html#STALLED-table","title":"STALLED","text":"A worker in this state was previously registered with the cluster, but stopped advertising itself for some reason. Ususally this is due to network connectivity issues, or the worker stopping unexpectedly.\n\nIf the worker remains in this state and cannot be recovered, it can be removed using the fly prune-worker command.\n\n","depth":4,"section_tag":"worker-lifecycle"},"action-matrix":{"location":"user-roles.html#action-matrix","title":"Action Matrix","text":"In this table, an action is marked as customizable if it is possible to change its permissions by providing the --config-rbac flag, documented below. Assigning an action to a role that is not customizable will have no effect on its permissions.\n\n| Action                        | fly commands affected                                       | UI actions affected                     | can be performed unauthenticated? | customizable |\n| GetBuild                      | n/a                                                                | view one-off build page                 | ✓                                 | ✓            |\n| BuildResources                | n/a                                                                | view build page                         | ✓                                 | ✓            |\n| GetBuildPreparation           | n/a                                                                | view build page                         | ✓                                 | ✓            |\n| BuildEvents                   | fly watch,Running tasks with fly execute                      | view build page                         | ✓                                 | ✓            |\n| GetBuildPlan                  | n/a                                                                | view build page                         | ✓                                 | ✓            |\n| ListBuildArtifacts            | n/a                                                                | n/a                                     | ✓                                 | ✓            |\n| AbortBuild                    | fly abort-build                                        | abort button on build page              | ✘                                 | ✓            |\n| PruneWorker                   | fly prune-worker                                       | n/a                                     | ✘                                 | ✓            |\n| LandWorker                    | fly land-worker                                        | n/a                                     | ✘                                 | ✓            |\n| RetireWorker                  | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| ListDestroyingVolumes         | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| ListDestroyingContainers      | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| ReportWorkerContainers        | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| ReportWorkerVolumes           | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| GetPipeline                   | n/a                                                                | view pipeline page                      | ✓                                 | ✓            |\n| GetJobBuild                   | n/a                                                                | view build page                         | ✓                                 | ✓            |\n| PipelineBadge                 | n/a                                                                | n/a                                     | ✓                                 | ✓            |\n| JobBadge                      | n/a                                                                | n/a                                     | ✓                                 | ✓            |\n| ListJobs                      | fly jobs                                               | view pipeline page                      | ✓                                 | ✓            |\n| GetJob                        | n/a                                                                | view job page                           | ✓                                 | ✓            |\n| ListJobBuilds                 | fly builds                                             | view job page                           | ✓                                 | ✓            |\n| ListPipelineBuilds            | fly builds                                             | n/a                                     | ✓                                 | ✓            |\n| GetResource                   | n/a                                                                | view resource page                      | ✓                                 | ✓            |\n| ListBuildsWithVersionAsInput  | n/a                                                                | expand version on resource page         | ✓                                 | ✓            |\n| ListBuildsWithVersionAsOutput | n/a                                                                | expand version on resource page         | ✓                                 | ✓            |\n| GetResourceCausality          | n/a                                                                | n/a                                     | ✓                                 | ✓            |\n| GetResourceVersion            | n/a                                                                | n/a                                     | ✓                                 | ✓            |\n| ListResources                 | fly resources                                               | view pipeline page                      | ✓                                 | ✓            |\n| ListResourceTypes             | n/a                                                                | n/a                                     | ✓                                 | ✓            |\n| ListResourceVersions          | fly resource-versions,fly pin-resource          | view resource page                      | ✓                                 | ✓            |\n| CreateBuild                   | Running tasks with fly execute                                            | n/a                                     | ✘                                 | ✓            |\n| GetContainer                  | n/a                                                                | n/a                                     | ✘                                 | ✓            |\n| HijackContainer               | fly intercept                                          | n/a                                     | ✘                                 | ✓            |\n| ListContainers                | fly containers                                         | n/a                                     | ✘                                 | ✓            |\n| ListWorkers                   | fly workers                                            | n/a                                     | ✘                                 | ✓            |\n| RegisterWorker                | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| HeartbeatWorker               | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| DeleteWorker                  | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| GetTeam                       | fly get-team                                           | n/a                                     | ✘                                 | ✓            |\n| SetTeam                       | fly set-team                                           | n/a                                     | ✘                                 | ✓            |\n| ListTeamBuilds                | fly builds                                             | n/a                                     | ✘                                 | ✓            |\n| RenameTeam                    | fly rename-team                                        | n/a                                     | ✘                                 | ✓            |\n| DestroyTeam                   | fly destroy-team                                       | n/a                                     | ✘                                 | ✓            |\n| ListVolumes                   | fly volumes                                            | n/a                                     | ✘                                 | ✓            |\n| DownloadCLI                   | fly sync                                               | icons on dashboard and pipeline pages   | ✓                                 | ✘            |\n| CheckResourceWebHook          | n/a                                                                | n/a                                     | ✓                                 | ✘            |\n| GetInfo                       | n/a                                                                | n/a                                     | ✓                                 | ✘            |\n| GetCheck                      | fly check-resource,fly check-resource-type | check button on resource page           | ✘                                 | ✓            |\n| ListTeams                     | fly teams                                              | view dashboard page                     | ✓                                 | ✘            |\n| ListAllPipelines              | n/a                                                                | view dashboard page                     | ✓                                 | ✘            |\n| ListPipelines                 | fly pipelines                                          | n/a                                     | ✓                                 | ✓            |\n| ListAllJobs                   | fly teams                                              | view dashboard page                     | ✓                                 | ✘            |\n| ListAllResources              | n/a                                                                | view dashboard page                     | ✓                                 | ✘            |\n| ListBuilds                    | fly builds                                             | n/a                                     | ✓                                 | ✘            |\n| MainJobBadge                  | n/a                                                                | n/a                                     | ✓                                 | ✘            |\n| GetLogLevel                   | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| ListActiveUsersSince          | fly active-users                                       | n/a                                     | ✘                                 | ✘            |\n| GetInfoCreds                  | n/a                                                                | n/a                                     | ✘                                 | ✘            |\n| CheckResource                 | fly check-resource                                     | check button on resource page           | ✘                                 | ✓            |\n| CheckResourceType             | fly check-resource-type                                | n/a                                     | ✘                                 | ✓            |\n| CreateJobBuild                | fly trigger-job                                        | trigger button on job and build pages   | ✘                                 | ✓            |\n| RerunJobBuild                 | fly rerun-build                                        | rerun button on build page              | ✘                                 | ✓            |\n| CreatePipelineBuild           | Running tasks with fly execute                                            | n/a                                     | ✘                                 | ✓            |\n| DeletePipeline                | fly destroy-pipeline                                   | n/a                                     | ✘                                 | ✓            |\n| DisableResourceVersion        | fly disable-resource-version                           | version disable widget on resource page | ✘                                 | ✓            |\n| EnableResourceVersion         | fly enable-resource-version                            | version enable widget on resource page  | ✘                                 | ✓            |\n| PinResourceVersion            | fly pin-resource                                       | pin buttons on resource page            | ✘                                 | ✓            |\n| UnpinResource                 | fly unpin-resource                                          | pin buttons on resource page            | ✘                                 | ✓            |\n| SetPinCommentOnResource       | fly pin-resource                                       | comment overlay on resource page        | ✘                                 | ✓            |\n| GetConfig                     | fly get-pipeline                                       | n/a                                     | ✘                                 | ✓            |\n| GetCC                         | n/a                                                                | n/a                                     | ✘                                 | ✓            |\n| GetVersionsDB                 | n/a                                                                | n/a                                     | ✘                                 | ✓            |\n| ListJobInputs                 | n/a                                                                | n/a                                     | ✘                                 | ✓            |\n| OrderPipelines                | fly order-pipelines                                    | drag and drop on dashboard              | ✘                                 | ✓            |\n| PauseJob                      | fly pause-job                                          | pause button on job page                | ✘                                 | ✓            |\n| PausePipeline                 | fly pause-pipeline                                     | pause button on pipeline or dashboard   | ✘                                 | ✓            |\n| RenamePipeline                | fly rename-pipeline                                    | n/a                                     | ✘                                 | ✓            |\n| UnpauseJob                    | fly unpause-job                                        | play button on job page                 | ✘                                 | ✓            |\n| UnpausePipeline               | fly unpause-pipeline                                   | play button on pipeline or dashboard    | ✘                                 | ✓            |\n| ExposePipeline                | fly expose-pipeline                                    | eyeball button on dashboard             | ✘                                 | ✓            |\n| HidePipeline                  | fly hide-pipeline                                      | slashed eyeball button on dashboard     | ✘                                 | ✓            |\n| SaveConfig                    | fly set-pipeline                                       | n/a                                     | ✘                                 | ✓            |\n| ClearTaskCache                | fly clear-task-cache                                   | n/a                                     | ✘                                 | ✓            |\n| CreateArtifact                | Running tasks with fly execute                                            | n/a                                     | ✘                                 | ✓            |\n| GetArtifact                   | Running tasks with fly execute                                            | n/a                                     | ✘                                 | ✓            |\n\n","depth":4,"section_tag":"action-matrix"},"adding-cf-users-to-the-main-team":{"location":"cf-uaa-auth.html#adding-cf-users-to-the-main-team","title":"Adding CF Users to the main Team","text":"CloudFoundry users and org/space members can be added to the The main team authorization config by setting the following env on the Running a web node:\n\nCONCOURSE_MAIN_TEAM_CF_USER=username\nCONCOURSE_MAIN_TEAM_CF_ORG=org-name\nCONCOURSE_MAIN_TEAM_CF_SPACE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_ANY_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_DEVELOPER_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_AUDITOR_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_WITH_MANAGER_ROLE=org-name:space-name\nCONCOURSE_MAIN_TEAM_CF_SPACE_GUID=SPACE_GUID\nMultiple users, spaces, etc. may be specified by comma-separating them.\n\n","depth":6,"section_tag":"adding-cf-users-to-the-main-team"},"administration":{"location":"administration.html","title":"Administration","text":"","depth":3,"section_tag":"administration"},"aggregate-step":{"location":"jobs.html#aggregate-step","title":"aggregate step","text":"","depth":3,"section_tag":"steps"},"algorithm":{"location":"scheduler.html#algorithm","title":"Algorithm","text":"The algorithm is a subcomponent of the scheduler which is used to determine the input versions to the next build of a job. There are many factors that contribute to figuring out the next input versions. It can be anything that affects which resource versions will be used to schedule a build, such as version constraints or passed constraints in a get step, disabling versions through the web UI, etc. The algorithm can also fail to determine a successful set of input versions, which the error will be propogated to the preparation view in the build page.\n\nIf the algorithm computes a successful set of input versions, it will figure out whether or not the versions it computed can be used to produce a new build. This is done by comparing the triggerable input versions to the versions used by the previous build and if any of them have a different version, then the scheduler will know to schedule a new build. Conversly, if the input versions produced by the algorithm are the same as the previous build, then the scheduler will not create a new build.\n\n","depth":5,"section_tag":"algorithm"},"architecture-worker":{"location":"internals.html#architecture-worker","title":"Workers: container runtime \u0026 cache management","text":"Workers are machines running Garden and Baggageclaim servers and registering themselves via the TSA.\n\nWorkers have no important state configured on their machines, as everything runs in a container and thus shouldn't care about what packages are installed on the host (well, except for those that allow it to be a worker in the first place). This is very different from workers in other non-containerized CI solutions, where the state of packages on the worker is crucial to whether your pipeline works or not.\n\nEach worker registers itself with the Concourse cluster via the TSA.\n\nWorkers by default listen on port 7777 for Garden and port 7788 for Baggageclaim. If they are within a private network reachable by the ATC, they'll probably bind on all addresses (0.0.0.0) and register themselves directly. Otherwise they should bind on 127.0.0.1 and forward themselves through the TSA.\n\n","depth":3,"section_tag":"architecture-worker"},"audit-logs":{"location":"concourse-web.html#audit-logs","title":"Enabling audit logs","text":"A very simplistic form of audit logging can be enabled with the following vars:\n\n# Enable auditing for all api requests connected to builds.\nCONCOURSE_ENABLE_BUILD_AUDITING=true\n\n# Enable auditing for all api requests connected to containers.\nCONCOURSE_ENABLE_CONTAINER_AUDITING=true\n\n# Enable auditing for all api requests connected to jobs.\nCONCOURSE_ENABLE_JOB_AUDITING=true\n\n# Enable auditing for all api requests connected to pipelines.\nCONCOURSE_ENABLE_PIPELINE_AUDITING=true\n\n# Enable auditing for all api requests connected to resources.\nCONCOURSE_ENABLE_RESOURCE_AUDITING=true\n\n# Enable auditing for all api requests connected to system transactions.\nCONCOURSE_ENABLE_SYSTEM_AUDITING=true\n\n# Enable auditing for all api requests connected to teams.\nCONCOURSE_ENABLE_TEAM_AUDITING=true\n\n# Enable auditing for all api requests connected to workers.\nCONCOURSE_ENABLE_WORKER_AUDITING=true\n\n# Enable auditing for all api requests connected to volumes.\nCONCOURSE_ENABLE_VOLUME_AUDITING=true\nWhen enabled, API requests will result in an info-level log line like so:\n\n{\"timestamp\":\"2019-05-09T14:41:54.880381537Z\",\"level\":\"info\",\"source\":\"atc\",\"message\":\"atc.audit\",\"data\":{\"action\":\"Info\",\"parameters\":{},\"user\":\"test\"}}\n{\"timestamp\":\"2019-05-09T14:42:36.704864093Z\",\"level\":\"info\",\"source\":\"atc\",\"message\":\"atc.audit\",\"data\":{\"action\":\"GetPipeline\",\"parameters\":{\":pipeline_name\":[\"booklit\"],\":team_name\":[\"main\"]},\"user\":\"test\"}}\n","depth":5,"section_tag":"audit-logs"},"auth":{"location":"auth.html","title":"Auth \u0026 Teams","text":"A single Concourse installation can accomodate many projects and users.\n\nPipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the concourse GitHub organization to be a member.\n\nWhen a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.\n\n","depth":2,"section_tag":"auth"},"authenticating-with-vault":{"location":"vault-credential-manager.html#authenticating-with-vault","title":"Authenticating with Vault","text":"There are many ways to authenticate with a Vault server. The web-node can be configured with either a token or an arbitrary auth backend and arbitrary auth params, so just about all of them should be configurable.\n\nWhen the web node acquires a token, either by logging in with an auth backend or by being given one directly, it will continuously renew the token to ensure it doesn't expire. The renewal interval is half of the token's lease duration.\n\n","depth":5,"section_tag":"authenticating-with-vault"},"aws-asm-credential-manager":{"location":"aws-asm-credential-manager.html","title":"The AWS Secrets Manager credential manager","text":"","depth":4,"section_tag":"aws-asm-credential-manager"},"aws-secretsmanager-access-key":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-access-key","title":"aws-secretsmanager-access-key","text":"A valid AWS access key.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_ACCESS_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-pipeline-secret-template":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-pipeline-secret-template","title":"aws-secretsmanager-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-region":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-region","title":"aws-secretsmanager-region","text":"The AWS region that requests to Secrets Manager will be sent to.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_REGION.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-secret-key":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-secret-key","title":"aws-secretsmanager-secret-key","text":"The secret key that corresponds to the access key defined above.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_SECRET_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-session-token":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-session-token","title":"aws-secretsmanager-session-token","text":"A valid AWS session token.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_SESSION_TOKEN.\n\n","depth":5,"section_tag":"configuration"},"aws-secretsmanager-team-secret-template":{"location":"aws-asm-credential-manager.html#aws-secretsmanager-team-secret-template","title":"aws-secretsmanager-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_AWS_SECRETSMANAGER_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-access-key":{"location":"aws-ssm-credential-manager.html#aws-ssm-access-key","title":"aws-ssm-access-key","text":"A valid AWS access key.\n\nEnvironment variable CONCOURSE_AWS_SSM_ACCESS_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-credential-manager":{"location":"aws-ssm-credential-manager.html","title":"The AWS SSM credential manager","text":"","depth":4,"section_tag":"aws-ssm-credential-manager"},"aws-ssm-pipeline-secret-template":{"location":"aws-ssm-credential-manager.html#aws-ssm-pipeline-secret-template","title":"aws-ssm-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_AWS_SSM_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-region":{"location":"aws-ssm-credential-manager.html#aws-ssm-region","title":"aws-ssm-region","text":"The AWS region that requests to parameter store will be sent to.\n\nEnvironment variable CONCOURSE_AWS_SSM_REGION.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-secret-key":{"location":"aws-ssm-credential-manager.html#aws-ssm-secret-key","title":"aws-ssm-secret-key","text":"The secret key that corresponds to the access key defined above.\n\nEnvironment variable CONCOURSE_AWS_SSM_SECRET_KEY.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-session-token":{"location":"aws-ssm-credential-manager.html#aws-ssm-session-token","title":"aws-ssm-session-token","text":"A valid AWS session token.\n\nEnvironment variable CONCOURSE_AWS_SSM_SESSION_TOKEN.\n\n","depth":5,"section_tag":"configuration"},"aws-ssm-team-secret-template":{"location":"aws-ssm-credential-manager.html#aws-ssm-team-secret-template","title":"aws-ssm-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_AWS_SSM_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: /concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"badges":{"location":"observation.html#badges","title":"Badges","text":"The Concourse API supports returning SVG badges indicating the status of a job:\n\n/api/v1/teams/{team}/pipelines/{pipeline}/jobs/{job}/badge\n...and for an entire pipeline:\n\n/api/v1/teams/{team}/pipelines/{pipeline}/badge\nThis can be used to annotate your READMEs with a build status badge like so:\n\n![CI](https://wings.pivotal.io/api/v1/teams/vito/pipelines/booklit/jobs/unit/badge)]\n...which should render the following:\n\n{image: https://wings.pivotal.io/api/v1/teams/vito/pipelines/booklit/jobs/unit/badge}\n\n","depth":3,"section_tag":"badges"},"basic-architecture":{"location":"internals.html#basic-architecture","title":"Basic architecture","text":"Concourse is a fairly simple distributed system built up from the following components. You'll see them referenced here and there throughout the documentation, so you may want to skim this page just to get an idea of what they are.\n\nimages/concourse_architecture.png","depth":3,"section_tag":"basic-architecture"},"basic-schemas":{"location":"config-basics.html#basic-schemas","title":"Basic Schemas","text":"Throughout the Concourse documentation you will find come across schema definitions for each API.\n\nThe following are basic schema definitions that the other schemas refer to. You can probably skip past this and just make assumptions along the way; this is just here for completeness!\n\nAny integer, i.e. 1000.\n\nAn arbitrary string value with no content restrictions.\n\nAn arbitrary object representing configuration that is not directly interpreted by Concourse - typically given to a resource type.\n\nuri: https://github.com/vito/booklit\nbranch: master\nAll object keys must be strings, preferably snake_cased.\n\nAn arbitrary object representing key-value definitions for ((vars)).\n\nAs with config schema, all object keys must be strings, preferably snake_cased.\n\nAn object containing string keys and string values. Each pair represents an environment variable to set to the given value.\n\nSOME_SIMPLE_VAR: simple-var\nSOME_LONG_VAR: |\n  This is an example of using YAML multi-line string syntax to set a very\n  long environment variable value.\nSOME_NUMERIC_VALUE: \"1\"\nNote that in the last example we took special care to quote the number.\n\ntrue or false.\n\nYAML also supports the alias yes, no, on, or off, but...please don't.\n\nAn identifier is string value. Currently, there are no validations for what makes a string a valid identifier.\n\nRFC #40 proposes a much more limited character set, so for the sake of future-proofing you may want to conform to it.\n\nThe proposed valid character set is as follows:\n\n^\\p{L}[\\p{L}\\d\\-.]*$\n(Don't worry - if you currently have pipelines that do not conform to the above regexp validation, they will be permitted to function; this validation will only be peformed for newly configured pipelines.)\n\nA string value specifying a (typically relative) path of a directory.\n\nA string value specifying a (typically relative) path of a file.\n\nA string value in Go time.ParseDuration format. 1h for one hour, 5m for 5 minutes.\n\nAn object with string keys and string values.\n\nThe following is an array of versions:\n\n- {\"ref\": \"33042e15e930b6786fc9b0a9ea5dec78689c5e4b\"}\n- ref: v1.2.0,\n  sha: 33042e15e930b6786fc9b0a9ea5dec78689c5e4b\n- foo: \"0\"\nNote that in the last example we took special care to quote the number.\n\nIn many scenarios where a version can be specified, e.g. get step version, only a subset of the full set of fields is necessary. The latest version matching the fields specified will be chosen.\n\n","depth":3,"section_tag":"basic-schemas"},"benefits-of-global-resources":{"location":"global-resources.html#benefits-of-global-resources","title":"Benefits of Global Resources","text":"","depth":4,"section_tag":"benefits-of-global-resources"},"bitbucket-cloud-auth":{"location":"bitbucket-cloud-auth.html","title":"BitBucket Cloud auth","text":"A Concourse server can authenticate against BitBucket Cloud to leverage its permission model.\n\n","depth":4,"section_tag":"bitbucket-cloud-auth"},"bitbucket-cloud-authentication":{"location":"bitbucket-cloud-auth.html#bitbucket-cloud-authentication","title":"Authentication","text":"First, you'll need to create an OAuth consumer on Bitbucket Cloud.\n\nThe consumer will need the following permissions:\n\n* Account:\n\n  * Email\n\n  * Read\n\n* Team membership:\n\n  * Read\n\nThe \"Callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by BitBucket Cloud - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_ID=myclientid\nCONCOURSE_BITBUCKET_CLOUD_CLIENT_SECRET=myclientsecret\n","depth":5,"section_tag":"bitbucket-cloud-authentication"},"bitbucket-cloud-authorization":{"location":"bitbucket-cloud-auth.html#bitbucket-cloud-authorization","title":"Authorization","text":"BitBucket users and teams can be authorized for a team by passing the following flags to fly set-team:\n\n--bitbucket-cloud-user=LOGIN: Authorize an individual user.\n\n\n--bitbucket-cloud-team=TEAM_NAME: Authorize an entire organization's members.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --bitbucket-cloud-user my-bitbucket-login \\\n    --bitbucket-cloud-team my-bitbucket-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  bitbucket-cloud:\n    users: [\"my-bitbucket-login\"]\n    teams: [\"my-bitbucket-team\"]\n","depth":5,"section_tag":"bitbucket-cloud-authorization"},"build-plans":{"location":"jobs.html#steps","title":"Steps","text":"Each job has a single build plan configured as jobplan. A build plan is a recipe for what to run when a build of the job is created.\n\nA build plan is a sequence of steps:\n\n* the task step runs a task\n\n* the get step fetches a resource\n\n* the put step updates a resource\n\n* the set_pipeline step configures a pipeline\n\n* the load_var step loads a value into a local var\n\n* the in_parallel step runs steps in parallel\n\n* the do step runs steps in sequence\n\n* the try step attempts to run a step and succeeds even if the step fails\n\nWhen a new version is available for a get step with trigger: true configured, a new build of the job will be created from the build plan.\n\nWhen viewing the job in the pipeline, resources that are used as get steps appear as inputs, and resources that are used in put steps appear as outputs. Jobs are rendered downstream of any jobs they reference in passed constraints, connected by the resource.\n\nIf any step in the build plan fails, the build will fail and subsequent steps will not be executed. Additional steps may be configured to run after failure by configuring stepon_failure or stepensure (or the job equivalents, jobon_failure and jobensure).\n\n","depth":3,"section_tag":"steps"},"build-rerunning":{"location":"builds.html#build-rerunning","title":"Rerunning a Build","text":"Concourse supports build rerunning, which means to run a new build using the exact same set of input versions as the original build. There are two ways to rerun a build: through the web UI on the builds page and through the fly rerun-build.\n\nWhen a build is rerun, it will create a new build using the name of the original build with the rerun number appended to it, e.g. 3.1 for the first rerun of build 3.\n\nRerun builds are ordered chronologically after the original build, rather than becoming a new \"latest\" build. Similarly, when the scheduler is resolving passed constraints which reference a job which has rerun builds, those rerun builds are processed in this same order. This is so that the versions which made it through a rerun build do not become the new \"latest versions\" - instead, they act as if the original had succeeded at its point in the build history.\n\nThis may sound a little confusing, but the summary is that reruns should behave as if they replace the original failed build.\n\n","depth":3,"section_tag":"build-rerunning"},"build-tracker":{"location":"build-tracker.html","title":"Build Tracker","text":"The build tracker is the component that runs the execution of a build. It picks up any started builds, which can be orphaned builds (builds that an ATC started but did not finish) or builds that have just been scheduled. There is one build tracker per ATC, which runs on an interval that is defaulted to 10 seconds.\n\n","depth":4,"section_tag":"build-tracker"},"builds":{"location":"builds.html","title":"Builds","text":"A build is an execution of a build plan, which is either configured as a sequence of steps in a job, or submitted directly to Concourse as a one-off build via Running tasks with fly execute.\n\nContainers and volumes are created as get steps, put steps, and task steps run. When a build completes successfully, these containers go away.\n\nA failed build's containers and volumes are kept around so that you can debug the build via fly intercept. If the build belongs to a job, the containers will go away when the next build starts. If the build is a one-off, its containers will be removed immediately, so make sure you intercept while it's running if you want to debug.\n\n","depth":2,"section_tag":"builds"},"ccxml":{"location":"observation.html#ccxml","title":"cc.xml","text":"The Concourse API can return the status of a team's pipelines in a format compatible with tools like CCMenu. This endpoint is available at the following route:\n\n/api/v1/teams/{team}/cc.xml\nFor example: the main team cc.xml.\n\n","depth":3,"section_tag":"ccxml"},"cf-authentication":{"location":"cf-uaa-auth.html#cf-authentication","title":"Authentication","text":"You'll need to configure your UAA with a concourse client by setting the following under uaa.clients:\n\nconcourse:\n  id: myclientid\n  secret: myclientsecret\n  scope: openid,cloud_controller.read\n  authorized-grant-types: \"authorization_code,refresh_token\"\n  access-token-validity: 3600\n  refresh-token-validity: 3600\n  redirect-uri: https://concourse.example.com/sky/issuer/callback\nThe value for redirect-uri must be the external URL of your Concourse server with /sky/issuer/callback appended.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nNext, you'll need to take the same client ID and secret and configure it on the Running a web node by setting the following env:\n\nCONCOURSE_CF_API_URL=http://mycf.example.com\nCONCOURSE_CF_CLIENT_ID=myclientid\nCONCOURSE_CF_CLIENT_SECRET=myclientsecret\nNote: if you're integrating with Cloud Foundry, you're probably also deploying Concourse via BOSH - in which case you'll want to set the cf_auth.* properties in your manifest instead of setting the above env.\n\n","depth":5,"section_tag":"cf-authentication"},"cf-authorization":{"location":"cf-uaa-auth.html#cf-authorization","title":"Authorization","text":"CloudFoundry users and org/space members can be authorized for a team by passing the following flags to fly set-team:\n\n--cf-user=USERNAME: Authorize an individual user.\n\n\n--cf-org=ORG_NAME: Authorize an entire organization's members.\n\n\n--cf-space=ORG_NAME:SPACE_NAME: Deprecated in favor of --cf-space-with-developer-role. Authorize the members with developer role of a space within an organization.\n\n\n--cf-space-with-any-role=ORG_NAME:SPACE_NAME: Authorize the members with any role of a space within an organization.\n\n\n--cf-space-with-developer-role=ORG_NAME:SPACE_NAME: Authorize the members with developer role of a space within an organization.\n\n\n--cf-space-with-auditor-role=ORG_NAME:SPACE_NAME: Authorize the members with auditor role of a space within an organization.\n\n\n--cf-space-with-manager-role=ORG_NAME:SPACE_NAME: Authorize the members with manager role of a space within an organization.\n\n\n--cf-space-guid=SPACE_GUID: Authorize the members with any role of a space within an organization by space GUID.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --cf-user my-username \\\n    --cf-org my-org \\\n    --cf-space my-other-org:my-space\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  cf:\n    users: [\"my-username\"]\n    orgs: [\"my-org\"]\n    spaces: [\"my-other-org:my-space\"]\n","depth":5,"section_tag":"cf-authorization"},"cf-uaa-auth":{"location":"cf-uaa-auth.html","title":"CF/UAA auth","text":"Cloud Foundry (CF) auth can be used for operators who wish to authenticate their users configured against their Cloud Foundry instance via the UAA auth component.\n\n","depth":4,"section_tag":"cf-uaa-auth"},"checker":{"location":"checker.html","title":"Resource Checker","text":"Resources represent external state such as a git repository, an s3 bucket, or anything else that changes over time. By modelling these as resources, it allows you to use this external state as inputs (or triggers) to your workloads.\n\n","depth":4,"section_tag":"checker"},"cluster-wide-credential-manager":{"location":"vars.html#cluster-wide-credential-manager","title":"The cluster-wide credential manager","text":"Concourse can be configured with a single cluster-wide credential manager, which acts as a source for any vars which do not specify a source name.\n\nSee Credential Management for more information.\n\nIn the future we would like to introduce support for multiple cluster-wide var sources, configured using the var_source schema schema, and begin deprecating the The cluster-wide credential manager.\n\n","depth":4,"section_tag":"cluster-wide-credential-manager"},"complications-with-reusing-containers":{"location":"global-resources.html#complications-with-reusing-containers","title":"Complications with reusing containers","text":"There is an exception to sharing check containers within a deployment, which is workers belonging to a team and workers with tags.\n\nIf a resource has resourcetags configured, and the resource's check interval ends up acquiring the checking lock, a new container will be created on a worker matching the appropriate tags, even if a check container already exists for the same resource config elsewhere.\n\nSimilarly, if a team has its own workers, and their check interval ended up acquiring the lock, a new container will be created on the team's workers, rather than re-using a container from the shared worker pool.\n\nThis is a bit complicated to reason about and we plan to stop re-using check containers to simplify all of this. See 3079 for more information.\n\n","depth":6,"section_tag":"complications-with-reusing-containers"},"component-atc":{"location":"internals.html#component-atc","title":"ATC: web UI \u0026 build scheduler","text":"The ATC is the heart of Concourse. It runs the web UI and API and is responsible for all pipeline scheduling. It connects to PostgreSQL, which it uses to store pipeline data (including build logs).\n\nMultiple ATCs can be running as one cluster; as long as they're all pointing to the same database, they'll synchronize using basic locking mechanisms and roughly spread work across the cluster.\n\nThe ATC by default listens on port 8080, and is usually colocated with the TSA and sitting behind a load balancer.\n\nNote: for fly intercept to function, make sure your load balancer is configured to do TCP or SSL forwarding, not HTTP or HTTPS.\n\nThere are multiple components within the ATC that each have their own set of responsibilities. The main components consist of the checker, scheduler, build tracker and the garbage collector.\n\nThe checker's responsibility is to continously checks for new versions of resources. The scheduler is responsible for scheduling builds for a job and the build tracker is responsible for running any scheduled builds. The garbage collector is the cleanup mechanism for removing any unused or outdated objects, such as containers and volumes.\n\nAll the components in a Concourse deployment can be viewed in the components table in the database as of v5.7.0. The intervals that the components run at can also be adjusted through editing that table, as well as pausing the component from running entirely.\n\n","depth":3,"section_tag":"component-atc"},"component-tsa":{"location":"internals.html#component-tsa","title":"TSA: worker registration \u0026 forwarding","text":"The TSA is a custom-built SSH server that is used solely for securely registering workers with the ATC.\n\nThe TSA only supports two commands: register-worker and forward-worker.\n\nThe register-worker command is used to register a worker directly with the ATC. This should be used if the worker is running in the same (private) network as the ATC.\n\nThe forward-worker command is used to reverse-tunnel a worker's addresses through the TSA and register the forwarded connections with the ATC. This allows workers running in arbitrary networks to register securely, so long as they can reach the TSA. This is much safer than opening the worker up to the outside world.\n\nThe TSA by default listens on port 2222, and is usually colocated with the ATC and sitting behind a load balancer.\n\n","depth":3,"section_tag":"component-tsa"},"concourse-admin":{"location":"user-roles.html#concourse-admin","title":"Concourse Admin","text":"Admin is a special user attribute granted only to owners of the The main team.\n\nAdmins have the ability to administrate teams using fly set-team, fly destroy-team, fly rename-team, etc.\n\nAdmins always have permission to perform any action on any team. You cannot assign actions to the admin role using the --config-rbac flag.\n\n","depth":4,"section_tag":"concourse-admin"},"concourse-cli":{"location":"concourse-cli.html","title":"The concourse CLI","text":"The concourse CLI can be downloaded from the latest GitHub release - make sure to grab the appropriate archive for your platform. Each concourse-* archive contains the following files:\n\nconcourse/bin/concourse\nconcourse/bin/gdn            # on Linux\nconcourse/fly-assets/...\nconcourse/resource-types/... # on Linux\nWhen extracted, the concourse binary will auto-discover its sibling assets based on its file location, so you may extract it anywhere. On Linux a typical install location is /usr/local/concourse:\n\ntar -zxf concourse-*.tgz -C /usr/local\nFrom there, you can either add /usr/local/concourse/bin to your $PATH, or just execute /usr/local/concourse/bin/concourse directly.\n\nConfiguring concourseAll Concourse web node and worker node configuration is defined statically via flags. For a full list of flags, you can pass --help to any command.\n\nEach flag can also be set via an environment variable. The env var for each flag is based on the flag name, preceded with CONCOURSE_. These are also shown in --help.\n\nVarious sections in documentation may refer to configuration via env vars rather than flags, but they are both equivalent and interchangeable. Env vars are just easier to reference in isolation and are more useful to copy-paste.\n\n","depth":3,"section_tag":"concourse-cli"},"concourse-generate-key":{"location":"concourse-generate-key.html","title":"Generating Keys","text":"Concourse's various components use RSA keys to verify tokens and worker registration requests.\n\nA minimal deployment will require the following keys:\n\nsession_signing_key: Used by the Running a web node for signing and verifying user session tokens.\n\n\ntsa_host_key: Used by the Running a web node for the SSH worker registration gateway server (\"TSA\").\n\nThe public key is given to each Running a worker node to verify the remote host when connecting via SSH.\n\n\nworker_key (one per worker): Each Running a worker node verifies its registration with the Running a web node via a SSH key.\n\nThe public key must be listed in the Running a web node's authorized keys configuration in order for the worker to register.\n\n\n\nTo generate these keys, run:\n\nconcourse generate-key -t rsa -f ./session_signing_key\nconcourse generate-key -t ssh -f ./tsa_host_key\nconcourse generate-key -t ssh -f ./worker_key\n...and we'll also start on an authorized_keys file, currently listing this initial worker key:\n\ncp worker_key.pub authorized_worker_keys\n","depth":3,"section_tag":"concourse-generate-key"},"concourse-web":{"location":"concourse-web.html","title":"Running a web node","text":"The web node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.\n\n","depth":3,"section_tag":"concourse-web"},"concourse-worker":{"location":"concourse-worker.html","title":"Running a worker node","text":"The worker node registers with the Running a web node and is then used for executing builds and performing resource checks. It doesn't really decide much on its own.\n\n","depth":3,"section_tag":"concourse-worker"},"config-basics":{"location":"config-basics.html","title":"Config Basics","text":"Concourse configuration for things like Pipelines and Tasks is done through declarative YAML files.\n\nConcourse configuration supports basic variable substitution by way of ((vars)). There is no built-in support for fancier templating constructs, e.g. loops and conditionals; users are free to use whatever templating system they like.\n\n","depth":2,"section_tag":"config-basics"},"configuration":{"location":"php-example.html#configuration","title":"Pipeline Configuration","text":"---\nresources:\n  - name: laravel-websockets-git\n    type: git\n    icon: github-circle\n    source:\n      uri: https://github.com/beyondcode/laravel-websockets.git\n\njobs:\n  - name: test\n    public: true\n    plan:\n      - get: laravel-websockets-git\n        trigger: true\n      - task: run-tests\n        config:\n          platform: linux\n          image_resource:\n            type: registry-image\n            source: { repository: php, tag: 7.2-cli }\n          inputs:\n            - name: laravel-websockets-git\n          run:\n            path: /bin/sh\n            args:\n              - -c\n              - |\n                apt-get update\n                apt-get install -y git unzip\n\n                php -r \"copy('https://getcomposer.org/installer', 'composer-setup.php');\"\n                php -r \"if (hash_file('sha384', 'composer-setup.php') === 'e0012edf3e80b6978849f5eff0d4b4e4c79ff1609dd1e613307e16318854d24ae64f26d17af3ef0bf7cfb710ca74755a') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\"\n                php composer-setup.php --install-dir=/usr/local/bin --filename=composer\n                php -r \"unlink('composer-setup.php');\"\n\n                cd laravel-websockets-git\n\n                composer install\n                vendor/bin/phpunit --coverage-text --coverage-clover=coverage.clover\n","depth":3,"section_tag":"configuration"},"configuring-auth":{"location":"configuring-auth.html","title":"Configuring Auth","text":"The very first thing to configure with Concourse is how users will log in, and what those users should be able to do.\n\nThis is configured in two separate tiers:\n\n* Authentication, how users identify themselves, is configured on the Running a web node.\n\n* Authorization, how user access is determined, is configured on each team.\n\nConcourse currently supports the following auth methods:\n\nAny number of providers can be enabled at any one time. Users will be given a choice when logging in as to which one they would like to use.\n\nConcourse uses a fork of Dex for its authentication. You can find additional documentation on the supported auth providers in the Dex connectors documentation.\n\nAdding a new auth provider to Concourse is as simple as submitting a pull request to our fork of Dex and then adding a bit of configuration to the skymarshal component.\n\n","depth":3,"section_tag":"configuring-auth"},"configuring-gdn-server":{"location":"concourse-worker.html#configuring-gdn-server","title":"Configuring gdn server","text":"On Linux, the concourse binary is packaged alongside a gdn binary. This binary is used for running Guardian, which is a Garden backend implementation which runs containers via runc (the same technology underlying tools like Docker).\n\nThe concourse worker command automatically configures and runs gdn, but depending on the environment you're running Concourse in, you may need to pop open the hood and configure a few things.\n\nThe gdn server can be configured in two ways:\n\n1. By creating a config.ini file and passing it as --garden-config (or CONCOURSE_GARDEN_CONFIG).\n\n  The .ini file should look something like this:\n\n  [server]\n  flag-name = flag-value\n  To learn which flags can be set, consult gdn server --help. Each flag listed can be set under the [server] heading.\n\n2. By setting CONCOURSE_GARDEN_* environment variables.\n\n  This is primarily supported for backwards compatibility, and these variables are not present in concourse web --help. They are translated to flags passed to gdn server by lower-casing the * portion and replacing underscores with hyphens.\n\n","depth":5,"section_tag":"configuring-gdn-server"},"configuring-kubernetes-rbac":{"location":"kubernetes-credential-manager.html#configuring-kubernetes-rbac","title":"Configuring Kubernetes RBAC","text":"As the Web nodes need to retrieve secrets from namespaces that are not their own, they needs extra permissions to do so.\n\nIf you have RBAC enabled, that means creating the necessary Kubernetes objects to identify the Web nodes and give them access to a predefined list of namespaces where the secrets live.\n\nRegardless of how the Kubernetes RBAC-related objects are created, the basic requirement is that web must be able to read secrets in the  namespaces where each teams' secrets live.\n\nFor instance, if you have the following teams which you want to read secrets from:\n\n* team-a\n\n* team-b\n\nAssuming the following Running a web node configuration:\n\nCONCOURSE_KUBERNETES_NAMESPACE_PREFIX=myprefix-\nweb must be able to get secrets from the following namespaces:\n\n* myprefix-team-a\n\n* myprefix-team-b\n\nTo allow web to interpolate credentials for \"team-a\" and \"team-b\", we'd then need to create a few Kubernetes RBAC objects.\n\nStarting with identifying the web service as an actor, we can use a ServiceAccount for that:\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: web\n  labels:\n    app: web\nTo allow actors to do something, in this case, retrieve secrets from a given namespace, a ClusterRole is then needed.\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: read-secrets\n  labels:\n    app: web\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\"]\nAs that role is useless if not bound to an actor, the next step is creating the the object that represents binding the role to the web ServiceAccount that we had created before.\n\nThis is accomplished through the RoleBinding object, which is per-namespace (thus, per-team).\n\nEven though in this example we're binding to a ClusterRole (which is not tied to any namespace), the use of such cluster role is (see metadata.namespace), making the effective  permissions restricted to the namespace applied in the RoleBinding.\n\n---\n# Role binding for the first team (`team-b`), allowing `web`\n# to consume secrets from it.\n#\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: web-team-a\n  namespace: myprefix-team-a\n  labels:\n    app: web\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: read-secrets\nsubjects:\n- kind: ServiceAccount\n  name: web\n  namespace: concourse\n\n---\n# Role binding for the second team (`team-b`), allowing `web`\n# to consume secrets from it.\n#\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: web-team-b\n  namespace: myprefix-team-b\n  labels:\n    app: web\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: read-secrets\nsubjects:\n- kind: ServiceAccount\n  name: web\n  namespace: concourse\nTo finish the example, we need to associate the web Pod with the service, granting the pod access to those namespaces through the roles that have been bound to it.\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      serviceAccountName: web\n      containers:\n        - name: web\n          image: \"concourse/concourse:5.3.0\"\n          args: [ web ]\n          env:\n            - name: CONCOURSE_KUBERNETES_NAMESPACE_PREFIX\n              value: \"myprefix-\"\n          # ...\n","depth":5,"section_tag":"configuring-kubernetes-rbac"},"configuring-ldap-group-search":{"location":"ldap-auth.html#configuring-ldap-group-search","title":"Configuring LDAP group search","text":"The LDAP provider can also be configured with group search configuration, so that users can be configured for team authorization by their 'group' in LDAP.\n\nFor example, to find groups and identify them by their ou attribute, you would configure:\n\nCONCOURSE_LDAP_GROUP_SEARCH_BASE_DN='cn=groups,dc=example,dc=com'\nCONCOURSE_LDAP_GROUP_SEARCH_NAME_ATTR=ou\nThe attributes correlating a user to a group must be specified like so:\n\nCONCOURSE_LDAP_GROUP_SEARCH_USER_ATTR=uid\nCONCOURSE_LDAP_GROUP_SEARCH_GROUP_ATTR=members\nThis specifies that the uid attribute of the user must be present in the members attribute of the group.\n\nAn additional filter may be specified, just like with users:\n\nCONCOURSE_LDAP_GROUP_SEARCH_FILTER='(objectClass=posixGroup)'\n","depth":6,"section_tag":"configuring-ldap-group-search"},"configuring-main-team-authorization":{"location":"generic-oauth.html#configuring-main-team-authorization","title":"Configuring main Team Authorization","text":"OAuth users and groups can be added to the The main team authorization config by setting the following env on the Running a web node:\n\nCONCOURSE_MAIN_TEAM_OAUTH_USER=my-user\nCONCOURSE_MAIN_TEAM_OAUTH_GROUP=my-group\nMultiple users and groups may be specified by comma-separating them.\n\n","depth":6,"section_tag":"configuring-main-team-authorization"},"configuring-metrics":{"location":"metrics.html#configuring-metrics","title":"Configuring Metrics","text":"The Running a web node can be configured to emit metrics on start.\n\nCurrently supported metrics emitters are InfluxDB, NewRelic, Prometheus, and Datadog. There is also a dummy emitter that will just spit the metrics out in to the logs at DEBUG level, which can be enabled with the --emit-to-logs flag.\n\nRegardless of your metrics emitter, you can set CONCOURSE_METRICS_BUFFER_SIZE to determine how many metrics emissions are sent at a time. Increasing this number can be helpful if sending metrics is regularly failing (due to rate limiting or network failures) or if latency is particularly high.\n\nThere are various flags for different emitters; run concourse web --help and look for \"Metric Emitter\" to see what's available.\n\n","depth":4,"section_tag":"configuring-metrics"},"configuring-rbac":{"location":"user-roles.html#configuring-rbac","title":"Configuring RBAC","text":"Configuring RBAC is experimental, and this may change in the future.\n\nIt is possible to promote or demote the roles to which actions are assigned by passing the --config-rbac to the concourse web command with a path to a .yml file, like the following:\n\nconcourse web --config-rbac=/path/to/rbac/config.yml\nThis file should be a YAML map where the keys are role names (owner, member, pipeline-operator, and viewer are valid). For each role, the value should be a list of actions. On startup, Concourse will assign each role to its associated list of actions.\n\nFor example, in the default configuration only pipeline-operators and above can abort builds. To restrict aborting builds to only members and above, you could pass this as a --config-rbac file:\n\nmember:\n- AbortBuild\nOn the other hand, only members and above can order pipelines by default. To extend this privilege down to pipeline-operators, you can use a --config-rbac file like the following:\n\npipeline-operator:\n- OrderPipelines\nYou do not need to specify a role for every possible action; if an action does not appear in the file, then the default role (as described in the sections above) will be assigned to that action. Also, please avoid specifying the same action under multiple roles in this file - it can have unpredictable results.\n\n","depth":4,"section_tag":"configuring-rbac"},"configuring-the-secrets-engine":{"location":"vault-credential-manager.html#configuring-the-secrets-engine","title":"Configuring the secrets engine","text":"Concourse is currently limited to looking under a single path, meaning only one secrets engine is supported: kv, version 1. This may change in the future - we're still collecting ideas in RFC #21.\n\nSo, let's configure the kv secrets engine and mount it at /concourse:\n\n$ vault secrets enable -version=1 -path=concourse kv\nNext, you'll want to create a policy to allow Concourse to read from this path.\n\npath \"concourse/*\" {\n  policy = \"read\"\n}\nSave this to concourse-policy.hcl, and then run:\n\nvault policy write concourse ./concourse-policy.hcl\nThis configuration will allow Concourse to read all credentials under /concourse. This should match your configured path prefix.\n\n","depth":5,"section_tag":"configuring-the-secrets-engine"},"configuring-tracing":{"location":"tracing.html#configuring-tracing","title":"Configuring Tracing","text":"There's only one variable that is required to be set in order to leverage Jaeger's integration with Concourse:\n\nCONCOURSE_TRACING_JAEGER_ENDPOINT=http://jaeger:14268/api/traces\nThis tells Concourse how to target Jaeger's Thrift HTTP endpoint to send the traces to.\n\n","depth":4,"section_tag":"configuring-tracing"},"conjur-account":{"location":"conjur-credential-manager.html#conjur-account","title":"conjur-account","text":"The Conjur account.\n\nEnvironment variable CONCOURSE_CONJUR_ACCOUNT.\n\n","depth":5,"section_tag":"configuration"},"conjur-appliance-url":{"location":"conjur-credential-manager.html#conjur-appliance-url","title":"conjur-appliance-url","text":"URL of the Conjur instance.\n\nEnvironment variable CONCOURSE_CONJUR_APPLIANCE_URL.\n\n","depth":5,"section_tag":"configuration"},"conjur-authn-api-key":{"location":"conjur-credential-manager.html#conjur-authn-api-key","title":"conjur-authn-api-key","text":"The api key that corresponds to the Conjur host username.\n\nEnvironment variable CONCOURSE_CONJUR_AUTHN_API_KEY.\n\n","depth":5,"section_tag":"configuration"},"conjur-authn-login":{"location":"conjur-credential-manager.html#conjur-authn-login","title":"conjur-authn-login","text":"A valid Conjur host username.\n\nEnvironment variable CONCOURSE_CONJUR_AUTHN_LOGIN.\n\n","depth":5,"section_tag":"configuration"},"conjur-authn-token-file":{"location":"conjur-credential-manager.html#conjur-authn-token-file","title":"conjur-authn-token-file","text":"Token file used if Conjur instance is running in k8s or iam. \n\nEnvironment variable CONCOURSE_CONJUR_AUTHN_TOKEN_FILE.\n\n","depth":5,"section_tag":"configuration"},"conjur-cert-file":{"location":"conjur-credential-manager.html#conjur-cert-file","title":"conjur-cert-file","text":"Cert file used if conjur instance is using a self signed cert.\n\nEnvironment variable CONCOURSE_CONJUR_CERT_FILE.\n\n","depth":5,"section_tag":"configuration"},"conjur-credential-manager":{"location":"conjur-credential-manager.html","title":"The Conjur credential manager","text":"","depth":4,"section_tag":"conjur-credential-manager"},"conjur-permissions":{"location":"conjur-credential-manager.html#conjur-permissions","title":"Conjur Permissions","text":"The following is an example Conjur policy that can be used to grant permissions to a Conjur host. In this example host/concourse  will have permissions to read and update all of the secrets within  the TEAM_NAME and PIPELINE_NAME policies.\n\n- !host concourse\n- !policy\n  id: concourse\n  owner: !host concourse\n  body:\n  - !policy \n    id: TEAM_NAME\n    body:\n    - !variable team-secret-variable\n    - !policy\n      id: PIPELINE_NAME\n      body:\n      - !variable pipeline-secret-variable\nNote that the TEAM_NAME and PIPELINE_NAME text above should be replaced to fit your Concourse setup.\n\nFor more information on how to create and load Conjur policies, review the official documentation.\n\n","depth":5,"section_tag":"conjur-permissions"},"conjur-pipeline-secret-template":{"location":"conjur-credential-manager.html#conjur-pipeline-secret-template","title":"conjur-pipeline-secret-template","text":"The base path used when attempting to locate a pipeline-level secret.\n\nEnvironment variable CONCOURSE_CONJUR_PIPELINE_SECRET_TEMPLATE.\n\nExample:\n\nDefault: concourse/{{.Team}}/{{.Pipeline}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"conjur-secret-template":{"location":"conjur-credential-manager.html#conjur-secret-template","title":"conjur-secret-template","text":"The base path used when attempting to locate a vault or safe level secret.\n\nEnvironment variable CONCOURSE_CONJUR_SECRET_TEMPLATE.\n\nExample:\n\nDefault: vaultName/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"conjur-team-secret-template":{"location":"conjur-credential-manager.html#conjur-team-secret-template","title":"conjur-team-secret-template","text":"The base path used when attempting to locate a team-level secret.\n\nEnvironment variable CONCOURSE_CONJUR_TEAM_SECRET_TEMPLATE.\n\nExample:\n\nDefault: concourse/{{.Team}}/{{.Secret}}\n\n","depth":5,"section_tag":"configuration"},"container-placement":{"location":"container-placement.html","title":"Container Placement","text":"Each step in a build is executed inside a container. The Running a web node distributes containers across the worker cluster depending on the configured strategy.\n\n","depth":3,"section_tag":"container-placement"},"contribute":{"location":"contribute.html","title":"Contribute","text":"Concourse is a free and Open Source software project that relies on the contributions of sponsors and volunteers from around the world. As a growing community of continuous thing-doers, the team is always in need of more people to help out in our community.\n\nEven if you're just getting started with Concourse you can contribute in a few ways:\n\n* Discuss and Share your experiences with Concourse in our Discord forums\n\n* Blog about Concourse. We would love to hear how you got started with Concourse, the type of pipelines you're building, or any tips \u0026 tricks that you can share with the community.\n\n* Answer questions about Concourse in the Support section of our forums or on Stack Overflow\n\nAs you become more comfortable with Concourse, you might be interested in contributing to Concourse's development: * Review issues and report bugs in the concourse/concourse repo. We are by no means experts in every subject area; it sometimes takes us while to understand a problem space well enough to figure out things fit into Concourse's puritanical world. You can help us by: * Voting for issues by adding an emoji reaction to the issue\n\n  * Submitting new bugs when you run into a new problem with Concourse\n\n  * Review GitHub issues submitted by other members of the community; ask for (or provide!) clarification when necessary. A lot of the issues that come in are simply unclear and we end up spending a lot of time on clarifying issues.\n\n* Writing Documentation for the project. You can get started by reviewing the docs code at concourse/docs and making PRs against the project.\n\n* Contributing Code. If you're interested in contributing some work to the core project, you can get started by reviewing the CONTRIBUTING.md getting started guide. You can also get an overview of the Concourse internals by reviewing some of the documentation under Concourse Internals. If you're curious about what we're working on you can follow along with the team's progress on our public projects page.\n\n\n\nThe Concourse project is committed to fostering an open and welcoming environment for all project members, maintainers and community members. The Concourse project pledges to make our community  a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. You can read more about our commitment in the project's Contributor Code of Conduct\n\n","depth":2,"section_tag":"contribute"},"credential-lookup-rules":{"location":"conjur-credential-manager.html#credential-lookup-rules","title":"Credential Lookup Rules","text":"When resolving a parameter such as ((foo_param)), Concourse will look in the following paths, in order:\n\n* concourse/TEAM_NAME/PIPELINE_NAME/foo_param\n\n* concourse/TEAM_NAME/foo_param\n\n* vaultName/foo_param\n\nThe leading concourse can be changed by specifying --conjur-pipeline-secret-template or --conjur-team-secret-template variables.\n\nThe leading vaultName can be changed by specifying --conjur-secret-template variable.\n\n","depth":5,"section_tag":"credential-lookup-rules"},"credhub-credential-manager":{"location":"credhub-credential-manager.html","title":"The CredHub credential manager","text":"","depth":4,"section_tag":"credhub-credential-manager"},"creds":{"location":"creds.html","title":"Credential Management","text":"Going beyond Encryption, explicit credential management will provide credentials to your builds for a brief amount of time, without being persisted anywhere. It also allows for credentials to be rotated and managed external to the pipeline or team, and prevents them from being revealed by fly get-pipeline.\n\nCredential management works by replacing the credentials with ((vars)) in your pipeline or task config. When the Concourse is about to run the step or check that is configured with vars, it will resolve them by fetching the values from the credential manager. If the values are not present, the action will error.\n\nThe following configurations can be parameterized with a credential manager:\n\n* resourcesource under pipelineresources\n\n* resource_typesource under pipelineresource_types\n\n* resourcewebhook_token under pipelineresources\n\n* task step params on a task step in a pipeline\n\n* Tasks in their entirety - whether from task step file or task step config in a pipeline, or a config executed with Running tasks with fly execute\n\nWhere these values are looked up and how the credential manager is configured depends on the backend. Consult the relevant section below for whichever backend you want to use.\n\n","depth":3,"section_tag":"creds"},"creds-caching":{"location":"creds-caching.html","title":"Caching credentials","text":"By default, credentials are fetched each time they're used. When many pipelines are configured this can result in a ton of requests to the credential server.\n\nTo reduce load on your credential server you may want to enable caching by setting the following env on the Running a web node:\n\nCONCOURSE_SECRET_CACHE_ENABLED=true\nBy default, credentials will be cached for one minute at a time. This value can be increased to further reduce load on the server like so:\n\nCONCOURSE_SECRET_CACHE_DURATION=5m # increase from 1m default\nCredential cache duration can also determined by the credential manager itself - for example, if Vault returns a lease duration for a credential, the shorter value between the configured cache duration and the credential's lease duration will be used.\n\nBy default, the absence of a credential is also cached for 10 seconds so that Concourse doesn't keep looking for a misconfigured credential. This duration can be configured like so:\n\nCONCOURSE_SECRET_CACHE_DURATION_NOTFOUND=1s # decrease from 10s default\n","depth":4,"section_tag":"creds-caching"},"creds-redacting":{"location":"creds-redacting.html","title":"Redacting credentials","text":"Concourse can be configured to automatically redact credentials from build output like so:\n\nCONCOURSE_ENABLE_REDACT_SECRETS=true\nThis behavior is off by default as there is likely a CPU performance overhead on the Running a web nodes involved with enabling it. It will be on by default once we've confirmed that it performs well enough at large scale.\n\nWhen enabled, Concourse will keep track of the credential values which were used in a build. When writing build logs to the database, it will replace any occurrence of these values with the text ((redacted)).\n\nSay you're running a task which runs the following script:\n\nset -e -u -x\n\necho $SECRET \u003e some-file\nsha1sum some-file\n(Note the set -x - the root cause of many accidental credential leaks.)\n\nLet's say you have a job which runs this task, providing the $SECRET parameter using a credential manager ((var)):\n\nplan:\n- task: use-secret\n  file: # ...\n  params: {SECRET: ((some-var))}\nWith hello in some-var, this will result in the following build output:\n\n+ echo ((redacted))\n+ sha1sum some-file\nf572d396fae9206628714fb2ce00f72e94f2258f  some-file\nGoing a step further, what happens when that var has multiple lines of output, like \"hello\\ngoodbye\"?\n\n+ echo ((redacted)) ((redacted))\n+ sha1sum some-file\n638e5ebcd06a5208906960aa5fbe1d4ebd022771  some-file\nWhat happened here? Well, because we didn't quote the $SECRET var arg to echo, it squashed the lines together into arguments. This could have confused our redacting logic and resulted in leaking the credential, but because Concourse redacts secret values line-by-line, we're still OK. This will also help with JSON marshalled credential values, which get interspersed with \\n in a string literal.\n\nAlthough Concourse tries to be thorough in its redacting of credentials, the best way to prevent credential leakage is to not accidentally print them in the first place. Think of this as an airbag, not a seatbelt!\n\n","depth":4,"section_tag":"creds-redacting"},"creds-retry-logic":{"location":"creds-retry-logic.html","title":"Retrying failed fetches","text":"When a request to the credential manager fails due to an intermittent error (e.g. a timeout or connection refused), Concourse will automatically try the request again up to 5 times before giving up. After all attempts fail, the error will be surfaced in the UI for the resource check or build step that initiated the request.\n\nThe retry logic can be configured by specifying the following env on the Running a web node:\n\nCONCOURSE_SECRET_RETRY_ATTEMPTS=5   # how many times to try\nCONCOURSE_SECRET_RETRY_INTERVAL=10s # how long to wait between attempts\n","depth":4,"section_tag":"creds-retry-logic"},"current-caveats-with-rerunning":{"location":"builds.html#current-caveats-with-rerunning","title":"Current caveats with rerunning","text":"The current implementation of re-running is an early iteration with one key limitation: a rerun build will use the current state of the job config, instead of running the exact build plan the original build ran with.\n\nThis means that if the jobplan has changed in a way that is backwards-incompatible, the rerun build may error. For example, if a new input is added, its version will not be available as the original build did not use it.\n\nThere are future plans to have reruns execute the exact build plan from the original build. If you are interested in tracking the progress for the second pass at rerunning builds - or contributing yourself! - the project epic is called Build Lifecycle View.\n\n","depth":4,"section_tag":"current-caveats-with-rerunning"},"dashboard":{"location":"observation.html#dashboard","title":"The Dashboard","text":"The dashboard, available at the default route (/), provides a bird's-eye view of the Concourse cluster. All visible pipelines across all teams are listed here. A \"HD view\" is available at /hd.\n\n","depth":3,"section_tag":"dashboard"},"db-prerequisites":{"location":"postgresql-node.html#db-prerequisites","title":"Prerequisites","text":"PostgreSQL 9.5 or above is required, though the latest available version is recommended.\n\n","depth":4,"section_tag":"db-prerequisites"},"db-resource-utilization":{"location":"postgresql-node.html#db-resource-utilization","title":"Resource utilization","text":"CPU usage: this is one of the most volatile metrics, and one we try pretty hard to keep down. There will be near-constant database queries running, and while we try to keep them very simple, there is always more work to do. Expect to feed your database with at least a couple cores, ideally four to eight. Monitor this closely as the size of your deployment and the amount of traffic it's handling increases, and scale accordingly.\n\nMemory usage: similar to CPU usage, but not quite as volatile.\n\nDisk usage: pipeline configurations and various bookkeeping metadata for keeping track of jobs, builds, resources, containers, and volumes. In addition, all build logs are stored in the database. This is the primary source of disk usage. To mitigate this, users can configure jobbuild_logs_to_retain on a job, but currently there is no operator control for this setting. As a result, disk usage on the database can grow arbitrarily large.\n\nBandwidth usage: well, it's a database, so it most definitely uses the network. Something important to consider here is the number of simultaneous connections that the database server itself will allow. Postgres exposes a max_connections configuration variable, and depending on how many web nodes you are running and the size of their connection pool, you may need to tune these two numbers against each other.\n\nHighly available: up to you. Clustered PostgreSQL is kind of new and probably tricky to deploy, but there are various cloud solutions for this.\n\nHorizontally scalable: I...don't think so?\n\nOutbound traffic:\n\n* none\n\nInbound traffic:\n\n* only ever from the web node\n\n","depth":5,"section_tag":"db-resource-utilization"},"db-running":{"location":"postgresql-node.html#db-running","title":"Running PostgreSQL","text":"How this node is managed is up to you; Concourse doesn't actually have much of an opinion on it, it just needs a database.\n\nHow to install PostgreSQL is really dependent on your platform. Please refer to your Linux distribution or operating system's documentation.\n\nFor the most part, the instruction on Linux should look something like this:\n\nsudo apt install postgresql\nsudo su postgres -c \"createuser $(whoami)\"\nsudo su postgres -c \"createdb --owner=$(whoami) atc\"\nThis will install PostgreSQL (assuming your distro uses apt), create a user, and create a database that the current UNIX user can access, assuming this same user is going to be running the Running a web node. This is a reasonable default for distros like Ubuntu and Debian which default PostgreSQL to peer auth.\n\n","depth":4,"section_tag":"db-running"},"disable-version":{"location":"resource-versions.html#disable-version","title":"Disabling a Version","text":"A resource version can also be disabled through the web UI on the resource version history page. These disabled versions will not be used to schedule any further builds for any jobs that use the resource as an input.\n\nDisabled versions can also be re-enabled through the resource version history page.\n\nDisabling a version is useful for cases where you know that the version is broken or incompatible.\n\n","depth":4,"section_tag":"disable-version"},"disabling-encryption":{"location":"encryption.html#disabling-encryption","title":"Disabling Encryption","text":"To opt out of encryption entirely (I'm sure you have your reasons), simply pass --old-encryption-key (or old_encryption_key) alone. With no new encryption key, the Running a web node will decrypt all existing data on start.\n\n","depth":4,"section_tag":"disabling-encryption"},"do-step":{"location":"jobs.html#do-step","title":"do step","text":"","depth":3,"section_tag":"steps"},"docs":{"location":"docs.html","title":"Docs","text":"Concourse is a pipeline-based continuous thing-doer.\n\nThe word \"pipeline\" is all the rage in CI these days, so being more specific about this term is kind of important; Concourse's pipelines are significantly different from the rest.\n\nPipelines are built around Resources, which represent all external state, and Jobs, which interact with them. Concourse pipelines represent a dependency flow, kind of like distributed Makefiles. Pipelines are designed to be self-contained so as to minimize server-wide configuration. Maximizing portability also mitigates risk, making it easier for projects to recover from CI disasters.\n\nResources like the git resource and s3 resource are used to express source code, dependencies, deployments, and any other external state. This interface is also used to model more abstract things like scheduled or interval triggers, via the time resource.\n\nResource Types are defined as part of the pipeline itself, making the pipelines more self-contained and keeping Concourse itself small and generic without resorting to a complicated plugin system.\n\nJobs are sequences of get, put, and task steps to execute. These steps determine the job's inputs and outputs. Jobs are designed to be idempotent and loosely coupled, allowing the pipeline to grow with the project's needs without requiring engineers to keep too much in their head at a time.\n\nEverything in Concourse runs in a container. Instead of modifying workers to install build tools, Tasks describe their own container image (typically using Docker images via the registry-image resource).\n\n...What?Concourse admittedly has a steeper learning curve at first, and depending on your background it might be a lot to take in. A core goal of this project is for the curve to flatten out shortly after and result in higher productivity and less stress over time.\n\nIf this all sounds like gobbeldigook, that's OK - you may want to just continue on, start kicking the tires a bit, and use the above as a quick reference of the \"big picture\" as the mental model sets in.\n\n","depth":1,"section_tag":"docs"},"downgrading":{"location":"concourse-web.html#downgrading","title":"Downgrading","text":"If you're stuck in a pinch and need to downgrade from one version of Concourse to another, you can use the concourse migrate command.\n\nNote: support for down migrations is a fairly recent addition to Concourse; it is not supported for downgrading to v3.6.0 and below.\n\nFirst, grab the desired migration version by running the following:\n\n# make sure this is the *old* Concourse binary\n$ concourse migrate --supported-db-version\n1551110547\nThat number (yours will be different) is the expected migration version for that version of Concourse.\n\nNext, run the following with the new Concourse binary:\n\n$ concourse migrate --migrate-db-to-version=1551110547\nThis will need the same CONCOURSE_POSTGRES_* configuration described in Running concourse web.\n\nOnce this completes, switch all web nodes back to the older concourse binary and you should be good to go.\n\n","depth":5,"section_tag":"downgrading"},"dummy-var-source":{"location":"vars.html#dummy-var-source","title":"dummy var source","text":"","depth":4,"section_tag":"var-sources"},"dynamic-vars":{"location":"vars.html#dynamic-vars","title":"Dynamic vars","text":"Concourse can read values from \"var sources\" - typically credential managers like Vault - at runtime. This keeps them out of your configuration and prevents sensitive values from being stored in your database. Values will be read from the var source and optionally cached to reduce load on the var source.\n\nThe following attributes can be parameterized through a var source:\n\n* resourcesource under pipelineresources\n\n* resource_typesource under pipelineresources\n\n* resourcewebhook_token under pipelineresources\n\n* task step params on a task step in a pipeline\n\n* Tasks in their entirety - whether from task step file or task step config in a pipeline, or a config executed with Running tasks with fly execute\n\nConcourse will fetch values for vars as late as possible - i.e. when a step using them is about to execute. This allows the credentials to have limited lifetime and rapid rotation policies.\n\n","depth":3,"section_tag":"dynamic-vars"},"enabling-encryption":{"location":"encryption.html#enabling-encryption","title":"Enabling Encryption","text":"To enable encryption, you'll just need to come up with a 16 or 32-byte random character sequence and configure it as --encryption-key flag to the web command. For BOSH, this is the encryption_key property.\n\nOn startup, the Running a web node will encrypt all existing plaintext data, and any new data being written will be encrypted before it's sent over the network to the database.\n\nThe initial bulk encryption shouldn't take too long, but it will scale linearly with the amount of data that you have, and if another ATC is running it'll suddenly not be able to read the data until it's also given the key. So, expect some downtime.\n\n","depth":4,"section_tag":"enabling-encryption"},"encryption":{"location":"encryption.html","title":"Encryption","text":"Automating everything means authorizing something to automate many things. This makes CI systems a high-risk target for security leaks.\n\nConcourse pipelines are loaded with credentials: resources are configured with private keys, tasks are given credentials to servers they integrate via credential manager variables, task step vars, or task step params, etc. If someone gets their hands on your config, they have access to everything.\n\nTo mitigate this, Concourse supports encrypting sensitive information before it reaches the database. This way the plaintext credentials only exist in memory for as long as they need to, and if someone gains access to your database, they can't so easily gain the keys to the kingdom.\n\nWe strongly encourage anyone running Concourse to configure encryption. Going further, it's best to have Concourse not store the credentials in the first place, in which case you may want to configure credential management as well.\n\n","depth":3,"section_tag":"encryption"},"examples":{"location":"examples.html","title":"Examples","text":"Configuring self-contained Concourse pipelines is a great way to try things out before diving into the deeper content.\n\nEach example contains a pipeline YAML snippet which can be copy-pasted to a local file and configured on your instance via fly set-pipeline. From there you may want to poke around and try changing parts of the configuration to learn how things work. All the available knobs to turn are covered in the Docs.\n\nFor a real-world example, check out Concourse's own pipeline (and its config):\n\n","depth":1,"section_tag":"examples"},"exposing":{"location":"exposing.html","title":"Pipeline \u0026 Build Visibility","text":"Every newly configured pipeline is hidden to anyone but the pipeline's team. To make a pipeline publicly viewable, both by other teams and unauthenticated users, see fly expose-pipeline.\n\nEven with a pipeline exposed, all build logs are hidden by default. This is because CI jobs are prone to leaking credentials and other...unsavory information. After you've determined that a job's builds should be safe for public consumption, you can set public: true on the job in your pipeline.\n\n","depth":3,"section_tag":"exposing"},"fewer-resource-checks-to-perform":{"location":"global-resources.html#fewer-resource-checks-to-perform","title":"Fewer resource checks to perform","text":"With global resources, all resources that have the same configuration will share the same version history and share only one checking interval. This reduces load on the worker and on the external services that the resources point to.\n\nFor example, prior to global resources if there were three resources with the same configuration between three team's pipelines it would result in three check containers performing three resource checks every minute to fetch the versions.\n\nWith global resources, this configuration will result in only one check container and one resource check every minute to fetch versions for all the resources.\n\nSince there will be only one resource check for all resources that have the same configuration, the resource that has the shortest resourcecheck_every configured will result in its pipeline running the checks for that resource configuration.\n\n","depth":5,"section_tag":"fewer-resource-checks-to-perform"},"fewest-build-containers-strategy":{"location":"container-placement.html#fewest-build-containers-strategy","title":"The fewest-build-containers strategy","text":"When using the fewest-build-containers strategy, step containers are placed on the worker that has the fewest build containers (i.e. containers for other steps of other builds).\n\nTo use this strategy, set the following env var on the Running a web node:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=fewest-build-containers\n","depth":4,"section_tag":"fewest-build-containers-strategy"},"fly":{"location":"fly.html","title":"The fly CLI","text":"The first step to getting started with Concourse is to install the fly CLI tool. You can download fly from any Concourse installation. There are download links for common platforms in the bottom right hand corner of the main page.\n\nThroughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult fly -h to learn the short forms.\n\n","depth":2,"section_tag":"fly"},"fly-abort-build":{"location":"builds.html#fly-abort-build","title":"fly abort-build","text":"To abort a build of a job, run:\n\n$ fly -t example abort-build --job my-pipeline/my-job --build 3\nThis will cancel build 3 of the my-job job in the my-pipeline pipeline.\n\n","depth":3,"section_tag":"fly-abort-build"},"fly-active-users":{"location":"managing-teams.html#fly-active-users","title":"fly active-users","text":"To list all users that have logged into your instance in the last two months, run:\n\n$ fly -t example active-users\nThe output will include the username, connector (which method they used to authenticate) and the date of their last login.\n\nYou can list users whose last login was within a different range by using:\n\n$ fly -t example active-users --since yyyy-MM-dd\nThis can be helpful to get a sense of how active your cluster is. \n\n","depth":4,"section_tag":"fly-active-users"},"fly-builds":{"location":"builds.html#fly-builds","title":"fly builds","text":"To list the most recent builds, run:\n\n$ fly -t example builds\nTo list the builds of a job, run:\n\n$ fly -t example builds -j pipeline-name/job-name\nThis can be useful for periodically monitoring the state of a job. The output also works well with tools like awk and grep.\n\nBy default the most recent 50 builds are shown. To see more builds, use the -c flag, like so:\n\n$ fly -t example builds -c 100\n","depth":3,"section_tag":"fly-builds"},"fly-check-resource":{"location":"managing-resources.html#fly-check-resource","title":"fly check-resource","text":"To force immediate checking for new versions of a resource, rather than waiting for the periodic checking, run:\n\n$ fly -t example check-resource --resource my-pipeline/my-resource\nTo check from a particular version, including the given version, append the --from flag like so:\n\n$ fly -t example check-resource --resource my-pipeline/my-resource \\\n    --from ref:abcdef\nThis can be useful for collecting versions that are older than the current ones, given that a newly configured resource will only start from the latest version.\n\nNote the ref: prefix is resource-dependent. For example, the bosh-io-release resource might use version:11.2 in place of ref:abcdef.\n\n","depth":4,"section_tag":"fly-check-resource"},"fly-check-resource-type":{"location":"managing-resource-types.html#fly-check-resource-type","title":"fly check-resource-type","text":"To force immediate checking for new versions of a resource type, rather than waiting for the periodic checking, run:\n\n$ fly -t example check-resource-type --resource-type my-pipeline/my-resource-type\nThis can be useful for forcing an update if you're iterating on your own resource type implementation.\n\n","depth":4,"section_tag":"fly-check-resource-type"},"fly-clear-task-cache":{"location":"jobs.html#fly-clear-task-cache","title":"fly clear-task-cache","text":"If you've got a task cache that you need to clear out for whatever reason, this can be done like so:\n\n$ fly -t example clear-task-cache --job my-pipeline/my-job --step my-step-name\nThis will immediately invalidate the caches - they'll be garbage collected asynchronously and subsequent builds will run with empty caches.\n\nYou can also clear out a particular path for the given step's cache, using --cache-path:\n\n$ fly -t example clear-task-cache \\\n    --job my-pipeline/my-job \\\n    --step my-step-name \\\n    --cache-path go/pkg\nIf --cache-path is not specified, all caches for the given step will be cleared.\n\n","depth":4,"section_tag":"fly-clear-task-cache"},"fly-cli":{"location":"fly.html","title":"The fly CLI","text":"The first step to getting started with Concourse is to install the fly CLI tool. You can download fly from any Concourse installation. There are download links for common platforms in the bottom right hand corner of the main page.\n\nThroughout the Concourse documentation we'll stick to the long-form name of every command and flag. Once you've learned what the commands do, you may want to consult fly -h to learn the short forms.\n\n","depth":2,"section_tag":"fly"},"fly-completion":{"location":"fly.html#fly-completion","title":"fly completion","text":"Fly can output autocomplete configuration for some shells. For example, you can add an entry to your .bashrc like this:\n\nsource \u003c(fly completion --shell bash)\nor, using the /etc/bash_completion.d directory:\n\n$ fly completion --shell bash \u003e /etc/bash_completion.d/fly\nNote that, unlike other fly commands, this command does not interact with a remote server so you do not need to provide the -t or --target flag.\n\n","depth":3,"section_tag":"fly-completion"},"fly-containers":{"location":"administration.html#fly-containers","title":"fly containers","text":"To list the active containers across all your workers, run:\n\n$ fly -t example containers\nThis can be useful when discovering the containers available for fly intercepting.\n\n","depth":4,"section_tag":"fly-containers"},"fly-curl":{"location":"administration.html#fly-curl","title":"fly curl","text":"To execute an arbirary API request, you can run something like the following:\n\n$ fly -t example curl /api/v1/info\nThis command is just a shim that runs curl under the hood. To pass flags to curl, pass a -- argument after the path so that fly can distinguish them from its own flags:\n\n$ fly -t example curl /api/v1/builds -- \\\n    -X PUT \\\n    -H \"Content-type: application/json\" \\\n    -d @plan.json\nNote: if you use this command the assumption is that you know what you're doing. If you find yourself using this command often, let us know - perhaps there's a missing command!\n\n","depth":4,"section_tag":"fly-curl"},"fly-delete-target":{"location":"fly.html#fly-delete-target","title":"fly delete-target","text":"When logging out just isn't enough, a target can be completely removed from ~/.flyrc by running:\n\n$ fly -t example delete-target\nTo delete all targets, run:\n\n$ fly delete-target -a\nNote: These two variations are mutually exclusive. If the target parameter -t and all parameter -a are both specified, an error will occur.\n\n","depth":3,"section_tag":"fly-delete-target"},"fly-destroy-pipeline":{"location":"managing-pipelines.html#fly-destroy-pipeline","title":"fly destroy-pipeline","text":"Every now and then you just don't want a pipeline to be around anymore. Running fly destroy-pipeline will stop the pipeline activity and remove all data collected by the pipeline, including build history and collected versions.\n\nFor example, to destroy the my-pipeline pipeline, you would run:\n\n$ fly -t example destroy-pipeline --pipeline my-pipeline\n","depth":4,"section_tag":"fly-destroy-pipeline"},"fly-destroy-team":{"location":"managing-teams.html#fly-destroy-team","title":"fly destroy-team","text":"To remove a team, including all of its pipelines and one-off builds, first log in as the The main team, and then run:\n\n$ fly -t example destroy-team --team-name my-team\nCurrently, if there were any workers assigned specifically to this team, they'll be orphaned, without having their containers or volumes cleaned up.\n\n","depth":4,"section_tag":"fly-destroy-team"},"fly-disable-resource-version":{"location":"managing-resources.html#fly-disable-resource-version","title":"fly disable-resource-version","text":"To disable a specific version of a resource, run:\n\n$ fly -t example disable-resource-version --resource my-pipeline/my-resource \\\n    --version ref:bceaf\nNote that the version needs to be provided as a key-value pair. For the git resource the ref: prefix is used while the registry  resource might use digest as a prefix like digest:sha256:94be7d7b.\n\nThis command is idempotent. Disabling an already disabled resource version will do nothing.\n\nYou can also disable a resource version via the UI by clicking on the check mark button next to the desired version on the resource page.\n\n","depth":4,"section_tag":"fly-disable-resource-version"},"fly-edit-target":{"location":"fly.html#fly-edit-target","title":"fly edit-target","text":"To modify a target's name, team, or URL, run:\n\n$ fly -t example edit-target \\\n    --target-name new-name \\\n    --concourse-url https://ci.example.com \\\n    --team-name my-team\nEach flag is optional - only the specified flags will be changed.\n\n","depth":3,"section_tag":"fly-edit-target"},"fly-enable-resource-version":{"location":"managing-resources.html#fly-enable-resource-version","title":"fly enable-resource-version","text":"To enable a specific version of a resource, run:\n\n$ fly -t example enable-resource-version --resource my-pipeline/my-resource \\\n    --version ref:bceaf\nNote that the version needs to be provided as a key-value pair. For the git resource the ref: prefix is used while the registry  resource might use digest as a prefix like digest:sha256:94be7d7b.\n\nThis command is idempotent. Enabling an already enabled resource version will do nothing.\n\nYou can also enable a resource version via the UI by clicking on the check mark button next to the desired version on the resource page.\n\n","depth":4,"section_tag":"fly-enable-resource-version"},"fly-execute":{"location":"tasks.html#running-tasks","title":"Running tasks with fly execute","text":"One of the most common use cases of fly is taking a local project on your computer and submitting it up with a task configuration to be run inside a container in Concourse. This is useful to build Linux projects on OS X or to avoid all of those debugging commits when something is configured differently between your local and remote setup.\n\nYou can execute a task like this:\n\n$ fly -t example execute --config tests.yml\nYour files will be uploaded and the task will be executed with them. The working directory name will be used as the input name. If they do not match, you must specify -i name=. instead, where name is the input name from the task configuration.\n\nFly will automatically capture SIGINT and SIGTERM and abort the build when received. This allows it to be transparently composed with other toolchains.\n\nBy default, Fly will not send extra files or large files in your current directory that would normally be ignored by your version control system. You can use the --include-ignored flags in order to send ignored files to Concourse along with those that are not ignored.\n\nIf your task needs to run as root then you can specify the -p or --privileged flag.\n\nTasks in Concourse can take multiple inputs. Up until now we've just been submitting a single input (our current working directory) that has the same name as the directory.\n\nTasks must specify the inputs that they require as taskinputs. For fly to upload these inputs you can use the -i or --input arguments with name and path pairs. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --input stemcells=../stemcells\nThis would work together with a build-stemcell.yml if its inputs: section was as follows:\n\ninputs:\n- name: code\n- name: stemcells\nIf you specify an input then the default input will no longer be added automatically and you will need to explicitly list it (as with the code input above).\n\nThis feature can be used to mimic other resources and try out combinations of input that would normally not be possible in a pipeline.\n\nIf the --inputs-from flag is given, the specified job will be looked up in the pipeline, and the one-off build will base its inputs on those currently configured for the job.\n\nIf any --input flags are given (see above), they will override the base set of inputs.\n\nFor example:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --input foo=./foo\nThis will trigger a one-off-build using the task.yml task config, basing its inputs on the latest candidates for the integration job in the main pipeline, with the foo input overridden to specify local code to run.\n\nThis can be used to more closely replicate the state in CI when weeding out flakiness, or as a shortcut for local development so that you don't have to upload every single resource from your local machine.\n\nWhen using --inputs-from as above, you can additionally specify which input to use as the task's image by passing --image input-name.\n\nFor example, the following pipeline fetches an image via a get step and uses it for task step image:\n\nresources:\n- name: my-repo\n  type: git\n  source: {uri: https://example.com}\n\n- name: some-image\n  type: registry-image\n  source: {repository: ubuntu}\n\njobs:\n- name: integration\n  plan:\n  - get: my-repo\n  - get: some-image\n  - task: my-task\n    file: my-repo/task.yml\n    image: some-image\n...so to run the same task with the same image in a one-off build, you would run:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --image some-image\nIf a task specifies outputs then you're able to extract these back out of the build and back to your local system. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --output stemcell=/tmp/stemcell\nThis would work together with a build-stemcell.yml if its outputs: section was as follows:\n\noutputs:\n- name: stemcell\nThis feature is useful to farm work out to your Concourse server to build things in a repeatable manner.\n\nAny params listed in the task configuration can be specified by using environment variables.\n\nSo, if you have a task with the following params:\n\nparams:\n  FOO: fizzbuzz\n  BAR:\n...and you run:\n\nBAR=hello fly execute\nThe task would then run with BAR as \"hello\", and FOO as \"fizzbuzz\" (its default value).\n\nTask config files can contain Vars which can can be set during fly execute by using the -v, -y and -l flags:\n\nfly -t example execute --config tests.yml \\\n  -l vars.yml \\\n  -v some_string=\"Hello World!\" \\\n  -y some_bool=true\nAny variables not satisfied via the above flags will be deferred to the configured credential manager.\n\nTo satisfy these vars when running the task in a pipeline, see task step vars.\n\nIf you want to execute a task on a worker that has a specific tag, you can do so by passing --tag:\n\nfly -t example execute --config task.yml --tag bar\nThis will execute the task specified by task.yml on a worker that has been tagged bar.\n\n","depth":3,"section_tag":"running-tasks"},"fly-expose-pipeline":{"location":"managing-pipelines.html#fly-expose-pipeline","title":"fly expose-pipeline","text":"By default, newly configured pipelines are only visible to the pipeline's team. To make a pipeline viewable by other teams and unauthenticated users, run:\n\n$ fly -t example expose-pipeline --pipeline my-pipeline\nThis feature is useful if you're using Concourse for an open source project and you'd like your community to be able to see into your build pipeline.\n\nTo undo this change, see fly hide-pipeline.\n\nExposing a pipeline reveals basically everything except for build output and resource metadata.\n\nTo expose a resource's metadata, resourcepublic must be set to true.\n\nTo expose a job's build output, jobpublic must be set to true. This will also reveal resource metadata for any get step or put steps in the build output.\n\n","depth":4,"section_tag":"fly-expose-pipeline"},"fly-format-pipeline":{"location":"setting-pipelines.html#fly-format-pipeline","title":"fly format-pipeline","text":"To format a pipeline config in a \"canonical\" form (i.e. keys are in normal order, with name first for example), run:\n\n$ fly format-pipeline --config pipeline.yml\nThis will print the formatted pipeline config to stdout. To update the file in-place, pass --write/-w.\n\n","depth":4,"section_tag":"fly-format-pipeline"},"fly-get-pipeline":{"location":"managing-pipelines.html#fly-get-pipeline","title":"fly get-pipeline","text":"Fly can be used to fetch and update the configuration for your pipelines. This is achieved by using the fly get-pipeline and fly set-pipeline commands. For example, to fetch the current configuration of your my-pipeline Concourse pipeline and print it on STDOUT run the following:\n\n$ fly -t example get-pipeline --pipeline my-pipeline\nTo get JSON instead of YAML you can use the -j or --json argument. This can be useful when inspecting your config with jq.\n\n","depth":4,"section_tag":"fly-get-pipeline"},"fly-get-team":{"location":"managing-teams.html#fly-get-team","title":"fly get-team","text":"To show a team's configuration, run: $ fly -t example get-team -n some-team\n\n\n","depth":4,"section_tag":"fly-get-team"},"fly-hide-pipeline":{"location":"managing-pipelines.html#fly-hide-pipeline","title":"fly hide-pipeline","text":"If you realize that you've made a terrible mistake in exposing your pipeline, you can run:\n\n$ fly -t example hide-pipeline --pipeline my-pipeline\nIf you're panicking you can run the command's short form, hp, instead.\n\n","depth":4,"section_tag":"fly-hide-pipeline"},"fly-intercept":{"location":"builds.html#fly-intercept","title":"fly intercept","text":"Sometimes it's helpful to be on the same machine as your tasks so that you can profile or inspect them as they run or see the state the machine at the end of a run. Due to Concourse running tasks in containers on remote machines this would typically be hard to access.\n\nTo this end, there is a fly intercept command that will give you an interactive shell inside the specified container. Containers are identified by a few things, so you may need to specify a few flags to hone down the results. If there are multiple containers that the flags could refer to, an interactive prompt will show up allowing you to disambiguate.\n\nFor example, running the following will run a task and then enter the finished task's container:\n\n$ fly -t example execute\n$ fly -t example intercept --step build\nWhen intercepting a task running on a Windows worker, you will need to specifically tell fly to to run powershell:\n\n$ fly -t example intercept powershell\nContainers are around for a short time after a build finishes in order to allow people to intercept them.\n\nYou can also intercept builds that were run in your pipeline. By using --job, --build, and --step you can intercept a specific step from a build of a job in your pipeline. These flags also have short forms, like so:\n\n$ fly -t example intercept -j some-pipeline/some-job -b some-build -s some-step\nNote that --build can be omitted, and will default to the most recent build of the job. One-off builds can be reached by passing in their build ID to --build which can be found on the build list page.\n\nThe --step flag can also be omitted; this will let you pick the step interactively if you don't know the exact name.\n\nResource checking containers can also be intercepted with --check or -c:\n\n$ fly -t example intercept --check some-pipeline/some-resource\nA specific command can also be given, e.g. fly intercept ps auxf or fly intercept htop. This allows for patterns such as watch fly intercept ps auxf, which will continuously show the process tree of the current build's task, even as the \"current build\" changes.\n\nThe working directory and any relevant environment variables (e.g. those having come from task step params) used by the original process will also be used for the process run by intercept.\n\n","depth":3,"section_tag":"fly-intercept"},"fly-jobs":{"location":"jobs.html#fly-jobs","title":"fly jobs","text":"To list the jobs configured in a pipeline, run:\n\n$ fly -t example jobs -p my-pipeline\n","depth":4,"section_tag":"fly-jobs"},"fly-land-worker":{"location":"administration.html#fly-land-worker","title":"fly land-worker","text":"To initiate landing of a worker and eventually (after draining) cause it to exit, run:\n\n$ fly -t example land-worker --worker worker-name\n","depth":4,"section_tag":"fly-land-worker"},"fly-login":{"location":"fly.html#fly-login","title":"fly login","text":"The first thing you'll want to do is authenticate with your target. This is done with the fly login command. This is also useful to save targets under a more convenient alias, so you don't have to type out the URL all the time:\n\nThe login command serves double duty: it authenticates with a given endpoint, and saves it under a more convenient name. The name and token are stored in ~/.flyrc (though you shouldn't really edit the file manually).\n\nConcourse deployments can be occupied by multiple teams. To specify the team to which to log in, specify the --team-name or -n flag. If not specified, this defaults to the The main team.\n\nSo, to log in to a team my-team an endpoint served at https://ci.example.com and save it as the more convenient name example, you would run:\n\n$ fly --target example login --team-name my-team \\\n    --concourse-url https://ci.example.com\nThe login command will see which authentication methods are available for the specified team and prompt you to choose one. For basic auth, it will ask your username and password and use them to acquire a token. For OAuth, it will give you a link to click, and after you've gone through the OAuth flow it will print an OAuth token on the page that you can then copy and paste into the prompt.\n\nNote that if no authentication methods are configured, fly will acquire a token without any prompting. You can then use the alias like normal.\n\nIn any case, a token is saved in your ~/.flyrc, which will expire after one day.\n\nIf your Concourse uses SSL but does not have a certificate signed by a trusted CA, you can use the --ca-cert flag so that fly can trust the connection, like so:\n\n$ fly -t example login -c https://ci.example.com --ca-cert ./ca.crt\nThis will read the value out of the file ./ca.crt and save it into ~/.flyrc so you don't have to pass it on every login invocation.\n\nAfter you've logged in you can use --target example (or -t example for short) to run a command against the saved target example. For example, fly -t example builds will list the last few builds on the example Concourse instance.\n\nThe -t flag is intentionally stateless and must be explicitly added to each command. This reduces the risk of accidentally running a command against the wrong environment when you have multiple targets defined.\n\n","depth":3,"section_tag":"fly-login"},"fly-logout":{"location":"fly.html#fly-logout","title":"fly logout","text":"To clear out your token for a given target, run:\n\n$ fly -t example logout\nTo clear out your token for all targets, run:\n\n$ fly logout -a\nNote: These two variations are mutually exclusive. If the target parameter -t and all parameter -a are both specified, an error will occur.\n\n","depth":3,"section_tag":"fly-logout"},"fly-order-pipelines":{"location":"managing-pipelines.html#fly-order-pipelines","title":"fly order-pipelines","text":"To configure the ordering of pipelines, run:\n\n$ fly -t example order-pipelines \\\n    --pipeline pipeline-1 \\\n    --pipeline pipeline-2 \\\n    --pipeline pipeline-3\nNote that this command only ensures that the given pipelines are in the given order. If there are other pipelines that you haven't included in the command, they may appear in-between, before, or after the given set.\n\n","depth":4,"section_tag":"fly-order-pipelines"},"fly-pause-job":{"location":"jobs.html#fly-pause-job","title":"fly pause-job","text":"To prevent scheduling and running builds of a job, run:\n\n$ fly -t example pause-job --job my-pipeline/my-job\nThis will prevent pending builds of the job from being scheduled, though builds that are in-flight will still run, and pending builds will still be created as normal.\n\n","depth":4,"section_tag":"fly-pause-job"},"fly-pause-pipeline":{"location":"managing-pipelines.html#fly-pause-pipeline","title":"fly pause-pipeline","text":"To pause a pipeline, run:\n\n$ fly -t example pause-pipeline --pipeline my-pipeline\nThis will prevent jobs from being scheduled and stop the periodic checking for new versions of resources. Builds that are in-flight will still finish.\n\n","depth":4,"section_tag":"fly-pause-pipeline"},"fly-pin-resource":{"location":"managing-resources.html#fly-pin-resource","title":"fly pin-resource","text":"To pin a resource to a specific version of that resource, run:\n\n$ fly -t example pin-resource --resource my-pipeline/my-resource \\\n    --version ref:bceaf\nNote that the version needs to be provided as a key-value pair. For the git resource the ref: prefix is used while the registry  resource might use digest as a prefix like digest:sha256:94be7d7b.\n\nA comment can be provided using the --comment flag, which is then also visible in the UI :\n\n$ fly -t example pin-resource --resource my-pipeline/my-resource \\\n    --version ref:abcdef \\\n    --comment \"Some reason\"\nThis can, for example, be used to pull in a fixed version of an external dependency which might break your build in a new release. After the problem has been resolved, the pin can be removed. Another example could be running a build with a set of older inputs when needed.\n\nTo remove the pin on a resource use:\n\n$ fly -t example unpin-resource --resource my-pipeline/my-resource\nYou can also pin a resource via the UI by clicking on the pin button next to the desired version on the resource page. A default comment is automatically generated containing your username and a timestamp. This comment can be edited.\n\n","depth":4,"section_tag":"fly-pin-resource"},"fly-pipelines":{"location":"managing-pipelines.html#fly-pipelines","title":"fly pipelines","text":"To list the currently-configured pipelines and their paused state, run:\n\n$ fly -t example pipelines\n","depth":4,"section_tag":"fly-pipelines"},"fly-prune-worker":{"location":"administration.html#fly-prune-worker","title":"fly prune-worker","text":"To remove a stalled, landing, landed, or retiring worker, run:\n\n$ fly -t example prune-worker --worker worker-name\nTo prune all stalled workers, run:\n\n$ fly -t example prune-worker --all-stalled\nThis is for those cases where you know a worker is not coming back. Note that running workers cannot be pruned, since they'll just re-register themselves anyway.\n\n","depth":4,"section_tag":"fly-prune-worker"},"fly-rename-pipeline":{"location":"managing-pipelines.html#fly-rename-pipeline","title":"fly rename-pipeline","text":"To rename a pipeline, run:\n\n$ fly -t example rename-pipeline \\\n    --old-name my-pipeline \\\n    --new-name my-cool-pipeline\n","depth":4,"section_tag":"fly-rename-pipeline"},"fly-rename-team":{"location":"managing-teams.html#fly-rename-team","title":"fly rename-team","text":"To rename a team, run:\n\n$ fly -t example rename-team --old-name my-team --new-name cool-team\nThis can only be run by the main team.\n\n","depth":4,"section_tag":"fly-rename-team"},"fly-rerun-build":{"location":"jobs.html#fly-rerun-build","title":"fly rerun-build","text":"To queue a new build of a job with exactly the same inputs as a given build of the same job, run:\n\n$ fly -t example rerun-build --job my-pipeline/my-job --build 4\nThis will enqueue a new build of the my-job job in the my-pipeline pipeline, using the same input versions as build number 4.\n\nTo start watching the newly created build, append the --watch flag like so:\n\n$ fly -t example rerun-build --job my-pipeline/my-job --build 4 --watch\nYou can also rerun builds by visiting the build page for the build in question in the web UI and clicking the rerun button.\n\n","depth":4,"section_tag":"fly-rerun-build"},"fly-set-pipeline":{"location":"setting-pipelines.html#fly-set-pipeline","title":"fly set-pipeline","text":"To submit a pipeline configuration to Concourse from a file on your local disk you can use the -c or --config flag, like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml\nThis will present a diff of the changes and ask you to confirm the changes. If you accept then Concourse's pipeline configuration will switch to the pipeline definition in the YAML file specified.\n\n","depth":4,"section_tag":"fly-set-pipeline"},"fly-set-team":{"location":"managing-teams.html#fly-set-team","title":"fly set-team","text":"Once you've logged in as the main team with fly, you can run fly set-team to create or update other teams. Users with a owner role can also update their own configuration with the same command.\n\nFor example, to create a new team that authorizes the local foo user, you would run:\n\nfly -t example set-team --team-name my-team \\\n  --local-user foo\nNote that each time set-team is run, the team's authorization config is set as a whole - it is not a stateful operation.\n\nThere are many different ways to configure team auth; see Configuring Auth for more information.\n\nOnce the team has been created, you can use fly login to log in:\n\n$ fly -t example login -n my-team\nAny newly configured pipelines (via fly set-pipeline) and one-off builds (via Running tasks with fly execute) will be owned by the authorized team. Commands that list content will be scoped to the current team by default, such as fly pipelines and fly builds. The web UI will reflect the same state.\n\nNewly configured pipelines are hidden by default, meaning other teams and unauthorized visitors cannot view them. To make them publicly viewable, see Pipeline \u0026 Build Visibility.\n\n","depth":4,"section_tag":"fly-set-team"},"fly-status":{"location":"fly.html#fly-status","title":"fly status","text":"To check your current authentication status with a given target, run:\n\n$ fly -t example status\nThis will let you know if the token has expired.\n\n","depth":3,"section_tag":"fly-status"},"fly-sync":{"location":"fly.html#fly-sync","title":"fly sync","text":"Occasionally we add additional features to fly or make changes to the communication between it and Concourse's API server. To make sure you're running the latest and greatest version that works with the Concourse you are targeting we provide a command called sync that will update your local fly. It can be used like so:\n\n$ fly -t example sync\nThe fly command will also warn you if it notices that your CLI version is out of sync with the server.\n\n","depth":3,"section_tag":"fly-sync"},"fly-targets":{"location":"fly.html#fly-targets","title":"fly targets","text":"To see what targets are currently known to fly, run:\n\n$ fly targets\nThis will show each target's name, URL, and when its token expires.\n\n","depth":3,"section_tag":"fly-targets"},"fly-teams":{"location":"managing-teams.html#fly-teams","title":"fly teams","text":"To list all the teams, run:\n\n$ fly -t example teams\nThis can be useful if you've forgotten your team name.\n\nfly teams -d: With Details\n\nTo list all the teams with authentication details and members, run:\n\n$ fly -t example teams -d\nThis can be helpful when debugging OAuth, OIDC groups or listing all individual members.\n\n","depth":4,"section_tag":"fly-teams"},"fly-trigger-job":{"location":"jobs.html#fly-trigger-job","title":"fly trigger-job","text":"To immediately queue a new build of a job, run:\n\n$ fly -t example trigger-job --job my-pipeline/my-job\nThis will enqueue a new build of the my-job job in the my-pipeline pipeline.\n\nTo start watching the newly created build, append the --watch flag like so:\n\n$ fly -t example trigger-job --job my-pipeline/my-job --watch\nYou can also queue new builds by clicking the + button on the job or build pages in the web UI.\n\n","depth":4,"section_tag":"fly-trigger-job"},"fly-unpause-job":{"location":"jobs.html#fly-unpause-job","title":"fly unpause-job","text":"To resume scheduling of a job, run:\n\n$ fly -t example unpause-job --job my-pipeline/my-job\nThis will resume scheduling of builds queued for the job.\n\n","depth":4,"section_tag":"fly-unpause-job"},"fly-unpause-pipeline":{"location":"managing-pipelines.html#fly-unpause-pipeline","title":"fly unpause-pipeline","text":"To unpause a pipeline, run:\n\n$ fly -t example unpause-pipeline --pipeline my-pipeline\nThis will resume job scheduling and resource checking.\n\n","depth":4,"section_tag":"fly-unpause-pipeline"},"fly-userinfo":{"location":"fly.html#fly-userinfo","title":"fly userinfo","text":"To check what user you're logged in as, as well as which teams you are currently authenticated to and which roles within each team you have, run:\n\n$ fly -t example userinfo\n","depth":3,"section_tag":"fly-userinfo"},"fly-validate-pipeline":{"location":"setting-pipelines.html#fly-validate-pipeline","title":"fly validate-pipeline","text":"To validate a local pipeline configuration without submitting it to Concourse, run validate-pipeline:\n\n$ fly validate-pipeline --config pipeline.yml\nBy default, pipeline errors will cause validate-pipeline to fail, but warnings won't. To fail on both errors and warnings, pass the `--strict` flag.\n\n","depth":4,"section_tag":"fly-validate-pipeline"},"fly-volumes":{"location":"administration.html#fly-volumes","title":"fly volumes","text":"To list the active volumes across all your workers, run:\n\n$ fly -t example volumes\nThis can be useful to observe the caches warming across your cluster, and could be a good indicator of disk use.\n\n","depth":4,"section_tag":"fly-volumes"},"fly-watch":{"location":"builds.html#fly-watch","title":"fly watch","text":"Concourse emits streaming colored logs on the website but it can be helpful to have the logs available to the command line. (e.g. so that they can be processed by other commands).\n\nThe watch command can be used to do just this. You can also view builds that are running in your pipeline, or builds that have already finished.\n\nNote that unlike Running tasks with fly execute, killing fly watch via SIGINT or SIGTERM will not abort the build.\n\nTo watch the most recent one-off build, just run fly watch with no arguments. To watch a specific build (one-off or no), pass --build with the ID of the build to watch. This ID is available at the start of Running tasks with fly execute's output or by browsing to the builds list in the web UI.\n\nBy using the --job and --build flags you can pick out a specific build of a job to watch. For example, the following command will either show the archived logs for an old build if it has finished running or it will stream the current logs if the build is still in progress.\n\n$ fly -t example watch --job my-pipeline/tests --build 52\nIf the --job flag is specified and --build is omitted, the most recent build of the specified job will be selected.\n\n","depth":3,"section_tag":"fly-watch"},"fly-workers":{"location":"administration.html#fly-workers","title":"fly workers","text":"To list the currently registered workers, including additional metadata, run:\n\n$ fly -t example workers\nThis can be useful for monitoring the status of your workers, if you suspect that one keeps dropping out of the pool or getting tasked with too many containers, etc.\n\n","depth":4,"section_tag":"fly-workers"},"garbage-collector":{"location":"garbage-collector.html","title":"Garbage Collector","text":"Concourse runs everything in isolated environments by creating fresh containers and volumes to ensure things can safely run in a repeatable environment, isolated from other workloads running on the same worker.\n\nThis introduces a new problem of knowing when Concourse should remove these containers and volumes. Safely identifying things for removal and then getting rid of them, releasing their resources, is the process of garbage collection.\n\n","depth":4,"section_tag":"garbage-collector"},"generating-keys":{"location":"concourse-generate-key.html","title":"Generating Keys","text":"Concourse's various components use RSA keys to verify tokens and worker registration requests.\n\nA minimal deployment will require the following keys:\n\nsession_signing_key: Used by the Running a web node for signing and verifying user session tokens.\n\n\ntsa_host_key: Used by the Running a web node for the SSH worker registration gateway server (\"TSA\").\n\nThe public key is given to each Running a worker node to verify the remote host when connecting via SSH.\n\n\nworker_key (one per worker): Each Running a worker node verifies its registration with the Running a web node via a SSH key.\n\nThe public key must be listed in the Running a web node's authorized keys configuration in order for the worker to register.\n\n\n\nTo generate these keys, run:\n\nconcourse generate-key -t rsa -f ./session_signing_key\nconcourse generate-key -t ssh -f ./tsa_host_key\nconcourse generate-key -t ssh -f ./worker_key\n...and we'll also start on an authorized_keys file, currently listing this initial worker key:\n\ncp worker_key.pub authorized_worker_keys\n","depth":3,"section_tag":"concourse-generate-key"},"generic-oauth":{"location":"generic-oauth.html","title":"Generic oAuth","text":"A Concourse server can authenticate against any valid OAuth auth provider, though it's a bit \"closer to the metal\" as you'll need to explicitly configure the auth, token, and user-info URLs. You may want to see if you can use Generic OIDC auth if your auth provider is compatible with OIDC.\n\n","depth":4,"section_tag":"generic-oauth"},"generic-oauth-authentication":{"location":"generic-oauth.html#generic-oauth-authentication","title":"Authentication","text":"First you'll need to create a client with your oAuth provider.\n\nThe callback URL must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by your oAuth provider - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nThe Generic oAuth provider has many values to set - for a full list consult concourse web --help.\n\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_OAUTH_DISPLAY_NAME=Acme\nCONCOURSE_OAUTH_CLIENT_ID=myclientid\nCONCOURSE_OAUTH_CLIENT_SECRET=myclientsecret\nCONCOURSE_OAUTH_AUTH_URL=https://oauth.example.com/oauth2/auth\nCONCOURSE_OAUTH_TOKEN_URL=https://oauth.example.com/oauth2/token\nCONCOURSE_OAUTH_USERINFO_URL=https://oauth.example.com/oauth2/userinfo\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-oauth-authentication"},"generic-oauth-authorization":{"location":"generic-oauth.html#generic-oauth-authorization","title":"Authorization","text":"OAuth users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--oauth-user=USERNAME: Authorize an individual user.\n\n\n--oauth-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which claim points to the groups information by specifying CONCOURSE_OAUTH_GROUPS_KEY on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --oauth-user my-username \\\n    --oauth-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  oauth:\n    users: [\"my-username\"]\n    groups: [\"my-group\"]\n","depth":5,"section_tag":"generic-oauth-authorization"},"generic-oidc-auth":{"location":"generic-oidc-auth.html","title":"Generic OIDC auth","text":"A Concourse server can authenticate against any valid OIDC auth provider. This provider is similar to Generic oAuth except it only requires an issuer URL rather than auth/token/userinfo URLs.\n\n","depth":4,"section_tag":"generic-oidc-auth"},"generic-oidc-authentication":{"location":"generic-oidc-auth.html#generic-oidc-authentication","title":"Authentication","text":"First you'll need to create a client with your oAuth provider.\n\nThe callback URL must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by your OIDC provider - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nA typical Running a web node env config may look something like this:\n\nCONCOURSE_OIDC_DISPLAY_NAME=Acme\nCONCOURSE_OIDC_CLIENT_ID=myclientid\nCONCOURSE_OIDC_CLIENT_SECRET=myclientsecret\nCONCOURSE_OIDC_ISSUER=https://oidc.example.com\nConsult concourse web --help for a full list of flags with descriptions.\n\n","depth":5,"section_tag":"generic-oidc-authentication"},"generic-oidc-authorization":{"location":"generic-oidc-auth.html#generic-oidc-authorization","title":"Authorization","text":"OIDC users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--oidc-user=USERNAME: Authorize an individual user.\n\n\n--oidc-group=GROUP_NAME: Authorize anyone from the group.\n\nYou may only configure groups if the auth provider exposes this information in either the token itself, or in the contents of the userinfo endpoint.\n\nYou can configure which claim points to the groups information by specifying CONCOURSE_OIDC_GROUPS_KEY on the Running a web node.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --oidc-user my-username \\\n    --oidc-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  oidc:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"generic-oidc-authorization"},"get-step":{"location":"jobs.html#get-step","title":"get step","text":"","depth":3,"section_tag":"steps"},"git-trigger-example":{"location":"git-trigger-example.html","title":"git-triggered job example","text":"The git resource can be used to trigger a job.\n\n","depth":2,"section_tag":"git-trigger-example"},"github-auth":{"location":"github-auth.html","title":"GitHub auth","text":"A Concourse server can authenticate against GitHub to leverage their permission model and other security improvements in their infrastructure.\n\n","depth":4,"section_tag":"github-auth"},"github-authentication":{"location":"github-auth.html#github-authentication","title":"Authentication","text":"First, you'll need to create an OAuth application on GitHub.\n\nThe \"Authorization callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by GitHub - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_GITHUB_CLIENT_ID=myclientid\nCONCOURSE_GITHUB_CLIENT_SECRET=myclientsecret\nNote that the client must be created under an organization if you want to authorize users based on organization/team membership. If the client is created under a personal account, only individual users can be authorized.\n\nIf you're configuring GitHub Enterprise, you'll also need to set the following env:\n\nCONCOURSE_GITHUB_HOST=github.example.com\nCONCOURSE_GITHUB_CA_CERT=/path/to/ca_cert\nThe GitHub Enterprise host must not contain a scheme, or a trailing slash.\n\n","depth":5,"section_tag":"github-authentication"},"github-authorization":{"location":"github-auth.html#github-authorization","title":"Authorization","text":"Users, teams, and entire organizations can be authorized for a team by passing the following flags to fly set-team:\n\n--github-user=LOGIN: Authorize an individual user.\n\n\n--github-org=ORG_NAME: Authorize an entire organization's members.\n\n\n--github-team=ORG_NAME:TEAM_NAME: Authorize a team's members within an organization.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --github-user my-github-login \\\n    --github-org my-org \\\n    --github-team my-other-org:my-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  github:\n    users: [\"my-github-login\"]\n    orgs: [\"my-org\"]\n    teams: [\"my-other-org:my-team\"]\n","depth":5,"section_tag":"github-authorization"},"gitlab-auth":{"location":"gitlab-auth.html","title":"GitLab auth","text":"A Concourse server can authenticate against GitLab to leverage their permission model.\n\n","depth":4,"section_tag":"gitlab-auth"},"gitlab-authentication":{"location":"gitlab-auth.html#gitlab-authentication","title":"Authentication","text":"First you need to create an OAuth application on GitLab.\n\nThe \"Authorization callback URL\" must be the URL of your Concourse server with /sky/issuer/callback appended. This address must be reachable by GitLab - it can't be localhost.\n\nFor example, Concourse's own CI server's callback URL would be:\n\nhttps://ci.concourse-ci.org/sky/issuer/callback\nYou will be given a Client ID and a Client Secret for your new application. The client ID and secret must then be configured on the Running a web node by setting the following env:\n\nCONCOURSE_GITLAB_CLIENT_ID=myclientid\nCONCOURSE_GITLAB_CLIENT_SECRET=myclientsecret\nIf you're configuring a self hosted GitLab instance, you'll also need to set the following flag:\n\nCONCOURSE_GITLAB_HOST=https://gitlab.example.com\nThe GitLab host must contain a scheme and not a trailing slash.\n\n","depth":5,"section_tag":"gitlab-authentication"},"gitlab-authorization":{"location":"gitlab-auth.html#gitlab-authorization","title":"Authorization","text":"Users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--gitlab-user=USERNAME: Authorize an individual user.\n\n\n--gitlab-group=GROUP_NAME: Authorize an entire groups's members.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --gitlab-user my-gitlab-user \\\n    --gitlab-team my-team\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  gitlab:\n    users: [\"my-gitlab-login\"]\n    orgs: [\"my-org\"]\n    teams: [\"my-other-org:my-team\"]\n","depth":5,"section_tag":"gitlab-authorization"},"giving-your-cluster-a-name":{"location":"concourse-web.html#giving-your-cluster-a-name","title":"Giving your cluster a name","text":"If you've got many Concourse clusters that you switch between, you can make it slightly easier to notice which one you're on by giving each cluster a name:\n\nCONCOURSE_CLUSTER_NAME=production\nWhen set, this name will be shown in the top bar when viewing the dashboard.\n\n","depth":5,"section_tag":"giving-your-cluster-a-name"},"global-resources":{"location":"global-resources.html","title":"Global Resources (experimental)","text":"Concourse v5.0 contains an experimental feature known as \"global resources\". It is enabled by passing the --enable-global-resources flag to the concourse web command.\n\nThe basic concept of global resources is to share detected resource versions between all resources that have the same resourcetype and resourcesource configuration.\n\nBefore v5.0.0, each pipeline resource had its own version history, associated to the resource by name. This meant that multiple pipelines with the same resource configs would redundantly collect the same version and metadata information.\n\nWith v5.0.0's experimental 'global resources' feature, resource versions are instead associated to an anonymous 'resource config' i.e. its resourcetype and resourcesource.\n\n","depth":3,"section_tag":"global-resources"},"goals":{"location":"garbage-collector.html#goals","title":"Goals","text":"Let's define our metrics for success:\n\n* Safe. There should never be a case where a build is running and a container or volume is removed out from under it, causing the build to fail. Resource checking should also never result in errors from check containers being removed. No one should even know garbage collection is happening.\n\n* Airtight. Everything Concourse creates, whether it's a container or volume on a worker or an entry in the database, should never leak. Each object should have a fully defined lifecycle such that there is a clear end to its use. The ATC should be interruptible at any point in time and at the very least be able to remove any state it had created beforehand.\n\n* Resilient. Garbage collection should never be outpaced by the workload. A single misbehaving worker should not prevent garbage collection from being performed on other workers. A slow delete of a volume should not prevent garbage collecting of other things on the same worker.\n\n","depth":5,"section_tag":"goals"},"golang-library-example":{"location":"golang-library-example.html","title":"Golang library testing example","text":"You can run the tests for a Golang library across any specified versions.\n\nThis example shows how to have multiple versions of a language, environment, or dependency fetched and integrated in to a Pipeline.\n\nFor these Docker images, defining them as Resources has two advantages for this use case. First, this enables the pipeline to be triggered when there are new versions of those images available. Second, referencing them in the task's task step image param is helpful as it will ensure consistency between the image versions fetched by the Resource and the image version running in the job.\n\n","depth":2,"section_tag":"golang-library-example"},"gracefully-removing-a-worker":{"location":"concourse-worker.html#gracefully-removing-a-worker","title":"Gracefully Removing a Worker","text":"When a worker machine is going away, it should be retired. This is similar to landing, except at the end the worker is completely unregistered, along with its volumes and containers. This should be done when a worker's VM or container is being destroyed.\n\nTo retire a worker, send SIGUSR2 to the worker process. This will switch the worker to retiring state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the worker will be removed and the worker process will exit.\n\nJust like with landing, you may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like start-stop-daemon:\n\nstart-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR2/300/TERM/15/KILL\nThis will send SIGUSR2, wait up to 5 minutes, and then send SIGTERM. If it's still running, it will be killed after an additional 15 seconds.\n\n","depth":5,"section_tag":"gracefully-removing-a-worker"},"hello-world-example":{"location":"hello-world-example.html","title":"Hello World pipeline","text":"A single job is the simplest form of pipeline.\n\nWhile this is less of an example pipeline, this is a simple introduction to a critical primitive to form pipelines.\n\nAlso, due to the fact that there are minimal external factors (Resources) for the system to check and resolve. This is often used overall test system health.\n\n","depth":2,"section_tag":"hello-world-example"},"hooks-example":{"location":"hooks-example.html","title":"Job \u0026 task hooks example","text":"Job Hooks and Step hooks are available to perform actions based on the success, failure, or abortion of a job.\n\n","depth":2,"section_tag":"hooks-example"},"how-it-works":{"location":"garbage-collector.html#how-it-works","title":"How it Works","text":"The garbage collector is a batch operation that runs on an interval with a default of 30 seconds. It's important to note that the collector must be able to run frequently enough to not be outpaced by the workload producing things, and so the batch operation should be able to complete pretty quickly.\n\nThe batch operation first performs garbage collection within the database alone, removing rows that are no longer needed. The removal of rows from one stage will often result in removals in a later stage. There are individual collectors for each object, such as the volume collector or the container collector, and they are all run asynchronously.\n\nAfter the initial pass of garbage collection in the database, there should now be a set of containers and volumes that meet criteria for garbage collection. These two are a bit more complicated to garbage-collect; they both require talking to a worker, and waiting on a potentially slow delete.\n\nContainers and volumes are the costliest resources consumed by Concourse. There are also many of them created over time as builds execute and pipelines perform their resource checking. Therefore it is important to parallelize this aspect of garbage collection so that one slow delete or one slow worker does not cause them to pile up.\n\n","depth":5,"section_tag":"how-it-works"},"iam-permissions":{"location":"aws-asm-credential-manager.html#iam-permissions","title":"IAM Permissions","text":"The following is an example of an IAM policy that can be used to grant permissions to an IAM user or instance role. Note that the Resource section can contain a wildcard to a secret or be restricted to an individual secret.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Sid\": \"AllowAccessToSecretManagerParameters\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"secretsmanager:ListSecrets\"\n        ],\n          \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"AllowAccessGetSecret\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\"\n            ],\n            \"Resource\": [\n                \"arn:aws:secretsmanager:::secret:/concourse/*\",\n                \"arn:aws:secretsmanager:::secret:/concourse/TEAM_NAME/*\",\n                \"arn:aws:secretsmanager:::secret:/concourse/TEAM_NAME/PIPELINE_NAME/*\"\n            ]\n        }\n    ]\n}\nNote that the TEAM_NAME and PIPELINE_NAME text above should be replaced to fit your Concourse setup.\n\nFor more information on how to use IAM roles to restrict access to Secrets Manager, review the official documentation.\n\n","depth":5,"section_tag":"iam-permissions"},"implementing-resource-types":{"location":"implementing-resource-types.html","title":"Implementing a Resource Type","text":"A resource type is implemented by a container image with three scripts:\n\n* /opt/resource/check for checking for new versions of the resource\n\n* /opt/resource/in for pulling a version of the resource down\n\n* /opt/resource/out for idempotently pushing a version up\n\nDistributing resource types as containers allows them to package their own dependencies. For example, the Git resource comes with git installed.\n\nAll resources must implement all three actions, though the actions can just be no-ops (which still must be correctly implemented as detailed below).\n\nResources can emit logs to the user by writing to stderr. ANSI escape codes (coloring, cursor movement, etc.) will be interpreted properly by the web UI, so you should make your output pretty.\n\n","depth":3,"section_tag":"implementing-resource-types"},"in-parallel-step":{"location":"jobs.html#in-parallel-step","title":"in_parallel step","text":"","depth":3,"section_tag":"steps"},"index":{"location":"index.html","title":"Concourse","text":"Configure as coderesources:\n- name: booklit\n  type: git\n  source: {uri: \"https://github.com/vito/booklit\"}\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: true\n  - task: test\n    file: booklit/ci/test.yml\nVisualize to verifyA Concourse pipeline is like a distributed, continuous Makefile.\n\nEach job has a build plan declaring the job's input resources and what to run with them when they change.\n\nYour pipeline is then visualized in the web UI, taking only one click to get from a failed job to seeing why it failed.\n\nThe visualization provides a \"gut check\" feedback loop: if it looks wrong, it probably is wrong.\n\nA more complicated example...Jobs can depend on other jobs by configuring passed constraints. The resulting chain of jobs and resources is a dependency graph that continuously pushes your project forward, from source code to production.\n\nThis particular pipeline can be found in the Booklit repository.\n\nAll configuration and administration is done using the fly CLI.\n\nThe fly set-pipeline command pushes the config up to Concourse. Once it looks good, you can then check the file in to source control. This makes it easy to recover your project if the Concourse server burns down.\n\nEverything runs in containers, ensuring a clean environment on every run.\n\nEach task specifies its own image, giving it full control over its dependencies, rather than managing packages and state on your workers.\n\nThe fly intercept command will pop you right into one of your build's containers, making it easy to troubleshoot flaky builds.\n\nThe Running tasks with fly execute command lets you run a build with local changes.\n\nThis build runs in exactly the same way as it would run in your pipeline, without having to push broken commits until it works.\n\nWhen a build in the pipeline fails, you can run Running tasks with fly execute with the -j flag to run a one-off build with the same inputs as the failed build. You can then replace an input with your local changes with -i to see if your fix is valid.\n\nBring your own integrationsresource_types:\n- name: rubygem\n  type: registry-image\n  source:\n    repository: troykinsella/concourse-rubygems-resource\n\nresources:\n- name: rspec-gem\n  type: rubygem\n  source: {gem: rspec}\n\njobs:\n- name: bundle\n  plan:\n  - get: rspec-gem\n    trigger: true\n  - # ...\nConcourse does not have a complex plugin system. Instead, it focuses on a single strong abstraction: resource, which are implemented by resource types.\n\nThe pipelineresources field configures external artifacts that your pipeline will monitor for changes, fetch from, and push to.\n\nFor example, a resource with type git refers to a git repository, which will be cloned in a get step and pushed to using a put step. Behind the scenes, Concourse will continuously run git fetch to look for new commits that jobs may want to trigger on.\n\nAt its core, Concourse knows nothing about git. It comes with a git resource type out of the box, but you could just as easily bring your own into your pipeline by setting the pipelineresource_types field.\n\nTo see what resource types are available, check out the Resource Types catalog!\n\n","depth":0,"section_tag":"index"},"install":{"location":"install.html","title":"Install","text":"A Concourse installation is composed of a Running a web node, a Running a worker node, and a Running a PostgreSQL node.\n\nThere are many ways to deploy Concourse, depending on your personal preference. The Quick Start guide shows how to get Concourse up and running quickly via Docker Compose, and there is also an official Concourse Helm chart.\n\nThe documentation found here will primarily focus on the concourse CLI, which is the lowest common denominator, and can also be directly used if you want to just run Concourse yourself on real hardware or your own managed VMs.\n\n","depth":2,"section_tag":"install"},"intercept-admin-only":{"location":"global-resources.html#intercept-admin-only","title":"Intercepting check containers is no longer safe","text":"Now that check containers are shared across teams, it would be dangerous to allow anyone to fly intercept to check containers. For this reason, this capability is limited to admin users.\n\nWe recognize that this will make it a bit more difficult for end users to debug things like failing checks. We plan to improve this by introducing a way to provision a new check container to facilitate debugging. See 3344 for more information.\n\n","depth":5,"section_tag":"intercept-admin-only"},"internals":{"location":"internals.html","title":"Internals","text":"This section provides a deeper understanding of some of the concepts surrounding Concourse.\n\nAn understanding of the basics of Concourse concepts, such as pipelines, jobs, etc, is recommended as parts of this section might assume a level of knowledge from them. This section is not necessary for using Concourse but are more for experienced users that want to dig deeper into how Concourse works.\n\n","depth":2,"section_tag":"internals"},"intro-to-yaml":{"location":"config-basics.html#intro-to-yaml","title":"Intro to YAML","text":"YAML is a human-friendly syntax for writing structured documents. You can think of it as JSON without the sharp edges.\n\nHere's a quick example demonstrating common YAML syntax:\n\n# commented lines are prefixed with the '#' character\n\n# strings\nquoted_string: \"bar\"\nunquoted_string: hello world!\nmultiline_string: |\n  hello, world!\n  this is one big string with a trailing linebreak!\n\n# arrays\narray: [hello, world]\nmultiline_array:\n- hello\n- world\n\n# objects\nobject: {one: uno, two: dos}\nmultiline_object:\n  one: uno\n  two: dos\n\n# boolean values\nbooleans: [true, false]\n\n# numeric values\nnumeric: [1234, 12.34]\n","depth":3,"section_tag":"intro-to-yaml"},"jobs":{"location":"jobs.html","title":"Jobs","text":"Jobs determine the actions of your pipeline. They determine how resources progress through it, and how the pipeline is visualized.\n\nThe most important attribute of a job is its build plan, configured as jobplan. This determines the sequence of Steps to execute in any builds of the job.\n\nA pipeline's jobs are listed under pipelinejobs with the following schema:\n\n","depth":2,"section_tag":"jobs"},"kubernetes-credential-lookup-rules":{"location":"kubernetes-credential-manager.html#kubernetes-credential-lookup-rules","title":"Credential lookup rules","text":"When resolving a parameter such as ((foo)), Concourse will lookup for it in the following order in the namespace configured for that team:\n\n* Name:         PIPELINE_NAME.foo\n  Namespace:    concourse-TEAM_NAME\n  Type:         Opaque\n\n  Data\n  ====\n  value:        32 bytes\n\n* Name:         foo\n  Namespace:    concourse-TEAM_NAME\n  Type:         Opaque\n\n  Data\n  ====\n  value:        32 bytes\n\nYou can also have nested fields, which can be accessed using . syntax, e.g. ((foo.bar)):\n\n* Name:         PIPELINE_NAME.foo\n  Namespace:    concourse-TEAM_NAME\n  Type:         Opaque\n\n  Data\n  ====\n  bar:          32 bytes\n\n* Name:         foo\n  Namespace:    concourse-TEAM_NAME\n  Type:         Opaque\n\n  Data\n  ====\n  bar:          32 bytes\n\nThe prefix prepended to the namespace used by Concourse to search for secrets (in the examples above, concourse-) can be changed by configuring the following in the Running a web node:\n\nCONCOURSE_KUBERNETES_NAMESPACE_PREFIX=some-other-prefix-\nIf an action is being run in a one-off build, Concourse will not include the pipeline name in the secret that it looks for.\n\n","depth":5,"section_tag":"kubernetes-credential-lookup-rules"},"kubernetes-credential-manager":{"location":"kubernetes-credential-manager.html","title":"Kubernetes Credential Manager","text":"Concourse can be configured to pull credentials from Kubernetes secret objects.\n\nTo configure it, either enable the in-cluster client by setting the following environment variable on the Running a web node:\n\nCONCOURSE_KUBERNETES_IN_CLUSTER=true\nor set the path to a kubeconfig file:\n\nCONCOURSE_KUBERNETES_CONFIG_PATH=~/.kube/config\n","depth":4,"section_tag":"kubernetes-credential-manager"},"ldap-auth":{"location":"ldap-auth.html","title":"LDAP auth","text":"The LDAP provider can be used for operators who wish to authenticate their users against an LDAP server.\n\n","depth":4,"section_tag":"ldap-auth"},"ldap-authentication":{"location":"ldap-auth.html#ldap-authentication","title":"Authentication","text":"The LDAP provider is configured by pointing it to an LDAP host with a read-only bind DN and password. This bind DN and password is used for authenticating with the LDAP host and querying the users.\n\nAdditionally, the base DN under which users are searched as well as the attribute of the users to associate to 'usernames' must also be configured.\n\nThese can be specified via env to the Running a web node like so:\n\nCONCOURSE_LDAP_DISPLAY_NAME=Acme # optional; default \"LDAP\"\nCONCOURSE_LDAP_HOST=ldap.example.com # port defaults to 389 or 636\nCONCOURSE_LDAP_BIND_DN='cn=read-only-admin,dc=example,dc=com'\nCONCOURSE_LDAP_BIND_PW=read-only-admin-password\nCONCOURSE_LDAP_USER_SEARCH_BASE_DN='cn=users,dc=example,dc=com'\nCONCOURSE_LDAP_USER_SEARCH_USERNAME=uid\nTo configure TLS, you may need to set a CA cert:\n\nCONCOURSE_LDAP_CA_CERT=/path/to/ca_cert\nIf your LDAP host does not use TLS, you must set:\n\nCONCOURSE_LDAP_INSECURE_NO_SSL=true\nTo fine-tune which users are queried, you can specify a user search filter like so:\n\nCONCOURSE_LDAP_USER_SEARCH_FILTER='(objectClass=person)'\nTo set which user attributes map to the token claims, you can set the following:\n\nCONCOURSE_LDAP_USER_SEARCH_ID_ATTR=uid         # default\nCONCOURSE_LDAP_USER_SEARCH_EMAIL_ATTR=mail     # default\nCONCOURSE_LDAP_USER_SEARCH_NAME_ATTR=some-attr # no default\n","depth":5,"section_tag":"ldap-authentication"},"ldap-authorization":{"location":"ldap-auth.html#ldap-authorization","title":"Authorization","text":"LDAP users and groups can be authorized for a team by passing the following flags to fly set-team:\n\n--ldap-user=USERNAME: Authorize an individual user.\n\n\n--ldap-group=GROUP_NAME: Authorize anyone from the group.\n\n\n\nFor example:\n\n$ fly set-team -n my-team \\\n    --ldap-user my-username \\\n    --ldap-group my-group\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  ldap:\n    users: [\"my-username\"]\n    groups: [\"my-groups\"]\n","depth":5,"section_tag":"ldap-authorization"},"lets-encrypt":{"location":"concourse-web.html#lets-encrypt","title":"TLS via Let's Encrypt","text":"Concourse can be configured to automatically acquire a TLS certificate via Let's Encrypt:\n\n# Enable TLS\nCONCOURSE_TLS_BIND_PORT=443\n\n# Enable Let's Encrypt\nCONCOURSE_ENABLE_LETS_ENCRYPT=true\nConcourse's Let's Encrypt integration works by storing the TLS certificate and key in the database, so it is imperative that you enable database encryption as well.\n\nBy default, Concourse will reach out to Let's Encrypt's ACME CA directory. An alernative URL can be configured like so:\n\nCONCOURSE_LETS_ENCRYPT_ACME_URL=https://acme.example.com/directory\nIn order to negotiate the certificate, your web node must be reachable by the ACME server. There are intentionally no publicly listed IP addresses to whitelist, so this typically means just making your web node publicly reachable.\n\n","depth":5,"section_tag":"lets-encrypt"},"limit-active-tasks-strategy":{"location":"container-placement.html#limit-active-tasks-strategy","title":"The limit-active-tasks strategy","text":"limit-active-tasks is an experimental feature.\n\nWhen selecting the limit-active-tasks placement strategy, each task executed on a worker will increase the number of \"active tasks\" on that worker by one. When the task completes the number is decreased by one. The Running a web node then places get, put and task containers on the worker that currently has the least amount of active tasks.\n\nAdditionally max-active-tasks-per-worker can be set to be an integer of 1 or more, in which case a worker will not execute more than that amount of tasks. A value of 0 means that there is no limit on the maximum number of active tasks on the workers. If no worker can be selected because all of them already have max-active-tasks-per-worker active tasks, then the task will wait for a free worker, periodically polling the pool. Note that the parameter does not apply to get and put steps which will always be scheduled on the worker with the fewest active tasks.\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=limit-active-tasks\n and, optionally CONCOURSE_MAX_ACTIVE_TASKS_PER_WORKER=1\n\n\n","depth":4,"section_tag":"limit-active-tasks-strategy"},"load-var-step":{"location":"jobs.html#load-var-step","title":"load_var step","text":"","depth":3,"section_tag":"steps"},"local-auth":{"location":"local-auth.html","title":"Local User auth","text":"Local User auth is a primitive username/password-based auth mechanism. All users and passwords are configured statically.\n\nIn general, we recommend configuring one of the other providers instead, but for small deployments with only a few users, local user auth may be all you need.\n\n","depth":4,"section_tag":"local-auth"},"local-authentication":{"location":"local-auth.html#local-authentication","title":"Authentication","text":"Local users are configured on the Running a web node by setting the following env:\n\nCONCOURSE_ADD_LOCAL_USER=myuser:mypass,anotheruser:anotherpass\nThis configures two users, myuser and anotheruser, with their corresponding passwords.\n\nWhen local users are configured, the log-in page in the web UI will show a username/password prompt.\n\nLocal users can also log in via fly login with the --username and --password flags.\n\n","depth":5,"section_tag":"local-authentication"},"local-authorization":{"location":"local-auth.html#local-authorization","title":"Authorization","text":"Local users are granted access to teams via fly set-team, using the --local-user flag:\n\n$ fly set-team -n my-team \\\n    --local-user some_username\n...or via --config for setting user roles:\n\nroles:\n- name: member\n  local:\n    users: [\"some_username\"]\n","depth":5,"section_tag":"local-authorization"},"local-vars":{"location":"vars.html#local-vars","title":"The \".\" var source","text":"The special var source name . refers to a \"local var source.\"\n\nThe precise scope for these \"local vars\" depends on where they're being used. Currently the only mechanism that uses the local var source is the load_var step, which sets a var in a local var source provided to all steps executed in the build.\n\n","depth":3,"section_tag":"local-vars"},"main-team":{"location":"main-team.html","title":"The main team","text":"Out of the box, Concourse comes with a single team called main.\n\nThe main team is an admin team, meaning members (specifically, users with the owner role) can create and update other teams. Currently there is no way to promote a team to become an admin team, so main is a special-case.\n\nThe main team is different in that all flags normally passed to fly set-team are instead passed to the concourse web command, prefixed with --main-team-. The values set in these flags take effect whenever the web node starts up. This is done so that you can't get locked out.\n\nTo learn how to configure your main team, continue on to the appropriate section for your auth provider of choice under Configuring Auth.\n\n","depth":3,"section_tag":"main-team"},"managing-jobs":{"location":"jobs.html#managing-jobs","title":"Managing Jobs","text":"","depth":3,"section_tag":"managing-jobs"},"managing-pipelines":{"location":"managing-pipelines.html","title":"Managing Pipelines","text":"","depth":3,"section_tag":"managing-pipelines"},"managing-resource-types":{"location":"managing-resource-types.html","title":"Managing Resource Types","text":"","depth":3,"section_tag":"managing-resource-types"},"managing-resources":{"location":"managing-resources.html","title":"Managing Resources","text":"","depth":3,"section_tag":"managing-resources"},"managing-teams":{"location":"managing-teams.html","title":"Managing Teams","text":"","depth":3,"section_tag":"managing-teams"},"manual-trigger-example":{"location":"manual-trigger-example.html","title":"Manually triggered job example","text":"A job can be triggered by a resource. After it's complete, the next job can run automatically or manually.\n\n","depth":2,"section_tag":"manual-trigger-example"},"metrics":{"location":"metrics.html","title":"Metrics","text":"Metrics are essential in understanding how any large system is behaving and performing. Concourse can emit metrics about both the system health itself and about the builds that it is running. Operators can tap into these metrics in order to observe the health of the system.\n\nIn the spirit of openness, the metrics from our deployment are public. We consider it a bug to emit anything sensitive or secret into our metrics pipeline.\n\n","depth":3,"section_tag":"metrics"},"nodejs-example":{"location":"nodejs-example.html","title":"Nodejs application testing example","text":"You can run the tests for a Nodejs application.\n\n","depth":2,"section_tag":"nodejs-example"},"observation":{"location":"observation.html","title":"Observation","text":"This section outlines everything you need to know for observing the state of your pipelines.\n\n","depth":2,"section_tag":"observation"},"operation":{"location":"operation.html","title":"Operation","text":"The following sections describes operator-focused features and tools that Concourse provides, such as monitoring and credential management.\n\nThese concepts are not required to operate Concourse, but are for users that are looking to extend the capabilities of managing a Concourse deployment. For users that are new to these concepts, we do recommend learning how to set up Credential Management and Encryption.\n\n","depth":2,"section_tag":"operation"},"php-example":{"location":"php-example.html","title":"PHP application testing example","text":"You can run the tests for a PHP application.\n\n","depth":2,"section_tag":"php-example"},"pipeline-static-vars":{"location":"setting-pipelines.html#pipeline-static-vars","title":"Providing static values for vars","text":"The pipeline configuration can contain Vars which may be replaced with static values or loaded at runtime. This allows for credentials to be extracted from a pipeline config, making it safe to check in to a public repository or pass around.\n\nFor example, if you have a pipeline.yml as follows:\n\nresources:\n- name: private-repo\n  type: git\n  source:\n    uri: git@...\n    branch: master\n    private_key: ((private-repo-key))\n...you could then configure this pipeline like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --var \"private-repo-key=$(cat id_rsa)\"\nOr, if you had a credentials.yml as follows:\n\nprivate-repo-key: |\n  -----BEGIN RSA PRIVATE KEY-----\n  ...\n  -----END RSA PRIVATE KEY-----\n...you could configure it like so:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --load-vars-from credentials.yml\nWhen configuring a pipeline, any vars not provided statically will be left to resolve at runtime. To check that all vars are resolveable, you can pass the --check-creds flag:\n\n$ fly -t example set-pipeline \\\n    --pipeline my-pipeline \\\n    --config pipeline.yml \\\n    --load-vars-from credentials.yml \\\n    --check-creds\nThis will fill in all statically-provided vars and then attempt to resolve all remanining vars server-side. If any fail to resolve, configuring the pipeline will fail.\n\n","depth":5,"section_tag":"pipeline-static-vars"},"pipeline-vars-example":{"location":"pipeline-vars-example.html","title":"Pipeline ((vars)) example","text":"You can use params in a pipelines configuration file.\n\n","depth":2,"section_tag":"pipeline-vars-example"},"pipeline-visibility":{"location":"observation.html#pipeline-visibility","title":"Pipeline Visibility","text":"Pipelines may be exposed so that they can be monitored without having to authenticate. For more information, see Pipeline \u0026 Build Visibility.\n\n","depth":3,"section_tag":"pipeline-visibility"},"pipelines":{"location":"pipelines.html","title":"Pipelines","text":"A pipeline is the result of configuring Jobs and Resources together. When you configure a pipeline, it takes on a life of its own, to continuously detect resource versions and automatically queue new builds for jobs as they have new available inputs.\n\nPipelines are configured via fly set-pipeline or the set_pipeline step as declarative YAML files which conform to the following schema:\n\n","depth":2,"section_tag":"pipelines"},"pointing-to-external-dns-servers":{"location":"concourse-worker.html#pointing-to-external-dns-servers","title":"Pointing to external DNS servers","text":"If you have no need for special DNS resolution within your Concourse containers, you can just configure your containers to use specific DNS server addresses external to the VM.\n\nThis can be done by listing DNS servers in config.ini like so:\n\n[server]\n; configure Google DNS\ndns-server = 8.8.8.8\ndns-server = 8.8.4.4\nTo validate whether the changes have taken effect, you can fly intercept into any container and check /etc/resolv.conf once again:\n\n$ fly -t ci intercept -c concourse/concourse\nbash-5.0# cat /etc/resolv.conf\nnameserver 8.8.8.8\nnameserver 8.8.4.4\nbash-5.0# ping google.com\nPING google.com (108.177.111.139): 56 data bytes\n64 bytes from 108.177.111.139: seq=0 ttl=47 time=2.672 ms\n64 bytes from 108.177.111.139: seq=1 ttl=47 time=0.911 ms\n","depth":7,"section_tag":"pointing-to-external-dns-servers"},"postgresql-node":{"location":"postgresql-node.html","title":"Running a PostgreSQL node","text":"Concourse uses PostgreSQL for storing all data and coordinating work in a multi-Running a web node installation.\n\n","depth":3,"section_tag":"postgresql-node"},"project":{"location":"project.html","title":"Project","text":"Concourse started as a side-project by @vito (hi!) and @xoebus in 2014. Since then, Concourse has grown into a small but dedicated team of full-time engineers and part-time contributors.\n\nWhere does the magic happen?* The concourse/concourse GitHub repo is where the main code lives and where roadmap planning begins.\n\n* The concourse/concourse GitHub wiki is the main point of discovery for all community-created resource types/tools/tutorials/etc., and is also used as a contributor knowledge base.\n\n* Our Medium blog features (occasional) updates from the development side.\n\n* The Concourse forums for support, announcements, and general discussion.\n\n* Our Discord server is a great place to chat with other contributors.\n\nWhy make Concourse?When working on a sizable project, having a pipeline to reliably test, deploy, and publish the product is crucial for rapid iteration.\n\nBut with every CI system we used, we found ourselves constantly dealing with the same old problems: complicated configs hidden in many pages of the web UI, not knowing who changed what \u0026 when, managing dependencies and state on the workers, build pollution, annoying UX...\n\nOur project was growing larger, and with every box we checked and for every worker we hand-tweaked, the anxiety of having to do it all over again if something went wrong grew and grew. We started writing software to manage our CI instead of writing the software for the product we wanted to build.\n\nWe built Concourse to be a CI system that lets you sleep easier at night. A CI that's simple enough to fully grok and easy to manage as your project grows; in both the complexity of the product and the size of your team. We wanted to build a CI with strong abstractions and fewer things to learn, so that it can be easier to understand and so that Concourse can age gracefully.\n\nHow can I help?It's pretty hard to write a CI system that makes everyone happy! Concourse is by no means perfect, and it sometimes takes us while to understand a problem space well enough to figure out how it should work in Concourse's puritanical world.\n\nWe tend to move slowly rather than tack on feature request after feature request. We are also extremely cautious about anti-patterns and introducing ways for users to shoot themselves in the foot.\n\nConcourse is getting bigger and bigger, and we really appreciate any help we can get. There are many ways to contribute to Concourse; only some of them involve writing code! You help us out by being active in GitHub issues, voting with reactions for issues that matter to you, engaging in discussions while we map out a feature request, hanging out in our forums, writing documentation, coming up with new designs, and of course contributing code!\n\nAs a contributor, you can jump on over to Contribute for a more complete list of resources for contributors\n\n","depth":1,"section_tag":"project"},"put-step":{"location":"jobs.html#put-step","title":"put step","text":"","depth":3,"section_tag":"steps"},"quick-start":{"location":"quick-start.html","title":"Quick Start","text":"Concourse is distributed as a single concourse binary, making it easy to run just about anywhere, especially with Docker.\n\nIf you'd like to just get Concourse running somewhere quickly so you can start to kick the tires, the easiest way is to use the Docker Compose quick-start:\n\n$ wget https://concourse-ci.org/docker-compose.yml\n$ docker-compose up -d\nCreating docs_concourse-db_1 ...\nCreating docs_concourse-db_1 ... done\nCreating docs_concourse_1 ...\nCreating docs_concourse_1 ... done\nConcourse will be running at localhost:8080. You can log in with the username/password as test/test.\n\nNext, install The fly CLI by downloading it from the web UI and target your local Concourse as the test user:\n\n$ fly -t tutorial login -c http://localhost:8080 -u test -p test\nlogging in to team 'main'\n\ntarget saved\nOnce you've confirmed everything is up and running, you can skip the rest of the Install section and head straight to the Concourse Tutorial or to the rest of the Concourse docs.\n\n","depth":3,"section_tag":"quick-start"},"rails-example":{"location":"rails-example.html","title":"Rails application testing example","text":"You can run the tests for a Rails that requires a specific version of ruby and relies on a Postges database.\n\n","depth":2,"section_tag":"rails-example"},"random-strategy":{"location":"container-placement.html#random-strategy","title":"The random strategy","text":"With the random strategy, the Running a web node places get, put and task containers on any worker, ignoring any affinity.\n\nAs this is truly random, this will be fine until one day it's not fine.\n\nTo use this strategy, set the following env var on the Running a web node:\n\nCONCOURSE_CONTAINER_PLACEMENT_STRATEGY=random\n","depth":4,"section_tag":"random-strategy"},"reducing-redundant-data":{"location":"global-resources.html#reducing-redundant-data","title":"Reducing redundant data","text":"The majority of Concourse resources will benefit from having versions shared globally because most resource versions have an external source of truth.\n\nFor example, a check for the git resource that pulls in the concourse/concourse repository will always return the same set of versions as an equivalent resource pointing to the same repository. By consolidating the checks and the versions, there will essentially only be one set of versions collected from the repository and saved into the database.\n\n","depth":5,"section_tag":"reducing-redundant-data"},"references":{"location":"php-example.html#references","title":"References","text":"* Jobs\n\n* Steps\n\n* Tasks\n\n","depth":3,"section_tag":"references"},"reliable-resource-version-history":{"location":"global-resources.html#reliable-resource-version-history","title":"Reliable Resource Version History","text":"Prior to global resources, a resource's version history was directly associated to the resource name. This meant that any changes to a resource's configuration without changing its name would basically append the versions from the new configuration after the old versions, which are no longer accurate to the current configuration.\n\nGlobal resources instead associates the resource versions to the resource's resourcetype and resourcesource. Therefore, whenever a resource definition changes, the versions will \"reset\" and change along with it, resulting in truthful and reliable version histories.\n\n","depth":5,"section_tag":"reliable-resource-version-history"},"resource-certs":{"location":"implementing-resource-types.html#resource-certs","title":"Certificate Propagation","text":"Certificates can be automatically propagated into each resource container, if the worker is configured to do so. The BOSH release configures this automatically, while the concourse binary must be given a --certs-dir flag pointing to the path containing the CA certificate bundle.\n\nThe worker's certificate directory will then be always mounted at /etc/ssl/certs, read-only, in each resource container created on the worker. There's no single standard path for this so we picked one that would work out of the box in most cases.\n\nThis approach to certificate configuration is similar in mindset to the propagation of http_proxy/https_proxy - certs are kind of a baseline assumption when deploying software, so Concourse should do its best to respect it out-of-the-box, especially as they're often used in tandem with a man-in-the-middle corporate SSL proxy. (In this way it doesn't feel too much like the anti-pattern of hand-tuning workers.)\n\n","depth":4,"section_tag":"resource-certs"},"resource-check":{"location":"implementing-resource-types.html#resource-check","title":"check: Check for new versions.","text":"A resource type's check script is invoked to detect new versions of the resource. It is given the configured source and current version on stdin, and must print the array of new versions, in chronological order, to stdout, including the requested version if it's still valid.\n\nThe request body will have the following fields:\n\n* source is an arbitrary JSON object which specifies the location of the resource, including any credentials. This is passed verbatim from the resource configuration.\n\n  For git this would be the repo URI, which branch, and a private key if necessary.\n\n* version is a JSON object with string fields, used to uniquely identify an instance of the resource. For git this would be a commit SHA.\n\n  This will be omitted from the first request, in which case the resource should return the current version (not every version since the resource's inception).\n\nFor example, here's what the input for a git resource may look like:\n\n{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": { \"ref\": \"61cebf\" }\n}\nUpon receiving this payload the git resource would probably do something like:\n\n[ -d /tmp/repo ] || git clone git://some-uri /tmp/repo\ncd /tmp/repo\ngit pull \u0026\u0026 git log 61cbef..HEAD\nNote that it conditionally clones; the container for checking versions is reused between checks, so that it can efficiently pull rather than cloning every time.\n\nAnd the output, assuming d74e01 is the commit immediately after 61cbef:\n\n[\n  { \"ref\": \"61cbef\" },\n  { \"ref\": \"d74e01\" },\n  { \"ref\": \"7154fe\" }\n]\nThe list may be empty, if there are no versions available at the source. If the given version is already the latest, an array with that version as the sole entry should be listed.\n\nIf your resource is unable to determine which versions are newer than the given version (e.g. if it's a git commit that was push -fed over), then the current version of your resource should be returned (i.e. the new HEAD).\n\n","depth":4,"section_tag":"resource-check"},"resource-checker-components":{"location":"checker.html#resource-checker-components","title":"Resource checker components","text":"As of Concourse v5.6.0, resource checking has been redesigned to be asynchronous. There are now two processes that dictate how resource checking happens. The scanner, which determines if new checks need to run, and the checker which runs them.\n\nThe scanner will run every CONCOURSE_LIDAR_SCANNER_INTERVAL. It's job is to determine if new checks need to run. It will loop over every resource (and resource type) and if the check interval has elapsed since it last ran, it will schedule a new check for that resource.\n\nThe checker will run every CONCOURSE_LIDAR_CHECKER_INTERVAL. It's job is to run all the checks that were scheduled, which can be scheduled through the scanner or the manual resource checks.\n\n","depth":5,"section_tag":"resource-checker-components"},"resource-in":{"location":"implementing-resource-types.html#resource-in","title":"in: Fetch a given resource.","text":"The in script is passed a destination directory as command line argument $1, and is given on stdin the configured source and a precise version of the resource to fetch.\n\nThe script must fetch the resource and place it in the given directory.\n\nIf the desired resource version is unavailable (for example, if it was deleted), the script must error.\n\nThe script must emit the fetched version, and may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.\n\nThe request will contain the following fields:\n\n* source is the same value as passed to check: Check for new versions..\n\n* version is the same type of value passed to check: Check for new versions., and specifies the version to fetch.\n\n* params is an arbitrary JSON object passed along verbatim from get step params on a get step.\n\nExample request, in this case for the git resource:\n\n{\n  \"source\": {\n    \"uri\": \"git://some-uri\",\n    \"branch\": \"develop\",\n    \"private_key\": \"...\"\n  },\n  \"version\": { \"ref\": \"61cebf\" }\n}\nUpon receiving this payload the git resource would probably do something like:\n\ngit clone --branch develop git://some-uri $1\ncd $1\ngit checkout 61cebf\nAnd output:\n\n{\n  \"version\": { \"ref\": \"61cebf\" },\n  \"metadata\": [\n    { \"name\": \"commit\", \"value\": \"61cebf\" },\n    { \"name\": \"author\", \"value\": \"Hulk Hogan\" }\n  ]\n}\n","depth":4,"section_tag":"resource-in"},"resource-metadata":{"location":"implementing-resource-types.html#resource-metadata","title":"Metadata","text":"When used in a get step or a put step, metadata about the running build is made available via the following environment variables:\n\n$BUILD_ID: The internal identifier for the build. Right now this is numeric but it may become a guid in the future. Treat it as an absolute reference to the build.\n\n\n$BUILD_NAME: The build number within the build's job.\n\n\n$BUILD_JOB_NAME: The name of the build's job.\n\n\n$BUILD_PIPELINE_NAME: The pipeline that the build's job lives in.\n\n\n$BUILD_TEAM_NAME: The team that the build belongs to.\n\n\n$ATC_EXTERNAL_URL: The public URL for your ATC; useful for debugging.\n\n\n\nIf the build is a one-off, $BUILD_NAME, $BUILD_JOB_NAME, and $BUILD_PIPELINE_NAME will not be set.\n\nNone of these variables are available to /check.\n\nThese variables should be used solely for annotating things with metadata for traceability, i.e. for linking to the build in an alert or annotating an automated commit so its origin can be discovered.\n\nThey should not be used to emulate versioning (e.g. by using the increasing build number). They are not provided to task steps to avoid this anti-pattern.\n\n","depth":4,"section_tag":"resource-metadata"},"resource-out":{"location":"implementing-resource-types.html#resource-out","title":"out: Update a resource.","text":"The out script is called with a path to the directory containing the build's full set of sources as the first argument, and is given on stdin the configured params and the resource's source configuration.\n\nThe script must emit the resulting version of the resource. For example, the git resource emits the sha of the commit that it just pushed.\n\nAdditionally, the script may emit metadata as a list of key-value pairs. This data is intended for public consumption and will make it upstream, intended to be shown on the build's page.\n\nThe request will contain the following fields:\n\n* source is the same value as passed to check: Check for new versions..\n\n* params is an arbitrary JSON object passed along verbatim from get step params on a put step.\n\nExample request, in this case for the git resource:\n\n{\n  \"params\": {\n    \"branch\": \"develop\",\n    \"repo\": \"some-repo\"\n  },\n  \"source\": {\n    \"uri\": \"git@...\",\n    \"private_key\": \"...\"\n  }\n}\nUpon receiving this payload the git resource would probably do something like:\n\ncd $1/some-repo\ngit push origin develop\nAnd output:\n\n{\n  \"version\": { \"ref\": \"61cebf\" },\n  \"metadata\": [\n    { \"name\": \"commit\", \"value\": \"61cebf\" },\n    { \"name\": \"author\", \"value\": \"Mick Foley\" }\n  ]\n}\n","depth":4,"section_tag":"resource-out"},"resource-types":{"location":"resource-types.html","title":"Resource Types","text":"Each resource in a pipeline has a type. The resource's type determines what versions are detected, the bits that are fetched when the resource's get step runs, and the side effect that occurs when the resource's put step runs.\n\nConcourse comes with a few \"core\" resource types to cover common use cases like git and s3 - the rest are developed and supported by the Concourse community. An exhaustive list of all resource types is available in the Resource Types catalog.\n\nA pipeline's resource types are listed under pipelineresource_types with the following schema:\n\nResource Types can be used to extend the functionality of your pipeline and provide deeper integrations. This example uses one to trigger a job whenever a new Dinosaur Comic is out.\n\n---\nresource_types:\n- name: rss\n  type: docker-image\n  source:\n    repository: suhlig/concourse-rss-resource\n    tag: latest\n\nresources:\n- name: booklit-releases\n  type: rss\n  source:\n    url: http://www.qwantz.com/rssfeed.php\n\njobs:\n- name: announce\n  plan:\n  - get: booklit-releases\n    trigger: true\n","depth":2,"section_tag":"resource-types"},"resource-versions":{"location":"resource-versions.html","title":"Resource Versions","text":"As you may know, resources represent external state that changes over time. But how do we track those changes in a generic way that will properly represent all the different resource types? That is where resource versions are introduced. Concourse uses versions to represent the exact changes of a resource over time.\n\nThe versions of a resource is directly dependent on it's resource configuration and resource type. Each resource type has their own definition of what it's versions should be. For example, the versions of a git resource would be the commits of the github repository and the versions of a docker image resource are the image digests.\n\nIf you want to figure out what determines the version of a resource type, it is typically outlined in the `check` behavior for the resource type. For example, the git resource uses commits as versions git resource type check behavior.\n\n","depth":3,"section_tag":"resource-versions"},"resources":{"location":"resources.html","title":"Resources","text":"Resources are the heart and soul of Concourse. They represent all external inputs to and outputs of jobs in the pipeline.\n\nEach resource represents a versioned artifact with an external source of truth. Configuring the same resource in any pipeline on any Concourse cluster will behave the exact same way. Concouse will continuously check each configured resource to discover new versions. These versions then flow through the pipeline via get steps configured on Jobs.\n\nA pipeline's resources are listed under pipelineresources with the following schema.\n\n","depth":2,"section_tag":"resources"},"restarting-a-worker":{"location":"concourse-worker.html#restarting-a-worker","title":"Restarting a Worker","text":"Workers can be restarted in-place by sending SIGTERM to the worker process and starting it back up. Containers will remain running and Concourse will reattach to builds that were in flight.\n\nThis is a pretty aggressive way to restart a worker, and may result in errored builds - there are a few moving parts involved and we're still working on making this airtight.\n\nA safer way to restart a worker is to land it by sending SIGUSR1 to the worker process. This will switch the worker to landing state, and Concourse will stop scheduling new work on it. When all builds running on the worker have finished, the process will exit.\n\nYou may want to enforce a timeout for draining - that way a stuck build won't prevent your workers from being upgraded. This can be enforced by common tools like start-stop-daemon:\n\nstart-stop-daemon \\\n  --pidfile worker.pid \\\n  --stop \\\n  --retry USR1/300/TERM/15/KILL\nThis will send SIGUSR1, wait up to 5 minutes, and then send SIGTERM. If it's still running, it will be killed after an additional 15 seconds.\n\nOnce the timeout is enforced, there's still a chance that builds that were running will continue when the worker comes back.\n\n","depth":5,"section_tag":"restarting-a-worker"},"restarting-and-upgrading":{"location":"concourse-web.html#restarting-and-upgrading","title":"Restarting \u0026 Upgrading","text":"The web nodes can be killed and restarted willy-nilly. No draining is necessary; if the web node was orchestrating a build it will just continue where it left off when it comes back, or the build will be picked up by one of the other web nodes.\n\nTo upgrade a web node, stop its process and start a new one using the newly installed concourse. Any migrations will be run automatically on start. If web nodes are started in parallel, only one will run the migrations.\n\nNote that we don't currently guarantee a lack of funny-business if you're running mixed Concourse versions - database migrations can perform modifications that confuse other web nodes. So there may be some turbulence during a rolling upgrade, but everything should stabilize once all web nodes are running the latest version.\n\n","depth":5,"section_tag":"restarting-and-upgrading"},"risks-and-side-effects":{"location":"global-resources.html#risks-and-side-effects","title":"Risks and Side Effects","text":"","depth":4,"section_tag":"risks-and-side-effects"},"rotating-the-encryption-key":{"location":"encryption.html#rotating-the-encryption-key","title":"Rotating the Encryption Key","text":"To swap out the encryption key, you'll need to pass the previous key as --old-encryption-key (or old_encryption_key), and the new key as --encryption-key (or encryption_key).\n\nOn startup, the Running a web node will decrypt all existing data and re-encrypt it with the new key, in one go. If it encounters a row which is already encrypted with the new key, it will continue on (as may be the case when restarting with the flags again, or if the ATC died in the middle of rotating).\n\nIf the ATC encounters a row which cannot be decrypted with neither the old key nor the new one, it will log loudly and fail to start, telling you which row it choked on. This data must be dealt with in some way, either by re-configuring the key the row was encrypted with as the old key, or manually performing database surgery to remove the offending row. Hopefully this doesn't happen to you!\n\n","depth":4,"section_tag":"rotating-the-encryption-key"},"running-tasks":{"location":"tasks.html#running-tasks","title":"Running tasks with fly execute","text":"One of the most common use cases of fly is taking a local project on your computer and submitting it up with a task configuration to be run inside a container in Concourse. This is useful to build Linux projects on OS X or to avoid all of those debugging commits when something is configured differently between your local and remote setup.\n\nYou can execute a task like this:\n\n$ fly -t example execute --config tests.yml\nYour files will be uploaded and the task will be executed with them. The working directory name will be used as the input name. If they do not match, you must specify -i name=. instead, where name is the input name from the task configuration.\n\nFly will automatically capture SIGINT and SIGTERM and abort the build when received. This allows it to be transparently composed with other toolchains.\n\nBy default, Fly will not send extra files or large files in your current directory that would normally be ignored by your version control system. You can use the --include-ignored flags in order to send ignored files to Concourse along with those that are not ignored.\n\nIf your task needs to run as root then you can specify the -p or --privileged flag.\n\nTasks in Concourse can take multiple inputs. Up until now we've just been submitting a single input (our current working directory) that has the same name as the directory.\n\nTasks must specify the inputs that they require as taskinputs. For fly to upload these inputs you can use the -i or --input arguments with name and path pairs. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --input stemcells=../stemcells\nThis would work together with a build-stemcell.yml if its inputs: section was as follows:\n\ninputs:\n- name: code\n- name: stemcells\nIf you specify an input then the default input will no longer be added automatically and you will need to explicitly list it (as with the code input above).\n\nThis feature can be used to mimic other resources and try out combinations of input that would normally not be possible in a pipeline.\n\nIf the --inputs-from flag is given, the specified job will be looked up in the pipeline, and the one-off build will base its inputs on those currently configured for the job.\n\nIf any --input flags are given (see above), they will override the base set of inputs.\n\nFor example:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --input foo=./foo\nThis will trigger a one-off-build using the task.yml task config, basing its inputs on the latest candidates for the integration job in the main pipeline, with the foo input overridden to specify local code to run.\n\nThis can be used to more closely replicate the state in CI when weeding out flakiness, or as a shortcut for local development so that you don't have to upload every single resource from your local machine.\n\nWhen using --inputs-from as above, you can additionally specify which input to use as the task's image by passing --image input-name.\n\nFor example, the following pipeline fetches an image via a get step and uses it for task step image:\n\nresources:\n- name: my-repo\n  type: git\n  source: {uri: https://example.com}\n\n- name: some-image\n  type: registry-image\n  source: {repository: ubuntu}\n\njobs:\n- name: integration\n  plan:\n  - get: my-repo\n  - get: some-image\n  - task: my-task\n    file: my-repo/task.yml\n    image: some-image\n...so to run the same task with the same image in a one-off build, you would run:\n\n$ fly -t example execute \\\n    --config task.yml \\\n    --inputs-from main/integration \\\n    --image some-image\nIf a task specifies outputs then you're able to extract these back out of the build and back to your local system. For example:\n\n$ fly -t example execute \\\n    --config build-stemcell.yml \\\n    --input code=. \\\n    --output stemcell=/tmp/stemcell\nThis would work together with a build-stemcell.yml if its outputs: section was as follows:\n\noutputs:\n- name: stemcell\nThis feature is useful to farm work out to your Concourse server to build things in a repeatable manner.\n\nAny params listed in the task configuration can be specified by using environment variables.\n\nSo, if you have a task with the following params:\n\nparams:\n  FOO: fizzbuzz\n  BAR:\n...and you run:\n\nBAR=hello fly execute\nThe task would then run with BAR as \"hello\", and FOO as \"fizzbuzz\" (its default value).\n\nTask config files can contain Vars which can can be set during fly execute by using the -v, -y and -l flags:\n\nfly -t example execute --config tests.yml \\\n  -l vars.yml \\\n  -v some_string=\"Hello World!\" \\\n  -y some_bool=true\nAny variables not satisfied via the above flags will be deferred to the configured credential manager.\n\nTo satisfy these vars when running the task in a pipeline, see task step vars.\n\nIf you want to execute a task on a worker that has a specific tag, you can do so by passing --tag:\n\nfly -t example execute --config task.yml --tag bar\nThis will execute the task specified by task.yml on a worker that has been tagged bar.\n\n","depth":3,"section_tag":"running-tasks"},"scaling":{"location":"concourse-web.html#scaling","title":"Scaling","text":"The Running a web node can be scaled up for high availability. They'll also roughly share their scheduling workloads, using the database to synchronize. This is done by just running more web commands on different machines, and optionally putting them behind a load balancer.\n\nTo run a cluster of Running a web nodes, you'll first need to ensure they're all pointing to the same PostgreSQL server.\n\nNext, you'll need to configure a peer address. This is a DNS or IP address that can be used to reach this web node from other web nodes. Typically this uses a private IP, like so:\n\nCONCOURSE_PEER_ADDRESS=10.10.0.1\nThis address will be used for forwarded worker connections, which listen on the ephemeral port range.\n\nFinally, if all of these nodes are going to be accessed through a load balancer, you'll need to configure the external URL that will be used to reach your Concourse cluster:\n\nCONCOURSE_EXTERNAL_URL=https://ci.example.com\nAside from the peer URL, all configuration must be consistent across all web nodes in the cluster to ensure consistent results.\n\n","depth":5,"section_tag":"scaling"},"scaling-workers":{"location":"concourse-worker.html#scaling-workers","title":"Scaling Workers","text":"More workers should be added to accomodate more pipelines. To know when this is necessary you should probably set up Metrics and keep an eye on container counts. If it's starting to approach 200 or so, you should probably add another worker. Load average is another metric to keep an eye on.\n\nTo add a worker, just create another machine for the worker and follow the Running concourse worker instructions again.\n\nNote: it doesn't really make sense to run multiple workers on one machine, since they'll both just be contending for the same physical resources. Workers should be given their own VMs or physical machines.\n\n","depth":5,"section_tag":"scaling-workers"},"scheduler":{"location":"scheduler.html","title":"Build Scheduler","text":"As of the v6.0.0 release, there have been many changes to the scheduler so it would be advisable to assume that this documentation should only be used for Concourse deployments v6.0.0 and above.\n\nBuilds represent each execution of a job. Figuring out when to schedule a new build is the responsibility of the build scheduler. The scheduling of new builds can be dependent on many different factors such as when a new version of a resource is discovered, when a dependent upstream build finishes, or when a user manually triggers a build.\n\nThe build scheduler is a global component, where it deals with all the jobs within a deployment. It runs on an interval with a default of 10 seconds. If there are multiple ATCs, only one of the ATC's scheduler component will run per interval tick in order to ensure that there will be no duplicated work between ATC nodes.\n\nThe subcomponent used to figure out whether a build can be scheduled is called the algorithm.\n\n","depth":4,"section_tag":"scheduler"},"scheduling-behavior":{"location":"scheduler.html#scheduling-behavior","title":"Scheduling behavior","text":"The scheduler will schedule a new build if any of the versions produced by the algorithm for trigger: true resources is not equal to the version used by the previous build of the job.\n\nWhat this means is if the algorithm runs and computes an input version, the scheduler will create a new build as long as that version is different than the previous build's version for that same input. Even if that version has been used by a build 2 months ago, the scheduler will schedule a new build as long as it has not been used by the previous build.\n\nIf there are any input versions that are different than the previous build, it will trigger a new build. This can affect some users that are using the pinning functionality to run older versions.\n\nLet's say you have a job that takes a resource as an input. It currently has two builds, build 1 has ran with version v1 of the resource and build 2 has ran with v2.\n\nimages/new-pinning-behavior-1.pngIf I pin the input to an older version, a new build is produced because the pinned version v1 is not equivalent to the version of the previous build v2.\n\nimages/new-pinning-behavior-3.pngThen if I unpin the version, another build would be triggered again using the latest version.\n\nimages/new-pinning-behavior-4.pngThis is because after unpinning the resource, the input version for the next build becomes the latest version v2 which is not equal to the version v1 used by the previous build.\n\nThis is to allow the current state of the builds to always reflect the current state of the versions. That being said, this behavior can be undesirable if the goal is to only run a job using an old version. In this case, build rerunning is the better choice. If you would like to learn more about build rerunning, you can read more about it in the build rerunning section.\n\n","depth":5,"section_tag":"scheduling-behavior"},"scheduling-on-demand":{"location":"scheduler.html#scheduling-on-demand","title":"Scheduling on demand","text":"The scheduler runs on an interval, but rather than scheduling all the jobs within a deployment on every tick, it only schedules the jobs that need to be scheduled.\n\nFirst, the scheduler determines which jobs need to be scheduled. Below are all the reasons why Concourse will think a job needs to be scheduled: * Detecting new versions of a resource through a check\n\n* Saving a new version through a put\n\n* A build finishes for an upstream job (through passed constraints)\n\n* Enabling/Disabling a resource version\n\n* Pinning/Unpinning a resource version\n\n* Setting a pipeline\n\n* Updating a resource's resource_config\n\n* Manually triggering a build\n\n* Rerunning a build\n\n* Multiple versions available for a version every constraint\n\n\n\nEach job that is scheduled will use the algorithm to determine what inputs its next build should have. Then the build is scheduled and picked up by the Build Tracker.\n\n","depth":5,"section_tag":"scheduling-on-demand"},"schema.anonymous_resource":{"location":"tasks.html#schema.anonymous_resource","title":"anonymous_resource schema","text":"","depth":2,"section_tag":"tasks"},"schema.anonymous_resource.params":{"location":"tasks.html#schema.anonymous_resource.params","title":"anonymous_resourceparams","text":"A map of arbitrary configuration to forward to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":2,"section_tag":"tasks"},"schema.anonymous_resource.source":{"location":"tasks.html#schema.anonymous_resource.source","title":"anonymous_resourcesource","text":"The configuration for the resource; see resourcesource.\n\n","depth":2,"section_tag":"tasks"},"schema.anonymous_resource.type":{"location":"tasks.html#schema.anonymous_resource.type","title":"anonymous_resourcetype","text":"The type of the resource. Usually registry-image.\n\nYou can use any resource type that returns a filesystem in the correct format: a /rootfs directory containing a full filesystem, and a metadata.json file containing.\n\n","depth":2,"section_tag":"tasks"},"schema.anonymous_resource.version":{"location":"tasks.html#schema.anonymous_resource.version","title":"anonymous_resourceversion","text":"A specific version of the resource to fetch. This should be a map with string keys and values. If not specified, the latest version will be fetched.\n\n","depth":2,"section_tag":"tasks"},"schema.boolean":{"location":"config-basics.html#schema.boolean","title":"boolean schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.build_log_retention_policy":{"location":"jobs.html#schema.build_log_retention_policy","title":"build_log_retention_policy schema","text":"","depth":2,"section_tag":"jobs"},"schema.build_log_retention_policy.builds":{"location":"jobs.html#schema.build_log_retention_policy.builds","title":"build_log_retention_policybuilds","text":"Keep logs for the last specified number of builds.\n\n","depth":2,"section_tag":"jobs"},"schema.build_log_retention_policy.days":{"location":"jobs.html#schema.build_log_retention_policy.days","title":"build_log_retention_policydays","text":"Keep logs for builds which have finished within the specified number of days.\n\n","depth":2,"section_tag":"jobs"},"schema.build_log_retention_policy.minimum_succeeded_builds":{"location":"jobs.html#schema.build_log_retention_policy.minimum_succeeded_builds","title":"build_log_retention_policyminimum_succeeded_builds","text":"Keep a minimum number of successful build logs that would normally be reaped.\n\nRequires builds to be set to an integer higher than 0 in order to work. For example, if builds is set to 5, and this attribute to 1, say a job has the following build history: 7(f), 6(f), 5(f), 4(f), 3(f), 2(f), 1(s), where f means failed and s means succeeded, then builds 2 and 3 will be reaped, because it retains 5 build logs, and at least 1 succeeded build log. Default is 0.\n\n","depth":2,"section_tag":"jobs"},"schema.cache":{"location":"tasks.html#schema.cache","title":"cache schema","text":"","depth":2,"section_tag":"tasks"},"schema.cache.path":{"location":"tasks.html#schema.cache.path","title":"cachepath","text":"The path to a directory to be cached.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"schema.command":{"location":"tasks.html#schema.command","title":"command schema","text":"","depth":2,"section_tag":"tasks"},"schema.command.args":{"location":"tasks.html#schema.command.args","title":"commandargs","text":"Arguments to pass to the command. Note that when executed with Fly, any arguments passed to Fly are appended to this array.\n\n","depth":2,"section_tag":"tasks"},"schema.command.dir":{"location":"tasks.html#schema.command.dir","title":"commanddir","text":"A directory, relative to the initial working directory, to set as the working directory when running the script.\n\n","depth":2,"section_tag":"tasks"},"schema.command.path":{"location":"tasks.html#schema.command.path","title":"commandpath","text":"The name of or path to the executable to run.\n\npath is relative to the working directory. If dir is specified to set the working directory, then path is relative to it.\n\nThis is commonly a path to a script provided by one of the task's inputs, e.g. my-resource/scripts/test. It could also be a command like bash (respecting standard $PATH lookup rules), or an absolute path to a file to execute, e.g. /bin/bash.\n\n","depth":2,"section_tag":"tasks"},"schema.command.user":{"location":"tasks.html#schema.command.user","title":"commanduser","text":"Explicitly set the user to run as. If not specified, this defaults to the user configured by the task's image. If not specified there, it's up to the Garden backend, and may be e.g. root on Linux.\n\n","depth":2,"section_tag":"tasks"},"schema.config":{"location":"config-basics.html#schema.config","title":"config schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.container_limits":{"location":"tasks.html#schema.container_limits","title":"container_limits schema","text":"","depth":2,"section_tag":"tasks"},"schema.container_limits.cpu":{"location":"tasks.html#schema.container_limits.cpu","title":"container_limitscpu","text":"The maximum amount of CPU available to the task container, measured in shares. 0 means unlimited.\n\n","depth":2,"section_tag":"tasks"},"schema.container_limits.memory":{"location":"tasks.html#schema.container_limits.memory","title":"container_limitsmemory","text":"The maximum amount of memory available to the task container, measured in bytes. 0 means unlimited.\n\n","depth":2,"section_tag":"tasks"},"schema.dir-path":{"location":"config-basics.html#schema.dir-path","title":"dir-path schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.dummy_config":{"location":"vars.html#schema.dummy_config","title":"dummy_config schema","text":"","depth":4,"section_tag":"var-sources"},"schema.dummy_config.vars":{"location":"vars.html#schema.dummy_config.vars","title":"dummy var source vars","text":"A mapping of var name to var value.\n\n","depth":4,"section_tag":"var-sources"},"schema.duration":{"location":"config-basics.html#schema.duration","title":"duration schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.env-vars":{"location":"config-basics.html#schema.env-vars","title":"env-vars schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.file-path":{"location":"config-basics.html#schema.file-path","title":"file-path schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.group_config":{"location":"pipelines.html#schema.group_config","title":"group_config schema","text":"","depth":2,"section_tag":"pipelines"},"schema.group_config.jobs":{"location":"pipelines.html#schema.group_config.jobs","title":"group_configjobs","text":"A list of jobs that should appear in this group. A job may appear in multiple groups. Neighbours of jobs in the current group will also appear on the same page in order to give context of the location of the group in the pipeline.\n\n","depth":2,"section_tag":"pipelines"},"schema.group_config.name":{"location":"pipelines.html#schema.group_config.name","title":"group_configname","text":"A unique name for the group. This should be short and simple as it will be used as the tab name for navigation.\n\n","depth":2,"section_tag":"pipelines"},"schema.identifier":{"location":"config-basics.html#schema.identifier","title":"identifier schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.in_parallel_config":{"location":"jobs.html#schema.in_parallel_config","title":"in_parallel_config schema","text":"","depth":3,"section_tag":"steps"},"schema.in_parallel_config.fail_fast":{"location":"jobs.html#schema.in_parallel_config.fail_fast","title":"in_parallel step fail_fast","text":"Default false. When enabled the parallel step will fail fast by returning as soon as any sub-step fails. This means that running steps will be interrupted and pending steps will no longer be scheduled.\n\n","depth":3,"section_tag":"steps"},"schema.in_parallel_config.limit":{"location":"jobs.html#schema.in_parallel_config.limit","title":"in_parallel step limit","text":"Default unlimited. A sempahore which limits the parallelism when executing the steps in a in_parallel step. When set, the number of running steps will not exceed the limit.\n\nWhen not specified, in_parallel will execute all steps immediately, making the default behavior identical to aggregate.\n\n","depth":3,"section_tag":"steps"},"schema.in_parallel_config.steps":{"location":"jobs.html#schema.in_parallel_config.steps","title":"in_parallel step steps","text":"The steps to perform in parallel.\n\n","depth":3,"section_tag":"steps"},"schema.input":{"location":"tasks.html#schema.input","title":"input schema","text":"","depth":2,"section_tag":"tasks"},"schema.input.name":{"location":"tasks.html#schema.input.name","title":"inputname","text":"The name of the input.\n\n","depth":2,"section_tag":"tasks"},"schema.input.optional":{"location":"tasks.html#schema.input.optional","title":"inputoptional","text":"Default false. If true, then the input is not required by the task. The task may run even if this input is missing.\n\nAn optional input that is missing will not appear in the current directory of the running task.\n\n","depth":2,"section_tag":"tasks"},"schema.input.path":{"location":"tasks.html#schema.input.path","title":"inputpath","text":"The path where the input will be placed. If not specified, the input's name is used.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"schema.job":{"location":"jobs.html#schema.job","title":"job schema","text":"","depth":2,"section_tag":"jobs"},"schema.job.build_log_retention":{"location":"jobs.html#schema.job.build_log_retention","title":"jobbuild_log_retention","text":"Configures the retention policy for build logs. This is useful if you have a job that runs often but after some amount of time the logs aren't worth keeping around.\n\nBuilds which are not retained by the configured policy will have their logs reaped. If this configuration is omitted, logs are kept forever.\n\nThe following example will keep logs for any builds that have completed in the last 2 days, while also keeping the last 1000 builds and at least 1 succeeded build.\n\njobs:\n- name: smoke-tests\n  build_log_retention:\n    days: 2\n    builds: 1000\n    minimum_succeeded_builds: 1\n  plan:\n  - get: 10m\n  - task: smoke-tests\n    # ...\nIf more than 1000 builds finish in the past 2 days, all of them will be retained thanks to the days configuration. Similarly, if there are 1000 builds spanning more than 2 days, they will also be kept thanks to the builds configuration. And if they all happened to have failed, the minimum_succeeded_builds will keep around at least one successful build. All policies operate independently.\n\n","depth":2,"section_tag":"jobs"},"schema.job.build_logs_to_retain":{"location":"jobs.html#schema.job.build_logs_to_retain","title":"jobbuild_logs_to_retain","text":"Deprecated. Equivalent to setting job.build_log_retention.builds.\n\n","depth":2,"section_tag":"jobs"},"schema.job.disable_manual_trigger":{"location":"jobs.html#schema.job.disable_manual_trigger","title":"jobdisable_manual_trigger","text":"Default false. If set to true, manual triggering of the job (via the web UI or fly trigger-job) will be disabled.\n\n","depth":2,"section_tag":"jobs"},"schema.job.ensure":{"location":"jobs.html#schema.job.ensure","title":"jobensure","text":"Step to execute regardless of whether the job succeeds, fails, errors, or aborts. Equivalent to the stepensure hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.interruptible":{"location":"jobs.html#schema.job.interruptible","title":"jobinterruptible","text":"Default false. Normally, when a worker is shutting down it will wait for builds with containers running on that worker to finish before exiting. If this value is set to true, the worker will not wait on the builds of this job. You may want this if e.g. you have a self-deploying Concourse or long-running-but-low-importance jobs.\n\n","depth":2,"section_tag":"jobs"},"schema.job.max_in_flight":{"location":"jobs.html#schema.job.max_in_flight","title":"jobmax_in_flight","text":"If set, specifies a maximum number of builds to run at a time. If serial or serial_groups are set, they take precedence and force this value to be 1.\n\n","depth":2,"section_tag":"jobs"},"schema.job.name":{"location":"jobs.html#schema.job.name","title":"jobname","text":"The name of the job. This should be short; it will show up in URLs.\n\n","depth":2,"section_tag":"jobs"},"schema.job.old_name":{"location":"jobs.html#schema.job.old_name","title":"jobold_name","text":"The old name of the job. If configured, the history of old job will be inherited to the new one. Once the pipeline is set, this field can be removed as the builds have been transfered.\n\nThis can be used to rename a job without losing its history, like so:\n\njobs:\n- name: new-name\n  old_name: current-name\n  plan: [get: 10m]\nAfter the pipeline is set, because the builds have been inherited, the job can have the field removed:\n\njobs:\n- name: new-name\n  plan: [get: 10m]\n","depth":2,"section_tag":"jobs"},"schema.job.on_abort":{"location":"jobs.html#schema.job.on_abort","title":"jobon_abort","text":"Step to execute when the job aborts. Equivalent to the stepon_abort hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.on_error":{"location":"jobs.html#schema.job.on_error","title":"jobon_error","text":"Step to execute when the job errors. Equivalent to the stepon_error hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.on_failure":{"location":"jobs.html#schema.job.on_failure","title":"jobon_failure","text":"Step to execute when the job fails. Equivalent to the stepon_failure hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.on_success":{"location":"jobs.html#schema.job.on_success","title":"jobon_success","text":"Step to execute when the job succeeds. Equivalent to the stepon_success hook.\n\n","depth":2,"section_tag":"jobs"},"schema.job.plan":{"location":"jobs.html#schema.job.plan","title":"jobplan","text":"The sequence of steps to execute.\n\n","depth":2,"section_tag":"jobs"},"schema.job.public":{"location":"jobs.html#schema.job.public","title":"jobpublic","text":"Default false. If set to true, the build log of this job will be viewable by unauthenticated users. Unauthenticated users will always be able to see the inputs, outputs, and build status history of a job. This is useful if you would like to expose your pipeline publicly without showing sensitive information in the build log.\n\nNote: when this is set to true, any get step and put steps will show the metadata for their resource version, regardless of whether the resource itself has set resourcepublic to true.\n\n","depth":2,"section_tag":"jobs"},"schema.job.serial":{"location":"jobs.html#schema.job.serial","title":"jobserial","text":"Default false. If set to true, builds will queue up and execute one-by-one, rather than executing in parallel.\n\n","depth":2,"section_tag":"jobs"},"schema.job.serial_groups":{"location":"jobs.html#schema.job.serial_groups","title":"jobserial_groups","text":"Default []. When set to an array of arbitrary tag-like strings, builds of this job and other jobs referencing the same tags will be serialized.\n\nThis can be used to ensure that certain jobs do not run at the same time, like so:\n\njobs:\n- name: job-a\n  serial_groups: [some-tag]\n- name: job-b\n  serial_groups: [some-tag, some-other-tag]\n- name: job-c\n  serial_groups: [some-other-tag]\nIn this example, job-a and job-c can run concurrently, but neither job can run builds at the same time as job-b.\n\nThe builds are executed in their order of creation, across all jobs with common tags.\n\n","depth":2,"section_tag":"jobs"},"schema.number":{"location":"config-basics.html#schema.number","title":"number schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.output":{"location":"tasks.html#schema.output","title":"output schema","text":"","depth":2,"section_tag":"tasks"},"schema.output.name":{"location":"tasks.html#schema.output.name","title":"outputname","text":"The name of the output. The contents under path will be made available to the rest of the plan under this name.\n\n","depth":2,"section_tag":"tasks"},"schema.output.path":{"location":"tasks.html#schema.output.path","title":"outputpath","text":"The path to a directory where the output will be taken from. If not specified, the output's name is used.\n\nPaths are relative to the working directory of the task. Absolute paths are not respected.\n\n","depth":2,"section_tag":"tasks"},"schema.pipeline":{"location":"pipelines.html#schema.pipeline","title":"pipeline schema","text":"","depth":2,"section_tag":"pipelines"},"schema.pipeline.groups":{"location":"pipelines.html#schema.pipeline.groups","title":"pipelinegroups","text":"A list of job groups to use for organizing jobs in the web UI.\n\nGroups have no functional effect on your pipeline. They are purely for making it easier to grok large pipelines in the web UI.\n\nNote: once you have added groups to your pipeline, all jobs must be in a group.\n\nThe following example will make the \"tests\" group the default view (since it's listed first), separating the later jobs into a \"publish\" group:\n\ngroups:\n- name: test\n  jobs:\n  - unit\n  - integration\n- name: publish\n  jobs:\n  - deploy\n  - shipit\nThis would display two tabs at the top of the home page: \"test\" and \"publish\".\n\nFor a real world example of how groups can be used to simplify navigation and provide logical grouping, see the groups used at the top of the page in the Concourse pipeline.\n\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.jobs":{"location":"pipelines.html#schema.pipeline.jobs","title":"pipelinejobs","text":"A set of jobs for the pipeline to continuously schedule.\n\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.resource_types":{"location":"pipelines.html#schema.pipeline.resource_types","title":"pipelineresource_types","text":"A set of resource types for resources within the pipeline to use.\n\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.resources":{"location":"pipelines.html#schema.pipeline.resources","title":"pipelineresources","text":"A set of resources for the pipeline to continuously check.\n\n","depth":2,"section_tag":"pipelines"},"schema.pipeline.var_sources":{"location":"pipelines.html#schema.pipeline.var_sources","title":"pipelinevar_sources","text":"A set of Var sources (experimental) for the pipeline to use.\n\n","depth":2,"section_tag":"pipelines"},"schema.resource":{"location":"resources.html#schema.resource","title":"resource schema","text":"","depth":2,"section_tag":"resources"},"schema.resource.check_every":{"location":"resources.html#schema.resource.check_every","title":"resourcecheck_every","text":"Default 1m. The interval on which to check for new versions of the resource. Acceptable interval options are defined by the time.ParseDuration function.\n\n","depth":2,"section_tag":"resources"},"schema.resource.icon":{"location":"resources.html#schema.resource.icon","title":"resourceicon","text":"The name of a Material Design icon to show next to the resource name in the web UI. For example, github-circle.\n\n","depth":2,"section_tag":"resources"},"schema.resource.name":{"location":"resources.html#schema.resource.name","title":"resourcename","text":"The name of the resource. This should be short and simple. This name will be referenced by build plans of jobs in the pipeline.\n\n","depth":2,"section_tag":"resources"},"schema.resource.public":{"location":"resources.html#schema.resource.public","title":"resourcepublic","text":"Default false. If set to true, the metadata for each version of the resource will be viewable by unauthenticated users (assuming the pipeline has been exposed).\n\nResource metadata should never contain credentials or secret information, but this is off by default just to be safe in case users don't want to show things like commit messages and authors to the public.\n\nNote: even when set to false, the versions identifiers will be visible. In addition, if a resource is fetched in a build whose job is marked jobpublic, metadata will be visible in the build output.\n\n","depth":2,"section_tag":"resources"},"schema.resource.source":{"location":"resources.html#schema.resource.source","title":"resourcesource","text":"The location of the resource. This varies by resource type, and is a black box to Concourse; it is blindly passed to the resource at runtime.\n\nTo use git as an example, the source may contain the repo URI, the branch of the repo to track, and a private key to use when pushing/pulling.\n\nBy convention, documentation for each resource type's configuration is in each implementation's README.\n\nYou can find the source for the resource types provided with Concourse at the Concourse GitHub organization.\n\n","depth":2,"section_tag":"resources"},"schema.resource.tags":{"location":"resources.html#schema.resource.tags","title":"resourcetags","text":"Default []. A list of tags to determine which workers the checks will be performed on. You'll want to specify this if the source is internal to a worker's network, for example. See also steptags.\n\n","depth":2,"section_tag":"resources"},"schema.resource.type":{"location":"resources.html#schema.resource.type","title":"resourcetype","text":"The resource type implementing the resource.\n\n","depth":2,"section_tag":"resources"},"schema.resource.version":{"location":"resources.html#schema.resource.version","title":"resourceversion","text":"A version to pin the resource to across the pipeline. This has the same effect as setting get step version on every get step referencing the resource.\n\nResources can also be temporarily pinned to a version via the API and web UI. However this functionality is disabled if the resource is pinned via configuration, and if a pipeline is configured to have a version pinned while also pinned in the web UI, the configuration takes precedence and will clear out the temporary pin.\n\n","depth":2,"section_tag":"resources"},"schema.resource.webhook_token":{"location":"resources.html#schema.resource.webhook_token","title":"resourcewebhook_token","text":"If specified, web hooks can be sent to trigger an immediate check of the resource, specifying this value as a primitive form of authentication via query params.\n\nAfter configuring this value, you would then configure your hook sender with the following painfully long path appended to your external URL:\n\n/api/v1/teams/TEAM_NAME/pipelines/PIPELINE_NAME/resources/RESOURCE_NAME/check/webhook?webhook_token=WEBHOOK_TOKEN\nNote that the request payload sent to this API endpoint is entirely ignored. You should configure the resource as if you're not using web hooks, as the resource config is still the \"source of truth.\"\n\n","depth":2,"section_tag":"resources"},"schema.resource_type":{"location":"resource-types.html#schema.resource_type","title":"resource_type schema","text":"","depth":2,"section_tag":"resource-types"},"schema.resource_type.check_every":{"location":"resource-types.html#schema.resource_type.check_every","title":"resource_typecheck_every","text":"Default 1m. The interval on which to check for new versions of the resource type. Acceptable interval options are defined by the time.ParseDuration function.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.name":{"location":"resource-types.html#schema.resource_type.name","title":"resource_typename","text":"The name of the resource type. This should be short and simple. This name will be referenced by pipelineresources defined within the same pipeline, and taskimage_resources used by tasks running in the pipeline.\n\nPipeline-provided resource types can override the core resource types by specifying the same name.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.params":{"location":"resource-types.html#schema.resource_type.params","title":"resource_typeparams","text":"Arbitrary config to pass when running the get to fetch the resource type's image.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.privileged":{"location":"resource-types.html#schema.resource_type.privileged","title":"resource_typeprivileged","text":"Default false. If set to true, the resource's containers will be run with full capabilities, as determined by the worker backend the task runs on.\n\nFor Linux-based backends it typically determines whether or not the container will run in a separate user namespace, and whether the root user is \"actual\" root (if set to true) or a user namespaced root (if set to false, the default).\n\nThis is a gaping security hole; only configure it if the resource type needs it (which should be called out in its documentation). This is not up to the resource type to decide dynamically, so as to prevent privilege escalation via third-party resource type exploits.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.source":{"location":"resource-types.html#schema.resource_type.source","title":"resource_typesource","text":"The location of the resource type's resource. This varies by resource type, and is a black box to Concourse; it is blindly passed to the resource at runtime.\n\nTo use docker-image as an example, the source would contain something like repository: username/reponame. See the Docker Image resource (or whatever resource type your resource type uses) for more information.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.tags":{"location":"resource-types.html#schema.resource_type.tags","title":"resource_typetags","text":"Default []. A list of tags to determine which workers the checks will be performed on. You'll want to specify this if the source is internal to a worker's network, for example. See also steptags.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.type":{"location":"resource-types.html#schema.resource_type.type","title":"resource_typetype","text":"The type of the resource used to provide the resource type's container image.\n\nThis is a bit meta. Usually this value will be docker-image or registry-image, as the resource type must result in a container image.\n\nA resource type's type can refer to other resource types, and can also use the core type that it's overriding. This is useful for bringing in a newer or forked registry-image resource.\n\n","depth":2,"section_tag":"resource-types"},"schema.resource_type.unique_version_history":{"location":"resource-types.html#schema.resource_type.unique_version_history","title":"resource_typeunique_version_history","text":"Default false. Only relevant when Global Resources (experimental) is enabled. When set to true, resources using this resource type will have a version history that is unique to the resource, rather than sharing a global version history.\n\n","depth":2,"section_tag":"resource-types"},"schema.step":{"location":"jobs.html#schema.step","title":"step schema","text":"","depth":3,"section_tag":"steps"},"schema.step.aggregate-step.aggregate":{"location":"jobs.html#schema.step.aggregate-step.aggregate","title":"aggregate step aggregate","text":"Deprecated. Use in_parallel step instead.\n\n","depth":3,"section_tag":"steps"},"schema.step.attempts":{"location":"jobs.html#schema.step.attempts","title":"stepattempts","text":"The total number of times a step should be tried before it should fail, e.g. 5 will run the step up to 5 times before giving up.\n\nAttempts will retry on a Concourse error as well as build failure. When the number of attempts is reached and the step has still not succeeded then the step will fail.\n\nThe following will run the task and retry it up to 9 times (for a total of 10 attempts) if it fails:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  attempts: 10\nWhen used in combination with timeout, the timeout applies to each step.\n\nThis semi-arbitary decision was made because often things either succeed in a reasonable amount of time or fail due to hanging/flakiness. In this case it seems more useful to allow each attempt the allotted timeout rather than have one very long attempt prevent more attempts.\n\nplan:\n- get: flake\n- task: flaky-tests\n  file: flake/integration.yml\n  timeout: 10m\n  attempts: 3\n","depth":3,"section_tag":"steps"},"schema.step.do-step.do":{"location":"jobs.html#schema.step.do-step.do","title":"do step do","text":"Simply performs the given steps serially, with the same semantics as if they were at the top level step listing.\n\nThis can be used to perform multiple steps serially in an stepon_failure:\n\nplan:\n- get: my-repo\n- task: unit\n  file: my-repo/ci/unit.yml\n  on_failure:\n    do:\n    - put: alert\n    - put: email\n","depth":3,"section_tag":"steps"},"schema.step.ensure":{"location":"jobs.html#schema.step.ensure","title":"stepensure","text":"A hook step to execute after the parent step regardless of whether the parent step succeeds, fails, or errors. The step will also be executed if the build was aborted and its parent step was interrupted.\n\nIf the parent step succeeds and the ensured step fails, the overall step fails.\n\nThe following build plan acquires a lock and then ensures that the lock is released.\n\nplan:\n- put: some-lock\n  params: {acquire: true}\n- task: integration\n  file: foo/integration.yml\n  ensure:\n    put: some-lock\n    params: {release: some-lock}\n","depth":3,"section_tag":"steps"},"schema.step.get-step.get":{"location":"jobs.html#schema.step.get-step.get","title":"get step get","text":"Fetches a version of a resource.\n\nThe fetched bits will be registered in the build's artifact namespace under the given identifier. Subsequent task step and put step which list the identifier as an input will have a copy of the bits in their working directory.\n\nAlmost every simple unit test job will look something like this: fetch my code with a get step and run its tests with a task step.\n\nplan:\n- get: my-repo\n  trigger: true\n- task: unit\n  file: my-repo/ci/unit.yml\n","depth":3,"section_tag":"steps"},"schema.step.get-step.params":{"location":"jobs.html#schema.step.get-step.params","title":"get step params","text":"Arbitrary configuration to pass to the resource. Refer to the resource type's documentation to see what it supports.\n\nThe following plan fetches a version number via the semver resource, bumps it to the next release candidate, and puts it back.\n\nplan:\n- get: version\n  params:\n    bump: minor\n    rc: true\n- put: version\n  params: {version: version/number}\n","depth":3,"section_tag":"steps"},"schema.step.get-step.passed":{"location":"jobs.html#schema.step.get-step.passed","title":"get step passed","text":"When specified, only the versions of the resource that made it through the given list of jobs (AND-ed together) will be considered when triggering and fetching.\n\nIf multiple gets are configured with passed constraints, all of the mentioned jobs are correlated. That is, with the following set of inputs:\n\nplan:\n- get: a\n  passed: [a-unit, integration]\n- get: b\n  passed: [b-unit, integration]\n- get: x\n  passed: [integration]\nThis means \"give me the versions of a, b, and x that have passed the same build of integration, with the same version of a passing a-unit and the same version of b passing b-unit.\"\n\nThis is crucial to being able to implement safe \"fan-in\" semantics as things progress through a pipeline.\n\n","depth":3,"section_tag":"steps"},"schema.step.get-step.resource":{"location":"jobs.html#schema.step.get-step.resource","title":"get step resource","text":"Defaults to the value of get. The resource to fetch, as configured in pipelineresources.\n\nUse this attribute to rename a resource from the overall pipeline context into the job-specific context.\n\n","depth":3,"section_tag":"steps"},"schema.step.get-step.trigger":{"location":"jobs.html#schema.step.get-step.trigger","title":"get step trigger","text":"Default false. If set to true, new builds of the job will be automatically created when a new version for this input becomes available.\n\nNote: if none of a job's get steps are set to true, the job can only be manually triggered.\n\n","depth":3,"section_tag":"steps"},"schema.step.get-step.version":{"location":"jobs.html#schema.step.get-step.version","title":"get step version","text":"Default latest. The version of the resource to fetch.\n\nIf set to latest, scheduling will just find the latest available version of a resource and use it, allowing versions to be skipped.  This is usually what you want, e.g. if someone pushes 100 git commits.\n\nIf set to every, builds will walk through all available versions of the resource. Note that if passed is also configured, it will only step through the versions satisfying the constraints.\n\nIf set to a specific version (e.g. {ref: abcdef123}), only that version will be used. Note that the version must be available and detected by the resource, otherwise the input will never be satisfied. You may want to use fly check-resource to force detection of resource versions, if you need to use an older one that was never detected (as all newly configured resources start from the latest version).\n\n","depth":3,"section_tag":"steps"},"schema.step.in-parallel-step.in_parallel":{"location":"jobs.html#schema.step.in-parallel-step.in_parallel","title":"in_parallel step in_parallel","text":"Performs the given steps in parallel.\n\nIf any sub-steps (or task) in a parallel result in a failure or error, the parallel step as a whole is considered to have failed or errored.\n\nSteps are either configured as a array or within a in_parallel_config schema.\n\nUsing the in_parallel step where possible is the easiest way to speeding up a builds.\n\nIt is often used to fetch all dependent resources together at the start of a build plan:\n\nplan:\n- in_parallel:\n  - get: component-a\n  - get: component-b\n  - get: integration-suite\n- task: integration\n  file: integration-suite/task.yml\nIf any step in the in_parallel fails, the build will fail, making it useful for build matrices:\n\nplan:\n- get: some-repo\n- in_parallel:\n  - task: unit-windows\n    file: some-repo/ci/windows.yml\n  - task: unit-linux\n    file: some-repo/ci/linux.yml\n  - task: unit-darwin\n    file: some-repo/ci/darwin.yml\nUsing limit is useful for performing parallel execution of a growing number of tasks without overloading your workers. In the example below, two tasks will be run in parallel and in order until all steps have been executed:\n\nplan:\n- get: some-repo\n- in_parallel:\n    limit: 2\n    fail_fast: false\n    steps:\n      - task: unit-windows\n        file: some-repo/ci/windows.yml\n      - task: unit-linux\n        file: some-repo/ci/linux.yml\n      - task: unit-darwin\n        file: some-repo/ci/darwin.yml\n","depth":3,"section_tag":"steps"},"schema.step.load-var-step.file":{"location":"jobs.html#schema.step.load-var-step.file","title":"load_var step file","text":"The path to a file whose content shall be read and used as the var's value.\n\n","depth":3,"section_tag":"steps"},"schema.step.load-var-step.format":{"location":"jobs.html#schema.step.load-var-step.format","title":"load_var step format","text":"Default detect. The format of the file's content.\n\nIf set to detect, the format will be inferred from the file extension.\n\nIf set to json, yaml, or yml, the file content will be parsed accordingly and the resulting structure will be the value of the var.\n\nIf set to raw, the file content will be the value of the var as a single string value.\n\nLet's say we have a task, generate-creds, which produces a generated-user output containing a user.json file like so:\n\n{\n  \"username\": \"some-user\",\n  \"password\": \"some-password\"\n}\nWe could pass these credentials to subsequent steps by loading it into a var with load_var, which will detect that it is in JSON format based on the file extension:\n\nplan:\n- task: generate-creds\n- load_var: user\n  file: generated-user/user.json\n- task: use-creds\n  params:\n    USERNAME: ((.:user.username))\n    PASSWORD: ((.:user.username))\nIf the use-creds task were to print these values, they would be automatically redacted unless reveal: true is set.\n\n","depth":3,"section_tag":"steps"},"schema.step.load-var-step.load_var":{"location":"jobs.html#schema.step.load-var-step.load_var","title":"load_var step load_var","text":"The load_var step was introduced in Concourse v6.0.0. It is considered an experimental feature until its associated RFC is resolved.\n\nLoad the value for a var at runtime, making it available to subsequent steps as a build-local var named after the given identifier.\n\nThe following build plan uses a version produced by the semver resource as a tag:\n\nplan:\n- get: version\n- load_var: version-tag\n  file: version/version\n- put: image\n  params: {tag: ((.:version-tag))}\n","depth":3,"section_tag":"steps"},"schema.step.load-var-step.reveal":{"location":"jobs.html#schema.step.load-var-step.reveal","title":"load_var step reveal","text":"Default false. If set to true, allow the var's content to be printed in the build output even with secret redaction enabled.\n\n","depth":3,"section_tag":"steps"},"schema.step.on_abort":{"location":"jobs.html#schema.step.on_abort","title":"stepon_abort","text":"A hook step to execute if the build is aborted and the parent step is terminated.\n\nThe following will perform the cleanup task only if the build is aborted while the unit task was running:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  on_abort:\n    task: cleanup\n    file: foo/cleanup.yml\n","depth":3,"section_tag":"steps"},"schema.step.on_error":{"location":"jobs.html#schema.step.on_error","title":"stepon_error","text":"A hook step to execute after the parent step if the parent step terminates abnormally in any way other than those handled by the stepon_abort or stepon_failure. This covers scenarios as broad as configuration mistakes, temporary network issues with the workers, or running longer than a steptimeout.\n\nUntil notifications become first-class (RFC #28, this step can be used to notify folks if their builds errored out:\n\nplan:\n- do:\n  - get: ci\n  - task: unit\n    file: ci/unit.yml\n  on_error:\n    put: slack\n","depth":3,"section_tag":"steps"},"schema.step.on_failure":{"location":"jobs.html#schema.step.on_failure","title":"stepon_failure","text":"A hook step to execute if the parent step fails.\n\nThis does not \"recover\" the failure - it will still fail even if the hook step succeeds.\n\nThe following will perform the alert task only if the unit task fails:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  on_failure:\n    task: alert\n    file: foo/alert.yml\n","depth":3,"section_tag":"steps"},"schema.step.on_success":{"location":"jobs.html#schema.step.on_success","title":"stepon_success","text":"A hook step to execute if the parent step succeeds.\n\nThe following will perform the second task only if the first one succeeds:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  on_success:\n    task: alert\n    file: foo/alert.yml\nNote that this is semantically equivalent to the following:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n- task: alert\n  file: foo/alert.yml\nThe on_success hook is provided mainly for cases where there is an equivalent stepon_failure, and having them next to each other is more clear.\n\n","depth":3,"section_tag":"steps"},"schema.step.put-step.get_params":{"location":"jobs.html#schema.step.put-step.get_params","title":"put step get_params","text":"Arbitrary configuration to get to the resource during the implicit get step. Refer to the resource type's documentation to see what it supports.\n\n","depth":3,"section_tag":"steps"},"schema.step.put-step.inputs":{"location":"jobs.html#schema.step.put-step.inputs","title":"put step inputs","text":"Default all. When not set, or set to all, all artifacts will be provided. This can result in slow performance if the prior steps in the build plan register a bunch of large artifacts before this step, so you may want to consider being explicit.\n\nIf configured as a list of identifiers, only the listed artifacts will be provided to the container.\n\nIf set to detect, the artifacts are detected based on the configured put step params by looking for all string values and using the first path segment as an identifier. (This may become the default in the future.)\n\n","depth":3,"section_tag":"steps"},"schema.step.put-step.params":{"location":"jobs.html#schema.step.put-step.params","title":"put step params","text":"Arbitrary configuration to pass to the resource. Refer to the resource type's documentation to see what it supports.\n\n","depth":3,"section_tag":"steps"},"schema.step.put-step.put":{"location":"jobs.html#schema.step.put-step.put","title":"put step put","text":"Pushes to the given resource.\n\nWhen the step succeeds, the version by the step will be immediately fetched via an additional implicit get step. This is so that later steps in your plan can use the artifact that was produced. The artifact will be available under the identifier put specifies.\n\nThe following plan fetches a repo using get and pushes it to another repo (assuming repo-develop and repo-master are defined as git resources):\n\nplan:\n- get: repo-develop\n- put: repo-master\n  params:\n    repository: repo-develop\nIf the logical name (whatever put specifies) differs from the concrete resource, you would specify resource as well, like so:\n\nplan:\n- put: resource-image\n  resource: docker-image-resource\nAdditionally, you can control the settings of the implicit get step by setting get_params. For example, if you did not want a put step utilizing the  docker-image resource type to download the image, you would implement your put step as such:\n\nplan:\n- put: docker-build\n  params: {build: git-resource}\n  get_params: {skip_download: true}\n","depth":3,"section_tag":"steps"},"schema.step.put-step.resource":{"location":"jobs.html#schema.step.put-step.resource","title":"put step resource","text":"Defaults to the value of put. The resource to update, as configured in pipelineresources.\n\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.file":{"location":"jobs.html#schema.step.set-pipeline-step.file","title":"set_pipeline step file","text":"The path to the pipeline's configuration file.\n\nfile points at a .yml file containing the pipeline configuration, which allows this to be tracked with your resources or generated by a task step.\n\nThe first segment in the path should refer to another artifact from the plan, and the rest of the path is relative to that artifact.\n\nThe get step can be used to fetch your configuration from a git repo and auto-configure it using a set_pipeline step:\n\n- get: ci\n- set_pipeline: my-pipeline\n  file: ci/pipelines/my-pipeline.yml\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.set_pipeline":{"location":"jobs.html#schema.step.set-pipeline-step.set_pipeline","title":"set_pipeline step set_pipeline","text":"The set_pipeline step was introduced in Concourse v5.8.0. It is considered an experimental feature until its associated RFC is resolved.\n\nConfigures a pipeline.\n\nThe identifier specifies the name of the pipeline to configure. It will be configured within the current team and be created unpaused.\n\nThis is a way to ensure a pipeline stays up to date with its definition in a source code repository, eliminating the need to manually run fly set-pipeline.\n\nresources:\n- name: booklit\n  type: git\n  source: {uri: https://github.com/vito/booklit}\njobs:\n- name: reconfigure\n  plan:\n  - get: booklit\n    trigger: true\n  - set_pipeline: booklit\n    file: booklit/ci/pipeline.yml\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.var_files":{"location":"jobs.html#schema.step.set-pipeline-step.var_files","title":"set_pipeline step var_files","text":"A list of paths to .yml files that will be passed to the pipeline config in the same manner as the --load-vars-from flag to fly set-pipeline. This means that if a variable appears in multiple files, the value from a file that is passed later in the list will override the values from files earlier in the list.\n\n","depth":3,"section_tag":"steps"},"schema.step.set-pipeline-step.vars":{"location":"jobs.html#schema.step.set-pipeline-step.vars","title":"set_pipeline step vars","text":"A map of template variables to pass to the pipeline config.\n\nNote that variables set with this field will not propagate to tasks configured via task step file. If you want those variables to be determined at the time the pipeline is set, use task step vars as well.\n\nA var may be statically passed like so:\n\nplan:\n- get: my-repo\n- set_pipeline: configure-the-pipeline\n  file: my-repo/ci/pipeline.yml\n  vars:\n    text: \"Hello World!\"\nAny Vars in the pipeline config will be filled in statically using this field.\n\nFor example, if my-repo/ci/pipeline.yml looks like...:\n\nresources:\n- name: task-image\n    type: docker-image\n    source:\n      repository: my.local.registry:8080/my/image\n      username: ((myuser))\n      password: ((mypass))\njobs:\n- name: job\n  plan:\n  - get: task-image\n  - task: do-stuff\n    image: task-image\n    config:\n      platform: linux\n      run:\n        path: echo\n        args: [\"((text))\"]\n...this will resolve \"((text))\" to \"Hello World!\", while ((myuser)) and ((mypass)) will be left in the pipeline to be fetched at runtime.\n\n","depth":3,"section_tag":"steps"},"schema.step.tags":{"location":"jobs.html#schema.step.tags","title":"steptags","text":"Default []. The tags by which to match workers.\n\nThe step will be placed within the a pool of workers that match all of the given set of tags.\n\nFor example, if [a, b] is specified, only workers advertising the a and b tags (in addition to any others) will be used for running the step.\n\nYou may have a private cluster only reachable by special workers running on-premises. To run steps against those workers, just provide a matching tag:\n\nplan:\n- get: my-repo\n- put: my-site\n  tags: [private]\n  params: {path: my-repo}\n- task: acceptance-tests\n  tags: [private]\n  file: my-repo/ci/atteptance.yml\n","depth":3,"section_tag":"steps"},"schema.step.task-step.config":{"location":"jobs.html#schema.step.task-step.config","title":"task step config","text":"The task config to execute.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.file":{"location":"jobs.html#schema.step.task-step.file","title":"task step file","text":"A dynamic alternative to task step config.\n\nfile points at a .yml file containing the task config, which allows this to be tracked with your resources.\n\nThe first segment in the path should refer to another source from the plan, and the rest of the path is relative to that source.\n\nThe content of the config file may contain template ((vars)), which will be filled in using task step vars or a configured credential manager.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.image":{"location":"jobs.html#schema.step.task-step.image","title":"task step image","text":"Specifies an artifact source containing an image to use for the task. This overrides any taskimage_resource configuration present in the task configuration.\n\nThis is very useful when part of your pipeline involves building an image, possibly with dependencies pre-baked. You can then propagate that image through the rest of your pipeline, guaranteeing that the correct version (and thus a consistent set of dependencies) is used throughout your pipeline.\n\nThis can be used in to explicitly keep track of dependent images:\n\nresources:\n- name: my-image\n  type: registry-image\n  source: {repository: golang, tag: \"1.13\"}\n\n- name: my-repo\n  type: git\n  source: # ...\n\njobs:\n- name: use-image\n  plan:\n  - get: my-image\n  - get: my-repo\n  - task: unit\n    file: my-repo/ci/unit.yml\n    image: my-image\nHere's a pipeline which builds an image in one job and then propagates it to the next:\n\nresources:\n- name: my-project\n  type: git\n  source: {uri: https://github.com/my-user/my-project}\n\n- name: my-task-image\n  type: docker-image\n  source: {repository: my-user/my-repo}\n\njobs:\n- name: build-task-image\n  plan:\n  - get: my-project\n  - put: my-task-image\n    params: {build: my-project/ci/images/my-task}\n\n- name: use-task-image\n  plan:\n  - get: my-task-image\n    passed: [build-task-image]\n  - get: my-project\n    passed: [build-task-image]\n  - task: use-task-image\n    image: my-task-image\n    file: my-project/ci/tasks/my-task.yml\n","depth":3,"section_tag":"steps"},"schema.step.task-step.input_mapping":{"location":"jobs.html#schema.step.task-step.input_mapping","title":"task step input_mapping","text":"A map from task input names to concrete names in the build plan. This allows a task with generic input names to be used multiple times in the same plan, mapping its inputs to specific resources within the plan.\n\nThe following example demonstrates a task with a generic release-repo input being mapped to more specific artifact names:\n\nplan:\n- get: diego-release\n- get: cf-release\n- get: ci-scripts\n- task: audit-diego-release\n  file: ci-scripts/audit-release.yml\n  input_mapping: {release-repo: diego-release}\n- task: audit-cf-release\n  file: ci-scripts/audit-release.yml\n  input_mapping: {release-repo: cf-release}\n","depth":3,"section_tag":"steps"},"schema.step.task-step.output_mapping":{"location":"jobs.html#schema.step.task-step.output_mapping","title":"task step output_mapping","text":"A map from task output names to concrete names to register in the build plan. This allows a task with generic output names to be used multiple times in the same plan.\n\nThis is often used together with task step input_mapping:\n\nplan:\n- get: diego-release\n- get: cf-release\n- get: ci-scripts\n- task: create-diego-release\n  file: ci-scripts/create-release.yml\n  input_mapping: {release-repo: diego-release}\n  output_mapping: {release-tarball: diego-release-tarball}\n- task: create-cf-release\n  file: ci-scripts/create-release.yml\n  input_mapping: {release-repo: cf-release}\n  output_mapping: {release-tarball: cf-release-tarball}\n","depth":3,"section_tag":"steps"},"schema.step.task-step.params":{"location":"jobs.html#schema.step.task-step.params","title":"task step params","text":"A map of task environment variable parameters to set, overriding those configured in the task's config or file.\n\nThe difference between params and vars is that vars allows you to interpolate any template variable in an external task, while params can be used to overwrite task parameters (i.e. env variables) specifically. Also, params can have default values declared in the task.\n\nLet's say we have a task config in intgration.yml like so:\n\nplatform: linux\nimage_resource: # ...\nparams:\n  REMOTE_SERVER: https://example.com\n  USERNAME:\n  PASSWORD:\nThis indicates that there are three params which can be set: REMOTE_SERVER, which has a default, and USERNAME and PASSWORD.\n\nA pipeline could run the task with credentials passed in like so:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/integration.yml\n  params:\n    USERNAME: my-user\n    PASSWORD: my-pass\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/integration.yml\n  params:\n    REMOTE_SERVER: 10.20.30.40:8080\n    USERNAME: ((integration-username))\n    PASSWORD: ((integration-password))\n","depth":3,"section_tag":"steps"},"schema.step.task-step.privileged":{"location":"jobs.html#schema.step.task-step.privileged","title":"task step privileged","text":"Default false. If set to true, the task will run with escalated capabilities available on the task's platform.\n\nSetting privileged: true is a gaping security hole; use wisely and only if necessary. This is not part of the task configuration in order to prevent privilege escalation via pull requests changing the task file.\n\nFor the linux platform, this determines whether or not the container will run in a separate user namespace. When set to true, the container's root user is actual root, i.e. not in a user namespace. This is not recommended, and should never be used with code you do not trust - e.g. pull requests.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.task":{"location":"jobs.html#schema.step.task-step.task","title":"task step task","text":"Executes a task.\n\nWhen a task completes, the artifacts specified by taskoutputs will be registered in the build's artifact namespace. This allows subsequent task steps and put steps to access the result of a task.\n\nThe identifier value is just a name - short and sweet. The value is shown in the web UI but otherwise has no affect on anything. This may change in the future; RFC #32 proposes that the name be used to reference a file within the project.\n\nThe following plan pulls down a repo, makes a commit to it, and pushes the commit to another repo (the task must have an output called repo-with-commit):\n\nplan:\n- get: my-repo\n- task: commit\n  file: my-repo/commit.yml\n- put: other-repo\n  params:\n    repository: repo-with-commit\nThe following plan fetches a single repository and executes multiple tasks, using the in_parallel step, in a build matrix style configuration:\n\nplan:\n- get: my-repo\n- in_parallel:\n  - task: go-1.3\n    file: my-repo/go-1.3.yml\n  - task: go-1.4\n    file: my-repo/ci/go-1.4.yml\nOnly if both tasks succeed will the overall step succeed. See also in_parallel step.\n\n","depth":3,"section_tag":"steps"},"schema.step.task-step.vars":{"location":"jobs.html#schema.step.task-step.vars","title":"task step vars","text":"A map of template variables to pass to an external task. Not to be confused with taskparams, which provides environment variables to the task.\n\nThis is to be used with external tasks defined in task step file.\n\nA var may be statically passed like so:\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/task.yml\n  vars:\n    text: \"Hello World!\"\nThis is often used in combination with Vars in the pipeline (note the replacement of the string literal with the ((text)) pipeline var):\n\nplan:\n- get: my-repo\n- task: integration\n  file: my-repo/ci/task.yml\n  vars:\n    text: ((text))\nWhen run with the following task.yml:\n\n---\nplatform: linux\n\nimage_resource:\n  type: docker-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\n\nrun:\n  path: echo\n  args: [\"((text))\"]\n...this will resolve \"((text))\" to \"Hello World!\", while ((myuser)) and ((mypass))  will be resolved in runtime via a credential manager, if it has been configured.\n\n","depth":3,"section_tag":"steps"},"schema.step.timeout":{"location":"jobs.html#schema.step.timeout","title":"steptimeout","text":"The amount of time to limit the step's execution to, e.g. 30m for 30 minutes.\n\nWhen exceeded, the step will be interrupted, with the same semantics as aborting the build (except the build will be failed, not aborted, to distinguish between human intervention and timeouts being inforced).\n\nThe following will run the unit task and cancel it if it takes longer than 1 hour and 30 minutes:\n\nplan:\n- get: foo\n- task: unit\n  file: foo/unit.yml\n  timeout: 1h30m\n","depth":3,"section_tag":"steps"},"schema.step.try-step.try":{"location":"jobs.html#schema.step.try-step.try","title":"try step try","text":"Performs the given step, ignoring any failure and masking it with success.\n\nThis can be used when you want to perform some side-effect, but you don't really want the whole build to fail if it doesn't work.\n\nWhen emitting logs somewhere for analyzing later, if the destination flakes out it may not really be critical, so we may want to just swallow the error:\n\nplan:\n- task: run-tests\n  config: # ...\n  on_success:\n    try:\n      put: test-logs\n      params:\n        from: run-tests/*.log\n- task: do-something-else\n  config: # ...\n","depth":3,"section_tag":"steps"},"schema.string":{"location":"config-basics.html#schema.string","title":"string schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.task":{"location":"tasks.html#schema.task","title":"task schema","text":"","depth":2,"section_tag":"tasks"},"schema.task.caches":{"location":"tasks.html#schema.task.caches","title":"taskcaches","text":"The cached directories shared between task runs.\n\nOn the task's first run, all cache directories will be empty. It is the responsibility of the task to populate these directories with any artifacts to be cached. On subsequent runs, the cached directories will contain those artifacts.\n\nCaches are scoped to the worker the task is run on, so you will not get a cache hit when subsequent builds run on different workers. This also means that caching is not intended to share state between workers, and your task should be able to run whether or not the cache is warmed.\n\nCaches are also scoped to a particular task name inside of a pipeline's job. As a consequence, if the job name, step name or cache path are changed, the cache will not be used. This also means that caches do not exist for one-off builds.\n\n","depth":2,"section_tag":"tasks"},"schema.task.container_limits":{"location":"tasks.html#schema.task.container_limits","title":"taskcontainer_limits","text":"CPU and memory limits to enforce on the task container.\n\nNote that these values, when specified, will override any limits set by passing the --default-task-cpu-limit or --default-task-memory-limit flags to the concourse web command.\n\n","depth":2,"section_tag":"tasks"},"schema.task.image_resource":{"location":"tasks.html#schema.task.image_resource","title":"taskimage_resource","text":"The container image to run with, as provided by an anonymous resource definition.\n\nWhenever the task runs, the anonymous resource will be checked to discover the latest version available. The image will then be fetched onto the worker if necessary just prior to running the task.\n\nTo use an image provided by a previous step within your build plan, set task step image on the task step instead.\n\nThe following task config will use the golang Docker image to run go version:\n\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source: {repository: golang}\n\nrun:\n  path: go\n  args: [version]\n","depth":2,"section_tag":"tasks"},"schema.task.inputs":{"location":"tasks.html#schema.task.inputs","title":"taskinputs","text":"The set of artifacts used by task, determining which artifacts will be available in the current directory when the task runs.\n\nThese are satisfied by get steps or taskoutputs of a previous task. These can also be provided by -i with Running tasks with fly execute.\n\nIf any required inputs are missing at run-time, then the task will error immediately.\n\n","depth":2,"section_tag":"tasks"},"schema.task.outputs":{"location":"tasks.html#schema.task.outputs","title":"taskoutputs","text":"The artifacts produced by the task.\n\nEach output configures a directory to make available to later steps in the build plan. The directory will be automatically created before the task runs, and the task should place any artifacts it wants to export in the directory.\n\n","depth":2,"section_tag":"tasks"},"schema.task.params":{"location":"tasks.html#schema.task.params","title":"taskparams","text":"A key-value mapping of string keys and values that are exposed to the task via environment variables.\n\nPipelines can override these params by setting task step params on the task step. This is a common method of providing credentials to a task.\n\n","depth":2,"section_tag":"tasks"},"schema.task.platform":{"location":"tasks.html#schema.task.platform","title":"taskplatform","text":"The platform the task should run on. This determines the pool of workers that the task can run against.\n\nTechnically any string value is allowed so long as a worker advertises the same platform, but in practice only linux, darwin, and windows are in use.\n\n","depth":2,"section_tag":"tasks"},"schema.task.rootfs_uri":{"location":"tasks.html#schema.task.rootfs_uri","title":"taskrootfs_uri","text":"A string specifying the rootfs uri of the container, as interpreted by your worker's Garden backend.\n\ntaskimage_resource is the preferred way to specify base image. You should only use this if you have no other option and you really know what you're doing.\n\n","depth":2,"section_tag":"tasks"},"schema.task.run":{"location":"tasks.html#schema.task.run","title":"taskrun","text":"The command to execute in the container.\n\nNote that this is not provided as a script blob, but explicit path and args values; this allows fly to forward arguments to the script, and forces your config .yml to stay fairly small.\n\n","depth":2,"section_tag":"tasks"},"schema.var_source":{"location":"vars.html#schema.var_source","title":"var_source schema","text":"","depth":4,"section_tag":"var-sources"},"schema.var_source.dummy-var-source.config":{"location":"vars.html#schema.var_source.dummy-var-source.config","title":"dummy var source config","text":"","depth":4,"section_tag":"var-sources"},"schema.var_source.dummy-var-source.type":{"location":"vars.html#schema.var_source.dummy-var-source.type","title":"dummy var source type","text":"The dummy type supports configuring a static map of vars to values.\n\nThis is really only useful if you have no better alternative for credential management but still have sensitive values that you would like to redact them from build output.\n\n","depth":4,"section_tag":"var-sources"},"schema.var_source.name":{"location":"vars.html#schema.var_source.name","title":"var_sourcename","text":"The name of the ((var)) source. This should be short and simple. This name will be referenced ((var)) syntax throughout the config.\n\n","depth":4,"section_tag":"var-sources"},"schema.var_source.vault-var-source.config":{"location":"vars.html#schema.var_source.vault-var-source.config","title":"vault var source config","text":"Configuration for the Vault server has the following schema:\n\n","depth":4,"section_tag":"var-sources"},"schema.var_source.vault-var-source.type":{"location":"vars.html#schema.var_source.vault-var-source.type","title":"vault var source type","text":"The vault type supports configuring a Vault server as a ((var)) source.\n\n","depth":4,"section_tag":"var-sources"},"schema.vars":{"location":"config-basics.html#schema.vars","title":"vars schema","text":"","depth":3,"section_tag":"basic-schemas"},"schema.vault_config":{"location":"vars.html#schema.vault_config","title":"vault_config schema","text":"","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_backend":{"location":"vars.html#schema.vault_config.auth_backend","title":"vault var source auth_backend","text":"Authenticate using an auth backend, e.g. cert or approle.\n\nSee Using the approle auth backend or Using the cert auth backend for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_max_ttl":{"location":"vars.html#schema.vault_config.auth_max_ttl","title":"vault var source auth_max_ttl","text":"Maximum duration to elapse before forcing the client to log in again.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_params":{"location":"vars.html#schema.vault_config.auth_params","title":"vault var source auth_params","text":"A key-value map of parameters to pass during authentication.\n\nSee Using the approle auth backend for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_retry_initial":{"location":"vars.html#schema.vault_config.auth_retry_initial","title":"vault var source auth_retry_initial","text":"When retrying during authentication, start with this retry interval. The interval will increase exponentially until auth_retry_max is reached.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.auth_retry_max":{"location":"vars.html#schema.vault_config.auth_retry_max","title":"vault var source auth_retry_max","text":"When failing to authenticate, give up after this amount of time.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.ca_cert":{"location":"vars.html#schema.vault_config.ca_cert","title":"vault var source ca_cert","text":"The PEM encoded contents of a CA certificate to use when connecting to the API.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.client_cert":{"location":"vars.html#schema.vault_config.client_cert","title":"vault var source client_cert","text":"A PEM encoded client certificate, for use with TLS based auth.\n\nSee Using the cert auth backend for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.client_key":{"location":"vars.html#schema.vault_config.client_key","title":"vault var source client_key","text":"A PEM encoded client key, for use with TLS based auth.\n\nSee Using the cert auth backend for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.client_token":{"location":"vars.html#schema.vault_config.client_token","title":"vault var source client_token","text":"Authenticate via a periodic client token.\n\nSee Using a periodic token for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.insecure_skip_verify":{"location":"vars.html#schema.vault_config.insecure_skip_verify","title":"vault var source insecure_skip_verify","text":"Skip TLS validation. Not recommended. Don't do it. No really, don't.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.namespace":{"location":"vars.html#schema.vault_config.namespace","title":"vault var source namespace","text":"A Vault namespace to operate under.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.path_prefix":{"location":"vars.html#schema.vault_config.path_prefix","title":"vault var source path_prefix","text":"Default /concourse. A prefix under which to look for all credential values.\n\nSee Credential lookup rules for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.server_name":{"location":"vars.html#schema.vault_config.server_name","title":"vault var source server_name","text":"The expected name of the server when connecting through TLS.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.shared_path":{"location":"vars.html#schema.vault_config.shared_path","title":"vault var source shared_path","text":"An additional path under which credentials will be looked up.\n\nSee Configuring a shared path for more information.\n\n","depth":4,"section_tag":"var-sources"},"schema.vault_config.url":{"location":"vars.html#schema.vault_config.url","title":"vault var source url","text":"The URL of the Vault API.\n\n","depth":4,"section_tag":"var-sources"},"schema.version":{"location":"config-basics.html#schema.version","title":"version schema","text":"","depth":3,"section_tag":"basic-schemas"},"security-contact":{"location":"security-contact.html","title":"Security Contact","text":"To be notified of any security issues or vulnerabilities, join the Concourse Security mailing list. To report a security issue, send an email to concourseteam+security@gmail.com.\n\n","depth":2,"section_tag":"security-contact"},"serial-job-example":{"location":"serial-job-example.html","title":"Serial job example","text":"Setting the jobserial flag restricts a job to run one build at a time.\n\nBy default, jobs are run in parallel. For some use cases this might be ideal (ex. testing all incoming commits from a repository). For other use cases this might be less ideal (ex. deploying an application).\n\nYou can also set the jobmax_in_flight value to 1 to disable parallel job runs.\n\n","depth":2,"section_tag":"serial-job-example"},"set-pipeline-step":{"location":"jobs.html#set-pipeline-step","title":"set_pipeline step","text":"","depth":3,"section_tag":"steps"},"set-pipelines-example":{"location":"set-pipelines-example.html","title":"Set Pipelines Example","text":"You can set a static set of pipelines from another pipeline on the same team.\n\n","depth":2,"section_tag":"set-pipelines-example"},"setting-pipelines":{"location":"setting-pipelines.html","title":"Setting Pipelines","text":"Pipelines are configured entirely via the The fly CLI. There is no GUI.\n\n","depth":3,"section_tag":"setting-pipelines"},"setting-roles":{"location":"managing-teams.html#setting-roles","title":"Setting User Roles","text":"By default, authorization config passed to set-team configures the owner role.\n\nMore advanced roles configuration can be specified through the --config or -c flag.\n\nThe -c flag expects a .yml file with a single field, roles:, pointing to a list of role authorization configs.\n\nAll of the attributes in each config will vary by provider. Consult the appropriate section for your provider under Configuring Auth for specifics.\n\nFor example, the following config sets three roles with different auth config for each role's provider:\n\nroles:\n- name: owner\n  github:\n    users: [\"admin\"]\n- name: member\n  github:\n    teams: [\"org:team\"]\n- name: viewer\n  github:\n    orgs: [\"org\"]\n  local:\n    users: [\"visitor\"]\n","depth":5,"section_tag":"setting-roles"},"some-resources-should-opt-out":{"location":"global-resources.html#some-resources-should-opt-out","title":"Some resources should opt-out","text":"Sharing versions isn't always a good idea. For example, the time resource is often used to generate versions on an interval so that jobs can fire periodically. If version history were to be shared for all users with e.g. a 10 minute interval, that would lead to a thundering herd of builds storming your workers, leading to load spikes and a lot of unhappy builds.\n\nFor this reason, resource types can opt out of sharing version history for all resources that use them. This way all existing usage of the time resource don't have to change, and continue to have their own version history, unique to the pipeline resource.\n\nThe time resource opts out of this by configuring unique_version_history: true in its metadata.json - but this is something that only \"core\" resource types can do. We plan on supporting this as part of the Resources v2 RFC.\n\nUsers can also set this value themselves by configuring resource_typeunique_version_history on the resource type.\n\nAnother case where version history shouldn't be shared is when resources \"automagically\" learn their auth credentials using things like IAM roles. In these cases, the credentials aren't in the resourcesource. If version history were to be shared, anyone could configure the same source:, not specifying any credentials, and see the version history discovered by some other pipeline that ran its checks on workers that had access via IAM roles.\n\nFor this reason, any resource types that acquire credentials outside of source: should not share version history. Granted, the user won't be able to fetch these versions, but it's still an information leak.\n\nIAM roles are a bit of a thorn in our side when it comes to designing features like this. We're planning on introducing support for them in a way that doesn't have this problem in 3023.\n\n","depth":5,"section_tag":"some-resources-should-opt-out"},"static-vars":{"location":"vars.html#static-vars","title":"Static vars","text":"Var values may also be specified statically using the set_pipeline step and task step.\n\nWhen running the The fly CLI equivalent commands (fly set-pipeline and Running tasks with fly execute), var values may be provided using the following flags:\n\n* -v or --var NAME=VALUE sets the string VALUE as the value for the var NAME.\n\n* -y or --yaml-var NAME=VALUE parses VALUE as YAML and sets it as the value for the var NAME.\n\n* -l or --load-vars-from FILE loads FILE, a YAML document containing mapping var names to values, and sets them all.\n\nWhen used in combination with -l, the -y and -v flags take precedence. This way a vars file may be re-used, overriding individual values by hand.\n\nLet's say we have a task config like so:\n\nplatform: linux\n\nimage_resource:\n  type: registry-image\n  source:\n    repository: golang\n    tag: ((tag))\n\ninputs:\n- name: booklit\n\nrun:\n  path: booklit/ci/unit\nWe could use vars to run this task against different versions of Go:\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: true\n  - task: unit-1.13\n    file: booklit/ci/unit.yml\n    vars: {tag: 1.13}\n  - task: unit-1.8\n    file: booklit/ci/unit.yml\n    vars: {tag: 1.8}\nWith a pipeline template like so:\n\nresources:\n- name: booklit\n  type: booklit\n  source:\n    uri: https://github.com/vito/booklit\n    branch: ((branch))\n    private_key: ((private_key))\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: ((trigger))\n  - task: unit\n    file: booklit/ci/unit.yml\nLet's say we have a private key in a file called private_key.\n\nThe fly validate-pipeline command may be used to test how interpolation is applied, by passing the --output flag.\n\n$ fly validate-pipeline \\\n  -c pipeline.yml \\\n  -y trigger=true \\\n  -v private_key=\"$(cat private_key)\" \\\n  -v branch=master \\\n  --output\nThe above incantation should print the following:\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: true\n  - file: booklit/ci/unit.yml\n    task: unit\nresources:\n- name: booklit\n  type: booklit\n  source:\n    branch: master\n    private_key: |\n      -----BEGIN RSA PRIVATE KEY-----\n      # ... snipped ...\n      -----END RSA PRIVATE KEY-----\n    uri: https://github.com/vito/booklit\nNote that we had to use -y so that the trigger: true ends up with a boolean value instead of the string \"true\".\n\nWith a pipeline template like so:\n\nresources:\n- name: booklit\n  type: booklit\n  source:\n    uri: https://github.com/vito/booklit\n    branch: ((branch))\n    private_key: ((private_key))\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: ((trigger))\n  - task: unit\n    file: booklit/ci/unit.yml\nLet's say I've put the private_key var in a file called vars.yml, since it's quite large and hard to pass through flags:\n\nprivate_key: |\n  -----BEGIN RSA PRIVATE KEY-----\n  # ... snipped ...\n  -----END RSA PRIVATE KEY-----\nThe fly validate-pipeline command may be used to test how interpolation is applied, by passing the --output flag.\n\n$ fly validate-pipeline \\\n  -c pipeline.yml \\\n  -l vars.yml \\\n  -y trigger=true \\\n  -v branch=master \\\n  --output\nThe above incantation should print the following:\n\njobs:\n- name: unit\n  plan:\n  - get: booklit\n    trigger: true\n  - task: unit\n    file: booklit/ci/unit.yml\nresources:\n- name: booklit\n  type: booklit\n  source:\n    branch: master\n    private_key: |\n      -----BEGIN RSA PRIVATE KEY-----\n      # ... snipped ...\n      -----END RSA PRIVATE KEY-----\n    uri: https://github.com/vito/booklit\nNote that we had to use -y so that the trigger: true ends up with a boolean value instead of the string \"true\".\n\n","depth":3,"section_tag":"static-vars"},"steps":{"location":"jobs.html#steps","title":"Steps","text":"Each job has a single build plan configured as jobplan. A build plan is a recipe for what to run when a build of the job is created.\n\nA build plan is a sequence of steps:\n\n* the task step runs a task\n\n* the get step fetches a resource\n\n* the put step updates a resource\n\n* the set_pipeline step configures a pipeline\n\n* the load_var step loads a value into a local var\n\n* the in_parallel step runs steps in parallel\n\n* the do step runs steps in sequence\n\n* the try step attempts to run a step and succeeds even if the step fails\n\nWhen a new version is available for a get step with trigger: true configured, a new build of the job will be created from the build plan.\n\nWhen viewing the job in the pipeline, resources that are used as get steps appear as inputs, and resources that are used in put steps appear as outputs. Jobs are rendered downstream of any jobs they reference in passed constraints, connected by the resource.\n\nIf any step in the build plan fails, the build will fail and subsequent steps will not be executed. Additional steps may be configured to run after failure by configuring stepon_failure or stepensure (or the job equivalents, jobon_failure and jobensure).\n\n","depth":3,"section_tag":"steps"},"task-environment":{"location":"tasks.html#task-environment","title":"Task runtime environment","text":"A task runs in a new container every time, using the image provided by taskimage_resource as its base filesystem (i.e. /).\n\nThe command specified by taskrun will be executed in a working directory containing each of the taskinputs. If any inputs are missing the task will not run (and the container will not even be created).\n\nThe working directory will also contain empty directories for each of the taskoutputs. The task must place artifacts in the output directories for them to be exported. This meshes well with build tools with configurable destination paths.\n\nIf your build tools don't support output paths you can configure an input and output with the same path. The directory will be populated by the input, and any changes made to the directory will propagate downstream as an output.\n\nAny task step params configured will be set in the environment for the task's command, along with any environment variables provided by the task's image (i.e. ENV rules from your Dockerfile).\n\nThe user the command runs as is determined by the image. If you're using a Docker image, this will be the user set by a USER rule in your Dockerfile, or root if not specified.\n\nAnother relevant bit of configuration is task step privileged, which determines whether the user the task runs as will have full privileges (primarily when running as root). This is intentionally not configurable by the task itself, to prevent privilege escalation by way of pull requests to repositories containing task configs.\n\nPutting all this together, the following task config:\n\n---\nplatform: linux\n\nimage_resource:\n  type: docker-image\n  source:\n    repository: golang\n    tag: '1.6'\n\nparams:\n  SOME_PARAM: some-default-value\n\ninputs:\n- name: some-input\n- name: some-input-with-custom-path\n  path: some/custom/path\n\noutputs:\n- name: some-output\n\nrun:\n  path: sh\n  args:\n  - -exc\n  - |\n    whoami\n    env\n    go version\n    find .\n    touch some-output/my-built-artifact\n...will produce the following output:\n\n+ whoami\nroot\n+ env\nUSER=root\nHOME=/root\nGOLANG_DOWNLOAD_SHA256=5470eac05d273c74ff8bac7bef5bad0b5abbd1c4052efbdbc8db45332e836b0b\nPATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nGOPATH=/go\nPWD=/tmp/build/e55deab7\nGOLANG_DOWNLOAD_URL=https://golang.org/dl/go1.6.linux-amd64.tar.gz\nGOLANG_VERSION=1.6\nSOME_PARAM=some-default-value\n+ go version\ngo version go1.6 linux/amd64\n+ find .\n.\n./some-input\n./some-input/foo\n./some\n./some/custom\n./some/custom/path\n./some/custom/path/bar\n./some-output\n+ touch some-output/my-built-artifact\n...and propagate my-built-artifact to any later task steps or put steps that reference the some-output artifact, in the same way that this task had some-input as an input.\n\n","depth":3,"section_tag":"task-environment"},"task-inputs-outputs-example":{"location":"task-inputs-outputs-example.html","title":"Task inputs and outputs example","text":"A task can pass an artifacts to another task in the same job.\n\nTasks within a job have the ability to pass artifacts directly inbetween them to allow you to process artifacts in many ways.\n\nWhile you are free to create as many jobs as you'd like for your pipeline, you have to use resources to pass artifacts inbetween them.\n\nThese constructs give you the ability to design a pipeline that can process artifacts in many different ways via Tasks, and then store those processed artifacts externally via Resources.\n\n","depth":2,"section_tag":"task-inputs-outputs-example"},"task-step":{"location":"jobs.html#task-step","title":"task step","text":"","depth":3,"section_tag":"steps"},"tasks":{"location":"tasks.html","title":"Tasks","text":"The smallest configurable unit in a Concourse pipeline is a single task. A task can be thought of as a function from taskinputs to taskoutputs that can either succeed or fail.\n\nGoing a bit further, ideally tasks are pure functions: given the same set of inputs, it should either always succeed with the same outputs or always fail. This is entirely up to your script's level of discipline, however. Flaky tests or dependencies on the internet are the most common source of impurity.\n\nOnce you have a running Concourse deployment, you can start configuring your tasks and executing them interactively from your terminal with the Fly commandline tool.\n\nOnce you've figured out your tasks's configuration, you can reuse it for a Job in your Pipeline.\n\nConventionally a task's configuration is placed in the same repository as the code it's testing, possibly under some ci directory.\n\nA task's configuration specifies the following:\n\nThis configuration specifies that the task must run with the ruby:2.1 Docker image with a my-app input, and when the task is executed it will run the scripts/test script in the same repo.\n\n---\nplatform: linux\n\nimage_resource:\n  type: docker-image\n  source:\n    repository: ruby\n    tag: '2.1'\n\ninputs:\n- name: my-app\n\nrun:\n  path: my-app/scripts/test\nA task can configure taskoutputs to produce artifacts that can then be propagated to a put step or another task step in the same plan. They can also be downloaded with Running tasks with fly execute by passing -o.\n\n---\nplatform: linux\n\nimage_resource: # ...\n\ninputs:\n- name: project-src\n\noutputs:\n- name: built-project\n\nrun:\n  path: project-src/ci/build\n...assuming project-src/ci/build looks something like:\n\n#!/bin/bash\n\nset -e -u -x\n\nexport GOPATH=$PWD/project-src\n\ngo build -o built-project/my-project \\\n  github.com/concourse/my-project\n...this task could then be used in a build plan like so:\n\nplan:\n- get: project-src\n- task: build-bin\n  file: project-src/ci/build.yml\n- put: project-bin\n  params: file: built-project/my-project\nThe following task and script could be used by a Node project to cache the node_modules directory:\n\n---\nplatform: linux\n\nimage_resource: # ...\n\ninputs:\n- name: project-src\n\ncaches:\n- path: project-src/node_modules\n\nrun:\n  path: project-src/ci/build\n...assuming project-src/ci/build looks something like:\n\n#!/bin/bash\n\nset -e -u -x\n\ncd project-src\nnpm install\n\n# ...\n...this task would cache the contents of project-src/node_modules between runs of this task on the same worker.\n\nThe following external task uses an image from a private registry. Assuming the CA is configured properly on the workers, SSL should Just Work™.\n\nExternal tasks are now fully interpolated using credential manager variables and task step vars, so you can use template variables in an external task:\n\n---\nplatform: linux\n\nimage_resource:\n  type: docker-image\n  source:\n    repository: my.local.registry:8080/my/image\n    username: ((myuser))\n    password: ((mypass))\n\ninputs:\n- name: my-app\n\nrun:\n  path: my-app/scripts/test\n  args: [\"Hello, world!\", \"((myparam))\"]\n","depth":2,"section_tag":"tasks"},"team-member-role":{"location":"user-roles.html#team-member-role","title":"member role","text":"Team members can operate within their team in a read \u0026 write fashion, but they can not change the configuration of their team.\n\nActions assigned to the member role by default:\n\nmember:\n- SaveConfig\n- CreateBuild\n- DeletePipeline\n- OrderPipelines\n- ExposePipeline\n- HidePipeline\n- RenamePipeline\n- CreatePipelineBuild\n- RegisterWorker\n- LandWorker\n- RetireWorker\n- PruneWorker\n- HeartbeatWorker\n- DeleteWorker\n- SetLogLevel\n- HijackContainer\n- ReportWorkerContainers\n- ReportWorkerVolumes\n- CreateArtifact\n- GetArtifact\n","depth":4,"section_tag":"team-member-role"},"team-owner-role":{"location":"user-roles.html#team-owner-role","title":"owner role","text":"Team owners have read, write and auth management capabilities within the scope of their team, but they cannot rename or destroy the team.\n\nActions assigned to the owner role by default:\n\nowner:\n- SetTeam\n- RenameTeam\n- DestroyTeam\n","depth":4,"section_tag":"team-owner-role"},"team-pipeline-operator-role":{"location":"user-roles.html#team-pipeline-operator-role","title":"pipeline-operator role","text":"Team pipeline operators can perform pipeline operations such as triggering builds and pinning resources, however they cannot update pipeline configurations.\n\nActions assigned to the pipeline-operator role by default:\n\npipeline-operator:\n- AbortBuild\n- RerunJobBuild\n- CreateJobBuild\n- PauseJob\n- UnpauseJob\n- ClearTaskCache\n- UnpinResource\n- SetPinCommentOnResource\n- CheckResource\n- CheckResourceWebHook\n- CheckResourceType\n- EnableResourceVersion\n- DisableResourceVersion\n- PinResourceVersion\n- PausePipeline\n- UnpausePipeline\n","depth":4,"section_tag":"team-pipeline-operator-role"},"team-viewer-role":{"location":"user-roles.html#team-viewer-role","title":"viewer role","text":"Team viewers have \"read-only\" access to a team and its pipelines. This locks everything down, preventing users from doing a fly set-pipeline or fly intercept.\n\nActions assigned to the viewer role by default:\n\nviewer:\n- GetConfig\n- GetCC\n- GetBuild\n- GetCheck\n- GetBuildPlan\n- ListBuilds\n- BuildEvents\n- BuildResources\n- GetBuildPreparation\n- GetJob\n- ListAllJobs\n- ListJobs\n- ListJobBuilds\n- ListJobInputs\n- GetJobBuild\n- GetVersionsDB\n- JobBadge\n- MainJobBadge\n- ListAllResources\n- ListResources\n- ListResourceTypes\n- GetResource\n- ListResourceVersions\n- GetResourceVersion\n- ListBuildsWithVersionAsInput\n- ListBuildsWithVersionAsOutput\n- GetResourceCausality\n- ListAllPipelines\n- ListPipelines\n- GetPipeline\n- ListPipelineBuilds\n- PipelineBadge\n- ListWorkers\n- GetLogLevel\n- DownloadCLI\n- GetInfo\n- GetInfoCreds\n- ListContainers\n- GetContainer\n- ListDestroyingContainers\n- ListVolumes\n- ListDestroyingVolumes\n- ListTeams\n- GetTeam\n- ListTeamBuilds\n- ListBuildArtifacts\n","depth":4,"section_tag":"team-viewer-role"},"teams":{"location":"auth.html","title":"Auth \u0026 Teams","text":"A single Concourse installation can accomodate many projects and users.\n\nPipelines, builds, and all other user data are owned by teams. A team is just a conceptual owner and a separate namespace, tied to an authorization config. For example, a team may authorize all members of the concourse GitHub organization to be a member.\n\nWhen a user authenticates, each team's authorization config is checked against the user to determine which role, if any, to grant for the team. This information is then stored in the user's token to determine access control for future requests.\n\n","depth":2,"section_tag":"auth"},"teams-caveats":{"location":"teams-caveats.html","title":"Security Caveats","text":"At present, teams only provide trusted multi-tenancy. This means it should be used for cases where you know and trust who you're allowing access into your Concourse cluster.\n\nThere are a few reasons it'd be a bad idea to do otherwise:\n\n* Any team can run builds with task step privileged tasks. A bad actor in the mix could easily use this to harm your workers and your cluster.\n\n  In the future, we'll probably have this as a flag on a team, indicating whether they're permitted to run privileged builds.\n\n* There are no networking restrictions in place, and traffic to and from the workers is currently unencrypted and unauthorized. Anyone could run a task that does horrible things to your worker's containers, possibly stealing sensitive information.\n\n  This can be remedied with configuration specified on Garden to restrict access to the internal network, but this is not detailed in our docs, and we'll probably want to find a better answer than configuration in the future.\n\n","depth":3,"section_tag":"teams-caveats"},"thanks":{"location":"contribute.html#thanks","title":"Thanks!","text":"It's been a long journey and we've got a lot of people to thank for our continued success. We are deeply indebted to any and all who help keep this project going, but the heroic effort of the following organizations is worth giving special props.\n\n* Pivotal\n\n  Concourse wouldn't be what it is today without Pivotal. This goes beyond the sponsorship, which began in early 2015 - without the experiences we had and the practices we learned while working on Cloud Foundry and BOSH, we would have neither the technical experience nor the strong opinions that led to Concourse being born.\n\n  Pivotal's sponsorship continues strong into 2019, where we have a team of full-time engineers, PMs, and designers dedicated to pushing Concourse forward.\n\n* Stark \u0026 Wayne\n\n  The Concourse Tutorial by Stark \u0026 Wayne  is probably the only reason a lot of people were able to learn Concourse. It's a great asset and does its job so well that we've decided to basically just kill our tutorials and delegate to theirs. Thanks to Dr. Nic for writing and maintaining it, and everyone else that has helped out!\n\n","depth":3,"section_tag":"thanks"},"time-trigger-example":{"location":"time-trigger-example.html","title":"time-triggered job example","text":"The time resource can be used to trigger a job.\n\n","depth":2,"section_tag":"time-trigger-example"},"tracing":{"location":"tracing.html","title":"Tracing","text":"This is an experimental feature.\n\nTracing in Concourse enables the delivery of traces related to the internal processes that go into running builds, and other internal operations, breaking them down by time, and component.\n\nIt currently only integrates with Jaeger, although support for other systems is planned to expand as the underlying SDK (OpenTelemetry) evolves.\n\n","depth":3,"section_tag":"tracing"},"trademarks":{"location":"trademarks.html","title":"Trademark Policy","text":"* PURPOSE. VMware Inc. (“VMware”) owns a number of international trademarks and logos that identify the Concourse community and individual Concourse projects (“Concourse Marks”). These trademarks include, but are not limited to:\n\n  * Words: CONCOURSE\n\n  * Logos: {image: images/trademarks/concourse-black.png}\n\n  This policy outlines VMware’s policy and guidelines about the use of the Concourse trademarks by members of the Concourse development and user community.\n\n* WHY HAVE TRADEMARK GUIDELINES? The Concourse Marks are a symbol of the quality and community support associated with the Concourse open source software. Trademarks protect not only those using the marks but the entire community as well. Our community members need to know that they can rely on the quality and capabilities represented by the brand. We also want to provide a level playing field. No one should use the Concourse marks in ways that mislead or take advantage of the community or make unfair use of the trademarks. Also, use of the Concourse Marks should not be in a disparaging manner because we prefer that our marks not be used to be rude about the Concourse open source project or its members.\n\n* OPEN SOURCE LICENSE VS. TRADEMARKS. The Apache 2.0 license gives you the right to use, copy, distribute and modify the Concourse software. However, open source licenses like the Apache 2.0 license do not address trademarks.  Concourse Marks need to be used in a way consistent with trademark law, and that is why we have prepared this policy – to help you understand what branding is allowed or required when using our software under the Apache license.\n\n* PROPER USE OF THE CONCOURSE MARKS. We want to encourage a robust community for the Concourse open source project. Therefore, you may do any of the following, as long as you do so in a way that does not devalue, dilute, or disparage the Concourse brand. In other words, when you do these things, you should behave responsibly and reasonably in the interest of the community, but you do not need a trademark license from us to do them.\n\n  * Nominative Use. You may engage in “nominative use” of the Concourse name, but this does not allow you to use the logo.  Nominative use is sometimes called “fair use” of a trademark, and does not require a trademark license from us.  Here are examples:\n\n    * You may use the Concourse Marks in connection with the development of tools, add-ons, or utilities that are compatible with bit-for-bit identical copies of official Concourse software. For example, if you are developing a Foobar tool for Concourse, acceptable project titles would be “Foobar for Concourse\".\n\n    * You may use the Concourse Marks in connection with your non-commercial redistribution of (1) bit-for-bit identical copies of official Concourse software, and (2) unmodified copies of official Concourse source packages.  For example, if your Foobar product included a full redistribution of official Concourse Software or source code packages: \"Concourse-powered Foobar Product\". We strongly discourage, and likely would consider it a trademark problem, to use a name such as “Concourse Foobar.”\n\n    * If you offer maintenance, support, or hosting services for Concourse software, you may accurately state that in your marketing materials or portfolio, without using the Concourse logo.\n\n    * You may modify the Concourse software and state that your modified software is “based on the Concourse software” or a similar accurate statement, without using the Concourse logo.\n\n    * You may engage in community advocacy. The Concourse software is developed by and for its community. We will allow the use of the word trademark in this context, provided:\n\n      * The trademark is used in a manner consistent with this policy.\n\n      * There is no commercial purpose behind the use.\n\n      * There is no suggestion that your project is approved, sponsored, or affiliated with Concourse.\n\n      * You may create Concourse user or development groups, and publicize meetings or discussions for those groups.\n\n  * Attribution. Identify the trademarks as trademarks of Concourse, as set forth in Section 7.\n\n  * Redistribution of Binaries. If you redistribute binaries that you have downloaded from the Concourse repository, you should retain the logos and name of the product.  However, if you make any changes to the binaries (other than configuration or installation changes that do not involve changes to the source code), or if you re-build binaries from our source code, you should not use our logos.  Our logos represent our quality control, so they should be retained where the product has been built by us, but not otherwise.\n\n  * Capitalization. “Concourse” should always be capitalized and one word.\n\n* IMPROPER USE OF THE TRADEMARKS AND LOGOS. Use of the logo is reserved solely for use by Concourse in its unaltered form. Examples of unauthorized use of the Concourse trademarks include:\n\n  * Commercial Use: You may not use the Concourse Marks in connection with commercial redistribution of Concourse software (commercial redistribution includes, but is not limited to, redistribution in connection with any commercial business activities or revenue-generating business activities) regardless of whether the Concourse software is unmodified.\n\n  * Entity Names. You may not form a company, use a company name, or create a software product name that includes the “Concourse” trademark, or implies any foundational or authorship role. If you have a software product that works with Concourse, it is suggested you use terms such as ‘\u003cproduct name\u003e for Concourse’ or ‘\u003cproduct name\u003e, Concourse Edition.” If you wish to form an entity for a user or developer group, please contact us and we will be glad to discuss a license for a suitable name.\n\n  * Class or Quality. You may not imply that you are providing a class or quality of Concourse (e.g., \"enterprise-class\" or \"commercial quality\") in a way that implies Concourse is not of that class, grade or quality, nor that other parties are not of that class, grade, or quality.\n\n  * Combinations. Use of the Concourse Marks to identify software that combines any portion of the Concourse software with any other software, unless the combined distribution is an official Concourse distribution. For example, you may not distribute a combination of the Concourse  software with software released by the Foobar project under the name “Concourse Foobar Distro”.\n\n  * False or Misleading Statements. You may not make false or misleading statements regarding your use of Concourse (e.g., \"we wrote the majority of the code\" or \"we are major contributors\" or \"we are committers\").\n\n  * Domain Names. You must not use Concourse or any confusingly similar phrase in a domain name. For instance “www.concoursehost.com” is not allowed.  If you wish to use such a domain name for a non-commercial user or developer group to engage in community advocacy, please contact us and we will be glad to discuss a license for a suitable domain name.  Because of the many persons who, unfortunately, seek to spoof, swindle or deceive the community by using confusing domain names, we must be very strict about this rule.\n\n  * Merchandise. You must not manufacture, sell or give away merchandise items, such as T-shirts and mugs, bearing the Concourse logo, or create any mascot for the project. If you wish to use the logo to do this for a non-commercial user or developer group to engage in community advocacy, please contact us and we will be glad to discuss a license to do this.\n\n  * Variations, takeoffs or abbreviations. You may not use a variation of the Concourse name or logo for any purpose other than common usage of these in community communications. For example, the following are not acceptable:\n\n    * CONCRSE\n\n    * MyConcourse\n\n    * ConcourseDB\n\n    * ConcourseHost\n\n    * ConcourseGuru\n\n  * Endorsement or Sponsorship. You may not use the Concourse trademarks in a manner that would imply Concourse’s affiliation with or endorsement, sponsorship, or support of a product or service.\n\n  * Rebranding. You may not change the trademark on unmodified Concourse software to your own brand.  You may not hold yourself out as the source of the Concourse software, except to the extent you have modified it as allowed under the Apache 2.0 license, and you make it clear that you are the source only of the modification.\n\n  * Combination Marks. Do not use our trademarks in combination with any other marks or logos (for example Foobar Concourse, or the name of your company or product typeset to look like the Concourse logo).\n\n  * Web Tags. Do not use the Concourse trademark in a title or meta tag of a web page to influence search engine rankings or result listings, rather than for discussion or advocacy of the Concourse project.\n\n* PROPER ATTRIBUTION. When you use a Concourse trademark you should include a statement attributing the trademark to Concourse. For example, \"Concourse is a trademark of Concourse in the U.S. and other countries.\"\n\n* MORE QUESTIONS? If you have questions about this policy, please contact us at concourseteam+trademarks@gmail.com.\n\n","depth":2,"section_tag":"trademarks"},"troubleshooting-and-fixing-dns-resolution":{"location":"concourse-worker.html#troubleshooting-and-fixing-dns-resolution","title":"Troubleshooting and fixing DNS resolution","text":"By default, containers created by Guardian will carry over the /etc/resolv.conf from the host into the container. This is often fine, but some Linux distributions configure a special 127.x.x.x DNS resolver (e.g. systemd-resolved).\n\nWhen Guardian copies the resolv.conf over, it removes these entries, as they won't be reachable from the container's network namespace. As a result, your containers may not have any valid nameservers configured.\n\nTo diagnose this problem you can fly intercept into a failing container and check which nameservers are in /etc/resolv.conf:\n\n$ fly -t ci intercept -c concourse/concourse\nbash-5.0# grep nameserver /etc/resolv.conf\nbash-5.0#\nIn this case it is empty, as the host only listed a single 127.0.0.53 address which was then stripped out. To fix this, you'll just need to explicitly configure DNS instead of relying on the gdn default behavior.\n\n","depth":6,"section_tag":"troubleshooting-and-fixing-dns-resolution"},"try-step":{"location":"jobs.html#try-step","title":"try step","text":"","depth":3,"section_tag":"steps"},"upgrading-concourse":{"location":"upgrading-concourse.html","title":"Upgrading Concourse","text":"Concourse is upgraded by stopping the Concourse process, swapping out the concourse binary with the new one, and re-starting it. This should be done on each Running a web node and Running a worker node. Be careful to check the release notes for anything marked 'breaking' - in particular, you'll want to look for any flags that have changed.\n\nEach Running a web node will automatically run migrations on start and locks via the database to ensure only one of them runs the migrations. We currently do not guarantee zero-downtime upgrades, as migrations may make changes that confuse the older web nodes. This should resolve as each node is upgraded, and shouldn't result in any inconsistent state.\n\nTypically, Concourse can be upgraded from any version to any other version, though around 3.x and 4.x we made some changes to how migrations are run, and as a result the following upgrade paths must be followed:\n\n| Current Version | Upgrade Path |\n| \u003c v3.6.0 | v3.6.0 -\u003e v4.0.0 -\u003e latest |\n| = v3.6.0 | v4.0.0 -\u003e latest |\n\nWe'll try to minimize this kind of thing in the future.\n\n","depth":3,"section_tag":"upgrading-concourse"},"user-roles":{"location":"user-roles.html","title":"User Roles \u0026 Permissions","text":"Concourse comes with five roles:\n\n1. Concourse Admin\n\n2. Team Owner\n\n3. Team Member\n\n4. Pipeline Operator\n\n5. Team Viewer\n\nThese roles are strictly ordered, so that each role always has all the permissions of any other role lower on the list. This means that a Pipeline Operator can always do anything a Team Viewer can, and so on.\n\nIn this document we say an action is assigned to a role if that role is capable of performing the action, but any less-privileged role is not. For example, the SaveConfig action is assigned to the member role, so owners and members can set a pipeline config, but pipeline operators and viewers cannot.\n\n","depth":3,"section_tag":"user-roles"},"using-a-local-dns-server":{"location":"concourse-worker.html#using-a-local-dns-server","title":"Using a local DNS server","text":"If you would like to use Consul, dnsmasq, or some other DNS server running on the worker VM, you'll have to configure the internal address of the VM as the DNS server and allow the containers to reach the address, like so:\n\n[server]\n; internal IP of the worker machine\ndns-server = 10.1.2.3\n\n; allow containers to reach the above IP\nallow-host-access = true\nSetting allow-host-access will, well, allow containers to access your host VM's network. If you don't trust your container workloads, you may not want to allow this.\n\nTo validate whether the changes have taken effect, you can fly intercept into any container and check /etc/resolv.conf once again:\n\n$ fly -t ci intercept -c concourse/concourse\nbash-5.0# cat /etc/resolv.conf\nnameserver 10.1.2.3\nbash-5.0# nslookup concourse-ci.org\nServer:         10.1.2.3\nAddress:        10.1.2.3#53\n\nNon-authoritative answer:\nName:   concourse-ci.org\nAddress: 185.199.108.153\nName:   concourse-ci.org\nAddress: 185.199.109.153\nName:   concourse-ci.org\nAddress: 185.199.110.153\nName:   concourse-ci.org\nAddress: 185.199.111.153\nIf nslookup times out or fails, you may need to open up firewalls or security group configuration so that the worker VM can send UDP/TCP packets to itself.\n\n","depth":7,"section_tag":"using-a-local-dns-server"},"var-interpolation":{"location":"vars.html#var-interpolation","title":"Interpolation","text":"Values for vars are substituted structurally. That is, if you have foo: ((bar)), whatever value ((bar)) resolves to will become the value of the foo field in the object. This can be a value of any type and structure: a boolean, a simple string, a multiline credential like a certificate, or a complicated data structure like an array of objects.\n\nThis differs from text-based substitution in that it's impossible for a value to result in broken YAML syntax, and it relieves the template author from having to worry about things like whitespace alignment.\n\nWhen a ((var)) appears adjacent to additional string content, e.g. foo: hello-((bar))-goodbye, its value will be concatenated with the surrounding content. If the ((var)) resolves to a non-string value, an error will be raised.\n\n","depth":3,"section_tag":"var-interpolation"},"var-sources":{"location":"vars.html#var-sources","title":"Var sources (experimental)","text":"var_sources was introduced in Concourse v5.8.0. It is considered an experimental feature until its associated RFC is resolved.\n\nVar sources can be configured for a pipeline via pipelinevar_sources.\n\nEach var source has a name which is then referenced as the source-name in var syntax, e.g. ((my-vault:test-user.username)) to fetch the test-user var from the my-vault var source. See ((var)) syntax for a detailed explanation of this syntax.\n\nCurrently, only two types are supported: vault and dummy. In the future we want to make use of something like the Prototypes (RFC #37) so that third-party credential managers can be used just like resource types.\n\n","depth":4,"section_tag":"var-sources"},"var-syntax":{"location":"vars.html#var-syntax","title":"((var)) syntax","text":"The full syntax for vars is ((source-name:secret-path.secret-field)).\n\nThe optional source-name identifies the var source from which the value will be read. If omitted (along with the : delimiter), the cluster-wide credential manager will be used, or the value may be provided statically. The special name . refers to the local var source, while any other name refers to a var source.\n\nThe required secret-path identifies the location of the credential. The interpretation of this value depends on the var source type. For example, with Vault this may be a path like path/to/cred. For the Kubernetes secret manager this may just be the name of a secret. For credential managers which support path-based lookup, a secret-path without a leading / may be queried relative to a predefined set of path prefixes. This is how the Vault credential manager currently works; foo will be queried under /concourse/(team name)/(pipeline name)/foo.\n\nThe optional secret-field specifies a field on the fetched secret to read. If omitted, the credential manager may choose to read a 'default field' from the fetched credential if the field exists. For example, the Vault credential manager will return the value of the value field if present. This is useful for simple single-value credentials where typing ((foo.value)) would feel verbose.\n\n","depth":3,"section_tag":"var-syntax"},"variables":{"location":"pipeline-vars-example.html#variables","title":"Variables","text":"---\nfirst: initial\nnumber: \"9000\"\nhello: HAL\n","depth":3,"section_tag":"variables"},"vars":{"location":"vars.html","title":"Vars","text":"Concourse supports value substitution in YAML configuration by way of ((vars)).\n\nAutomation entails the use of all kinds of credentials. It's important to keep these values separate from the rest of your configuration by using vars instead of hardcoding values. This allows your configuration to be placed under source control and allows credentials to be tucked safely away into a secure credential manager like Vault instead of the Concourse database.\n\nAside from credentials, vars may also be used for generic parameterization of pipeline configuration templates, allowing a single pipeline config file to be configured multiple times with different parameters - e.g. ((branch_name)).\n\n","depth":2,"section_tag":"vars"},"vault-approle-auth":{"location":"vault-credential-manager.html#vault-approle-auth","title":"Using the approle auth backend","text":"The approle backend allows for an app (in this case, Concourse) to authenticate with a role pre-configured in Vault.\n\nWith this backend, the Running a web node is configured with a role_id corresponding to a pre-configured role, and a secret_id which is used to authenticate and acquire a token.\n\nThe approle backend must first be configured in Vault. Vault's approle backend allows for a few parameters which you may want to set to determine the permissions and lifecycle of its issued tokens:\n\npolicies=names: This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.\n\n\ntoken_ttl=duration: This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.\n\n\ntoken_max_ttl=duration: This sets a maximum lifetime for each token, after which the token can no longer be renewed.\n\nIf configured, be sure to set the same value on the web node so that it can re-auth before this duration is reached:\n\nCONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n\nperiod=duration: If configured, tokens issued will be periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both period and token_max_ttl as the max TTL will be ignored.\n\n\ntoken_num_uses=count: This sets a limit on how often a token can be used. We do not recommend setting this value, as it will effectively hamstring Concourse after a few credential acquisitions. The web node does not currently know to re-acquire a token when this limit is reached.\n\n\nsecret_id_ttl=duration and secret_id_num_uses=count: These two configurations will result in the secret ID expiring after the configured time or configured number of log-ins, respectively.\n\nYou should only set these if you have something periodically re-generating secret IDs and re-configuring your web nodes accordingly.\n\n\n\nGiven all that, a typical configuration may look something like this:\n\n$ vault auth enable approle\nSuccess! Enabled approle auth method at: approle/\n$ vault write auth/approle/role/concourse policies=concourse period=1h\nSuccess! Data written to: auth/approle/role/concourse\nNow that the backend is configured, we'll need to obtain the role_id and generate a secret_id:\n\n$ vault read auth/approle/role/concourse/role-id\nKey        Value\n---        -----\nrole_id    5f3420cd-3c66-2eff-8bcc-0e8e258a7d18\n$ vault write -f auth/approle/role/concourse/secret-id\nKey                   Value\n---                   -----\nsecret_id             f7ec2ac8-ad07-026a-3e1c-4c9781423155\nsecret_id_accessor    1bd17fc6-dae1-0c82-d325-3b8f9b5654ee\nThese should then be set on the Running a web node like so:\n\nCONCOURSE_VAULT_AUTH_BACKEND=\"approle\"\nCONCOURSE_VAULT_AUTH_PARAM=\"role_id:5f3420cd-3c66-2eff-8bcc-0e8e258a7d18,secret_id:f7ec2ac8-ad07-026a-3e1c-4c9781423155\"\n","depth":6,"section_tag":"vault-approle-auth"},"vault-cert-auth":{"location":"vault-credential-manager.html#vault-cert-auth","title":"Using the cert auth backend","text":"The cert auth method allows authentication using SSL/TLS client certificates.\n\nWith this backend, the Running a web node is configured with a client cert and a client key. Vault must be configured with TLS, which you should be almost certainly be doing anyway.\n\nThe cert backend must first be configured in Vault. The backend is associated to a policy and a CA cert used to verify the client certificate. It may also be given the client certificate itself.\n\nThe cert backend must first be configured in Vault. Vault's cert backend allows for a few parameters which you may want to set to determine the lifecycle of its issued tokens:\n\npolicies=names: This determines the policies (comma-separated) to set on each token. Be sure to set one that has access to the secrets path - see Configuring the secrets engine for more information.\n\n\nttl=duration: This determines the TTL for each token granted. The token can be continuously renewed, as long as it is renewed before the TTL elapses.\n\n\nmax_ttl=duration: This sets a maximum lifetime for each token, after which the token can no longer be renewed.\n\nIf configured, be sure to set the same value on the web node so that it can re-auth before this duration is reached:\n\nCONCOURSE_VAULT_AUTH_BACKEND_MAX_TTL=1h\n\nperiod=duration: If configured, tokens issued will be periodic. Periodic tokens are not bound by any configured max TTL, and can be renewed continuously. It does not make sense to configure both period and max_ttl as the max TTL will be ignored.\n\n\n\n$ vault auth enable cert\nSuccess! Enabled cert auth method at: cert/\n$ vault write auth/cert/certs/concourse policies=concourse certificate=@out/vault-ca.crt ttl=1h\nSuccess! Data written to: auth/cert/certs/concourse\nOnce that's all set up, you'll just need to configure the client cert and key on the web node like so:\n\nCONCOURSE_VAULT_AUTH_BACKEND=\"cert\"\nCONCOURSE_VAULT_CLIENT_CERT=vault-certs/concourse.crt\nCONCOURSE_VAULT_CLIENT_KEY=vault-certs/concourse.key\nIn this case no additional auth params are necessary, as the Vault's TLS auth backend will check the certificate against all roles if no name is specified.\n\n","depth":6,"section_tag":"vault-cert-auth"},"vault-credential-lookup-rules":{"location":"vault-credential-manager.html#vault-credential-lookup-rules","title":"Credential lookup rules","text":"When resolving a parameter such as ((foo_param)), Concourse will look in the following paths, in order:\n\n* /concourse/TEAM_NAME/PIPELINE_NAME/foo_param\n\n* /concourse/TEAM_NAME/foo_param\n\nVault credentials are actually key-value, so for ((foo)) Concourse will default to the field name value. You can specify the field to grab via . syntax, e.g. ((foo.bar)).\n\nIf the action is being run in the context of a pipeline (e.g. a check or a step in a build of a job), Concourse will first look in the pipeline path. If it's not found there, it will look in the team path. This allows credentials to be scoped widely if they're common across many pipelines.\n\nIf an action is being run in a one-off build, Concourse will only look in the team path.\n\nThe leading /concourse can be changed by specifying the following:\n\nCONCOURSE_VAULT_PATH_PREFIX=/some-other-prefix\n","depth":5,"section_tag":"vault-credential-lookup-rules"},"vault-credential-manager":{"location":"vault-credential-manager.html","title":"The Vault credential manager","text":"Concourse can be configured to pull credentials from a Vault instance.\n\nTo configure this, first configure the URL of your Vault server by setting the following env on the Running a web node:\n\nCONCOURSE_VAULT_URL=https://vault.example.com:8200\nYou may also need to configure the CA cert for Vault:\n\nCONCOURSE_VAULT_CA_CERT=path/to/ca.crt\nYou'll also need to configure how the web node authenticates with Vault - see Authenticating with Vault for more details as that step is quite involved.\n\n","depth":4,"section_tag":"vault-credential-manager"},"vault-periodic-token":{"location":"vault-credential-manager.html#vault-periodic-token","title":"Using a periodic token","text":"The simplest way to authenticate is by generating a periodic token:\n\n$ vault token create --policy concourse --period 1h\nKey                Value\n---                -----\ntoken              s.mSNnbhGAqxK2ZbMasOQ91rIA\ntoken_accessor     0qsib5YcYvROm86cT08IFxIT\ntoken_duration     1h\ntoken_renewable    true\ntoken_policies     [concourse default]\nChoose your --period wisely, as the timer starts counting down as soon as the token is created. You should also use a duration long enough to account for any planned web node downtime.\n\nOnce you have the token, just set the following env on the web node:\n\nCONCOURSE_VAULT_CLIENT_TOKEN=s.mSNnbhGAqxK2ZbMasOQ91rIA\nPeriodic tokens are the quickest way to get started, but they have one fatal flaw: if the web node is down for longer than the token's configured period, the token will expire and a new one will have to be created and configured. This can be avoided by using the approle auth backend.\n\n","depth":6,"section_tag":"vault-periodic-token"},"vault-shared-path":{"location":"vault-credential-manager.html#vault-shared-path","title":"Configuring a shared path","text":"A \"shared path\" can also be configured for credentials that you would like to share across all teams and pipelines, foregoing the default team/pipeline namespacing. Use with care!\n\nCONCOURSE_VAULT_SHARED_PATH=some-shared-path\nThis path must exist under the configured path prefix. The above configuration would correspond to /concourse/some-shared-path with the default /concourse prefix.\n\n","depth":6,"section_tag":"vault-shared-path"},"vault-var-source":{"location":"vars.html#vault-var-source","title":"vault var source","text":"","depth":4,"section_tag":"var-sources"},"version-pinning":{"location":"resource-versions.html#version-pinning","title":"Version Pinning","text":"A regular job workflow is to use the latest version of a resource in order to trigger new builds. This works most of the time until you run into a situation where you need to run the job using an old version of a resource. Concourse provides a solution to this, which is called resource pinning.\n\nThere are two different ways to pin a resource: through the pipeline config and through the web UI. Within the pipeline config, you can either pin the resource to a version through the resource configuration or through a get step version configuration. If you would like to pin through the web UI, the functionality can be found in the resource version history page which is accessed through clicking into the resource within the pipeline page.\n\nPinning through the pipeline config is useful for a more permanent pinned state. If a resource is pinned through the pipeline config, it cannot be modified through the web UI and can only be changed through modifiying and resetting the pipeline config.\n\nPinning through the web UI is useful for reactionary pinning of a resource. For example, it can be used in the event of a broken upstream dependency.\n\nIf you had a version pinned in the web UI and then pinned it through the pipeline config, the pipeline config pinned version will take precendence.\n\nA pinned version is associated to a resource and can be viewed in the resource page (excluding the case that the version was pinned on a get step). This pinned version will be propagated throughout the pipeline, and used by the jobs that take that pinned resource as an input. If there is a job that has a passed constraint on a pinned resource, this means that the input is only valid if that pinned version has been used by the passed constraint job.\n\nLet's say we have a pipeline with two jobs and one resource that is being used as a passed constraint between the two jobs. If that resource is pinned to a version, the first job will produce a build using the pinned version of the resource. After that build succeeds, the second job that has a passed constraint on the first will then be able to trigger off a build because the pinned version has successfully used by the first job.\n\n","depth":4,"section_tag":"version-pinning"},"version-unpinning":{"location":"resource-versions.html#version-unpinning","title":"Unpinning","text":"When a version is unpinned, Concourse will go back to using the latest available version. This means a new build will be queued up if the most recent build used the old pinned version and the input has trigger: true.\n\nIf you would like to learn more about how version pinning and unpinning works with the build scheduler, you can read more about it in the scheduling behavior section.\n\n","depth":5,"section_tag":"version-unpinning"},"volume-locality-strategy":{"location":"container-placement.html#volume-locality-strategy","title":"The volume-locality strategy","text":"When using volume-locality, the Running a web node places task step and put step containers on workers where a majority of their inputs are already present. This is the default strategy.\n\nThe advantage of this approach is that it reduces the likelihood that large artifacts will have to be streamed from one Running a worker node, through the Running a web node, and to the target Running a worker node. For large artifacts, this can result in quite a bit of overhead.\n\nThe disadvantage of this approach is that it can sometimes result in builds \"gravitating\" to a particular worker and overloading it, at least until the resource caches warm across the worker pool.\n\nIf your builds tend to be light on artifacts and heavy on task execution, you may want to try the The fewest-build-containers strategy instead.\n\n","depth":4,"section_tag":"volume-locality-strategy"},"web-configuration":{"location":"concourse-web.html#web-configuration","title":"Configuring the web node","text":"","depth":4,"section_tag":"web-configuration"},"web-connection-pooling":{"location":"concourse-web.html#web-connection-pooling","title":"Database connection pooling","text":"You may wish to configure the max number of parallel database connections that each node makes. There are two pools to configure: one for serving API requests, and one used for all the backend work such as pipeline scheduling. Another pool is reserved for garbage collection and has a hardcoded cap at 5 connections.\n\nThe sum of these numbers across all web nodes should not be greater than the maximum number of simultaneous connections your Postgres server will allow. See db node resource utilization for more information.\n\nFor example, if 3 web nodes are configured as such:\n\nCONCOURSE_API_MAX_CONNS=10     # default\nCONCOURSE_BACKEND_MAX_CONNS=50 # default\n...then your PostgreSQL server should be configured with a connection limit of at least 195: (10 + 50 + 5) * 3.\n\n","depth":6,"section_tag":"web-connection-pooling"},"web-ingress":{"location":"concourse-web.html#web-ingress","title":"Configuring ingress traffic","text":"If your web nodes are going to be accessed over the network, you will need to set CONCOURSE_EXTERNAL_URL to a URL accessible by your Concourse users. If you don't set this property, logging in will incorrectly redirect to its default value of 127.0.0.1.\n\nIf your instance is available on the public internet, you may wish to prevent the Concourse UI from being nefariously embedded as an iframe by setting CONCOURSE_X_FRAME_OPTIONS to deny (to prevent any iframe embeddings) or sameorigin (to only allow iframe embeddings in pages served from the same subdomain). This protects against clickjacking.\n\nNote: If setting the value to allow-from, please note that not all browsers support this value and when not supported, the header is ignored by the browser.\n\n","depth":5,"section_tag":"web-ingress"},"web-node":{"location":"concourse-web.html","title":"Running a web node","text":"The web node is responsible for running the web UI, API, and as well as performing all pipeline scheduling. It's basically the brain of Concourse.\n\n","depth":3,"section_tag":"concourse-web"},"web-operation":{"location":"concourse-web.html#web-operation","title":"Operating a web node","text":"The web nodes themselves are stateless - they don't store anything on disk, and coordinate entirely using the database.\n\n","depth":4,"section_tag":"web-operation"},"web-prerequisites":{"location":"concourse-web.html#web-prerequisites","title":"Prerequisites","text":"Nothing special - the web node is a pretty simple Go application that can be run like a 12-factor app.\n\n","depth":4,"section_tag":"web-prerequisites"},"web-resource-utilization":{"location":"concourse-web.html#web-resource-utilization","title":"Resource utilization","text":"CPU usage: peaks during pipeline scheduling, primarily when scheduling Jobs. Mitigated by adding more web nodes. In this regard, web nodes can be considered compute-heavy more than anything else at large scale.\n\nMemory usage: not very well classified at the moment as it's not generally a concern. Give it a few gigabytes and keep an eye on it.\n\nDisk usage: none\n\nBandwidth usage: aside from handling external traffic, the web node will at times have to stream bits out from one worker and into another while executing Steps.\n\nHighly available: yes; web nodes can all be configured the same (aside from --peer-address) and placed behind a load balancer. Periodic tasks like garbage-collection will not be duplicated for each node.\n\nHorizontally scalable: yes; they will coordinate workloads using the database, resulting in less work for each node and thus lower CPU usage.\n\nOutbound traffic:\n\n* db on its configured port for persistence\n\n* db on its configured port for locking and coordinating in a multi-web node deployment\n\n* other web nodes (possibly itself) on an ephemeral port when a worker is forwarded through the web node's TSA\n\nInbound traffic:\n\n* worker connects to the TSA on port 2222 for registration\n\n* worker downloads inputs from the ATC during Running tasks with fly execute via its external URL\n\n* external traffic to the ATC API via the web UI and The fly CLI\n\n","depth":5,"section_tag":"web-resource-utilization"},"web-running":{"location":"concourse-web.html#web-running","title":"Running concourse web","text":"The concourse CLI can run as a web node via the web subcommand.\n\nBefore running it, let's configure a local user so we can log in:\n\nCONCOURSE_ADD_LOCAL_USER=myuser:mypass\nCONCOURSE_MAIN_TEAM_LOCAL_USER=myuser\nThis will configure a single user, myuser, with the password mypass. You'll probably want to change those to sensible values, and later you may want to configure a proper auth provider - check out Auth \u0026 Teams whenever you're ready.\n\nNext, you'll need to configure the session signing key, the SSH key for the worker gateway, and the authorized worker key. Check Generating Keys to learn what these are and how they are created.\n\nCONCOURSE_SESSION_SIGNING_KEY=path/to/session_signing_key\nCONCOURSE_TSA_HOST_KEY=path/to/tsa_host_key\nCONCOURSE_TSA_AUTHORIZED_KEYS=path/to/authorized_worker_keys\nFinally, web needs to know how to reach your Postgres database. This can be set like so:\n\nCONCOURSE_POSTGRES_HOST=127.0.0.1 # default\nCONCOURSE_POSTGRES_PORT=5432      # default\nCONCOURSE_POSTGRES_DATABASE=atc   # default\nCONCOURSE_POSTGRES_USER=my-user\nCONCOURSE_POSTGRES_PASSWORD=my-password\nIf you're running PostgreSQL locally, you can probably just point it to the socket and rely on the peer auth:\n\nCONCOURSE_POSTGRES_SOCKET=/var/run/postgresql\nNow that everything's set, run:\n\nconcourse web\nAll logs will be emitted to stdout, with any panics or lower-level errors being emitted to stderr.\n\n","depth":4,"section_tag":"web-running"},"what-resources-produce":{"location":"checker.html#what-resources-produce","title":"What do resource checks produce?","text":"The whole point of running checks is to produce versions. Everything that happens in concourse is centered around the idea of resource versions. It's how concourse determines that something is new and a new build needs to be triggered. The versions produced by each resource all vary to a large degree. The git resource, for example, uses commits hashes, the registry image resource uses the digest sha, and the time resource uses timestamps. Every resource is different, but the end result is the same, a new input for your workload.\n\n","depth":5,"section_tag":"what-resources-produce"},"whats-emitted":{"location":"metrics.html#whats-emitted","title":"What's emitted?","text":"This reference section lists of all of the metrics that Concourse emits via the Prometheus emitter.\n\nTo make this document easy to maintain, Prometheus is used as the \"source of truth\" - primarily because it has help text built-in, making this list easy to generate. Treat this list as a reference when looking for the equivalent metric names for your emitter of choice.\n\nTotal number of Concourse builds aborted.Total number of Concourse builds errored.Total number of Concourse builds failed.Total number of Concourse builds finished.Number of Concourse builds currently running.Total number of Concourse builds started.Total number of Concourse builds succeeded.Current number of concourse database connectionsTotal number of database Concourse database queriesResponse time in secondsTotal number of Concourse jobs scheduled.Number of Concourse jobs currently being scheduled.The size of the checks queueTotal number of checks enqueuedTotal number of checks finishedTotal number of checks startedDatabase locks heldNumber of containers per workerNumber of workers per state as seen by the databaseNumber of active tasks per workerNumber of volumes per workerA summary of the GC invocation durations.Number of goroutines that currently exist.Information about the Go environment.Number of bytes allocated and still in use.Total number of bytes allocated, even if freed.Number of bytes used by the profiling bucket hash table.Total number of frees.The fraction of this program's available CPU time used by the GC since the program started.Number of bytes used for garbage collection system metadata.Number of heap bytes allocated and still in use.Number of heap bytes waiting to be used.Number of heap bytes that are in use.Number of allocated objects.Number of heap bytes released to OS.Number of heap bytes obtained from system.Number of seconds since 1970 of last garbage collection.Total number of pointer lookups.Total number of mallocs.Number of bytes in use by mcache structures.Number of bytes used for mcache structures obtained from system.Number of bytes in use by mspan structures.Number of bytes used for mspan structures obtained from system.Number of heap bytes when next garbage collection will take place.Number of bytes used for other system allocations.Number of bytes in use by the stack allocator.Number of bytes obtained from system for stack allocator.Number of bytes obtained from system.Number of OS threads created.Total user and system CPU time spent in seconds.Maximum number of open file descriptors.Number of open file descriptors.Resident memory size in bytes.Start time of the process since unix epoch in seconds.Virtual memory size in bytes.Maximum amount of virtual memory available in bytes.Current number of scrapes being served.Total number of scrapes by HTTP status code.\n\n","depth":4,"section_tag":"whats-emitted"},"whats-encrypted":{"location":"encryption.html#whats-encrypted","title":"What's encrypted?","text":"The following values are expected to contain credentials, and so will be encrypted:\n\n* Resource resourcesources, as they often contain private keys and other credentials for writing to (or simply granting access to) the resource.\n\n* Resource type resource_typesources, for the same reason as above, though this is probably a less common use case.\n\n* Pipeline task step vars and task step params, in case they contain sensitive information such as usernames and/or passwords.\n\n* Put step put step params and get step get step params are also encrypted, even though they rarely should contain credentials (they're usually in resourcesource).\n\n* Team auth configurations, as they often contain things like GitHub or other oAuth client secrets.\n\nNote that the actual implementation encrypts things in a more heavy-handed way than the above list implies. For example, pipeline configs are actually encrypted as one large blob.\n\nNotably, the following things are NOT encrypted:\n\n* Build logs. If your jobs are outputting credentials, encryption won't help you. We have chosen not to tackle this initially as it would introduce a performance burden for what is not as much of an obvious win.\n\n* Resource versions. These should never contain credentials, and are often meaningless on their own.\n\n* Resource metadata. These are visible to anyone if your pipeline is exposed, and should never contain credentials.\n\n* Pipeline names, job names, etc. - anything else that is not a high-risk target for credential leakage, as opposed to regular information leaks.\n\n  Resources and jobs in particular exist in their own tables, with their names in plaintext, and only their config encrypted. In this way, names are not protected, even though the pipeline config itself is also stored as one big encrypted blob.\n\n","depth":4,"section_tag":"whats-encrypted"},"when-are-resources-checked":{"location":"checker.html#when-are-resources-checked","title":"When are resources checked?","text":"The component that schedules and runs the resource checks is called the resource checker. The rate at which these checks happen is called the check interval. There's an obvious tradeoff, whereby the more frequently you poll, the bigger the strain on Concourse (as well as the external source), however if you want to pick up those new commits as quickly as possible, then you need to poll as often as possible.\n\nThe resource checker uses the resourcecheck_every interval in order to figure out if a resource needs to be checked. A resource's check_every interval dictates how often it should be checked for new versions, with a default of 1 minute. If that seems like a lot, it is, but it's how Concourse keeps everything snappy. You can configure this value independently for each resource, or if your external service supports it, you can set resourcewebhook_token to basically eliminate the need for polling altogether.\n\nOn every interval tick, the resource checker will see if there are any resources that need to be checked. It does this by first finding resources which are used as inputs to jobs, and then comparing the current time against the last time each resource was checked. If it has been longer than a resource's configured check_every interval, a new check will be enqueued. In practice this means that if a resource has a check_every of 1m, it is not guaranteed to be checked precisely every 60 seconds.\n\n","depth":5,"section_tag":"when-are-resources-checked"},"where-and-what-versions":{"location":"resource-versions.html#where-and-what-versions","title":"Where do they come from and what are they used for?","text":"The resource checker is responsible for checking for new versions of a resource. These versions are then saved into the database and can be viewed from the resource page in the web UI.\n\nResource versions are used by the build scheduler in order to schedule new builds for a job.\n\n","depth":4,"section_tag":"where-and-what-versions"},"worker-configuration":{"location":"concourse-worker.html#worker-configuration","title":"Configuring the worker node","text":"If there's something special about your worker and you'd like to target builds at it specifically, you can configure tags like so:\n\nCONCOURSE_TAG=tag-1,tag-2\nA tagged worker is taken out of the default placement logic. To run build steps on it, specify the steptags. Or, to perform resource checks on it, specify tags on the resource itself.\n\n","depth":4,"section_tag":"worker-configuration"},"worker-heartbeating-and-stalling":{"location":"concourse-worker.html#worker-heartbeating-and-stalling","title":"Worker Heartbeating \u0026 Stalling","text":"Workers will continuously heartbeat to the Concourse cluster in order to remain registered and healthy. If a worker hasn't checked in in a while, possibly due to a network partition, being overloaded, or having crashed, its state will transition to stalled and new workloads will not be scheduled on it until it recovers.\n\nIf the worker remains in this state and cannot be recovered, it can be removed using the fly prune-worker command.\n\n","depth":5,"section_tag":"worker-heartbeating-and-stalling"},"worker-lifecycle":{"location":"internals.html#worker-lifecycle","title":"The worker lifecycle","text":"","depth":4,"section_tag":"worker-lifecycle"},"worker-node":{"location":"concourse-worker.html","title":"Running a worker node","text":"The worker node registers with the Running a web node and is then used for executing builds and performing resource checks. It doesn't really decide much on its own.\n\n","depth":3,"section_tag":"concourse-worker"},"worker-operation":{"location":"concourse-worker.html#worker-operation","title":"Operating a worker node","text":"The worker nodes are designed to be stateless and as interchangeable as possible. Tasks and Resources bring their own Docker images, so you should never have to install dependencies on the worker.\n\nIn Concourse, all important data is represented by Resources, so the workers themselves are dispensible. Any data in the work-dir is ephemeral and should go away when the worker machine is removed - it should not be persisted between worker VM or container re-creates.\n\n","depth":4,"section_tag":"worker-operation"},"worker-prerequisites":{"location":"concourse-worker.html#worker-prerequisites","title":"Prerequisites","text":"* Linux: kernel v3.19 or later with support for user namespaces enabled. (This is off by default in some distributions!)\n\n  To enforce memory limits on tasks, memory + swap accounting must be enabled.\n\n* Windows/Darwin: no special requirements (that we know of).\n\n  Note that containerization is fairly primitive on these two platforms, so don't expect great support for multi-tenancy.\n\n","depth":4,"section_tag":"worker-prerequisites"},"worker-resource-utilization":{"location":"concourse-worker.html#worker-resource-utilization","title":"Resource utilization","text":"CPU usage: almost entirely subject to pipeline workloads. More resources configured will result in more checking, and in-flight builds will use as much CPU as they want.\n\nMemory usage: also subject to pipeline workloads. Expect usage to increase with the number of containers on the worker and spike as builds run.\n\nBandwidth usage: again, almost entirely subject to pipeline workloads. Expect spikes from periodic checking, though the intervals should spread out over enough time. Resource fetching and pushing will also use arbitrary bandwidth.\n\nDisk usage: arbitrary data will be written as builds run, and resource caches will be kept and garbage collected on their own life cycle. We suggest going for a larger disk size if it's not too much trouble. All state on disk must not outlive the worker itself; it is all ephemeral. If the worker is re-created (i.e. fresh VM/container and all processes were killed), it should be brought back with an empty disk.\n\nHighly available: not applicable. Workers are inherently singletons, as they're being used as drivers running entirely different workloads.\n\nHorizontally scalable: yes; workers directly correlate to your capacity required by however many pipelines, resources, and in-flight builds you want to run. It makes sense to scale them up and down with demand.\n\nOutbound traffic:\n\n* external traffic to arbitrary locations as a result of periodic resource checking and running builds\n\n* external traffic to the web node's configured external URL when downloading the inputs for a Running tasks with fly execute\n\n* external traffic to the web node's TSA port (2222) for registering the worker\n\nInbound traffic:\n\n* various connections from the Running a web node on port 7777 (Garden), 7788 (BaggageClaim), and 7799 (garbage collection)\n\n* repeated connections to 7777 and 7788 from the Running a web node's TSA component as it heartbeats to ensure the worker is healthy\n\n","depth":5,"section_tag":"worker-resource-utilization"},"worker-running":{"location":"concourse-worker.html#worker-running","title":"Running concourse worker","text":"The concourse CLI can run as a worker node via the worker subcommand.\n\nFirst, you'll need to configure a directory for the worker to store data:\n\nCONCOURSE_WORK_DIR=/opt/concourse/worker\nThis is where all the builds run, and where all resources are fetched in to, so make sure it's backed by enough storage. Then, configure it like so:\n\nNext, point the worker at your Running a web node like so:\n\nCONCOURSE_TSA_HOST=127.0.0.1:2222\nCONCOURSE_TSA_PUBLIC_KEY=path/to/tsa_host_key.pub\nCONCOURSE_TSA_WORKER_PRIVATE_KEY=path/to/worker_key\nFinally, run:\n\n# run with -E to forward env config, or just set it all as root\nsudo -E concourse worker\nNote that the worker must be run as root, as it orchestrates containers.\n\nAll logs will be emitted to stdout, with any panics or lower-level errors being emitted to stderr.\n\n","depth":4,"section_tag":"worker-running"},"yaml-quirks":{"location":"config-basics.html#yaml-quirks","title":"YAML Quirks","text":"YAML has some weird parts. For example, all of the following terms are acceptable boolean values: true, false, yes, no, on, off.\n\nYAML is also whitespace-sensitive. For the most part, this is really handy because it keeps you from having to count curly-braces in deeply nested parts of configuration such as jobplan. Sometimes, though, it can be hard to keep track of the correct indentation level.\n\n","depth":4,"section_tag":"yaml-quirks"},"yaml-tips-and-tricks":{"location":"config-basics.html#yaml-tips-and-tricks","title":"YAML Tips \u0026 Tricks","text":"YAML anchor syntax can be used to avoid repetition within configuration.\n\nFor example, the following YAML document...:\n\nlarge_value: \u0026my_anchor\n  do_the_thing: true\n  with_these_values: [1, 2, 3]\n\nduplicate_value: *my_anchor\n...is exactly equivalent to:\n\nlarge_value:\n  do_the_thing: true\n  with_these_values: [1, 2, 3]\n\nduplicate_value:\n  do_the_thing: true\n  with_these_values: [1, 2, 3]\nIf you find yourself repeating configuration throughout your pipeline, it may be a sign that Concourse is missing some kind of abstraction to make your pipeline less verbose. If you have the time and are interested in helping out with Concourse's design, feedback of this sort is welcome in the forums!\n\n","depth":4,"section_tag":"yaml-tips-and-tricks"}}
